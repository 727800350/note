#!/bin/bash

$local_hadoop dfs -ls $input | awk '{if(NF == 8) print $NF}' > file.list
CHK_RET FATAL "generate file.list error"
$local_hadoop dfs -rm $top_dir/file.list
$local_hadoop dfs -put file.list $top_dir/file.list
CHK_RET FATAL "put file.list error"

	-input $top_dir/file.list \
	-reducer "NONE" \
	-inputformat "org.apache.hadoop.mapred.lib.NLineInputFormat" \
	-jobconf "mapred.line.input.format.linespermap=1" \

	-partitioner "org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner" \
	-partitioner "com.baidu.sos.mapred.lib.IntHashPartitioner" \

	-input $input_0 \
	-input $input_1 \
	-partitioner "org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner" \
	-jobconf stream.num.map.output.key.fields=2 \
	-jobconf num.key.fields.for.partition=1 \

	-inputformat "org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat" \
	-outputformat "org.apache.hadoop.mapred.SequenceFileAsBinaryOutputFormat" \
	$hadoop_fs -rmr $mapred_output_dir/part-* 1>&2

	## min 1g, max 10g
	-inputformat "org.apache.hadoop.mapred.CombineTextInputFormat" \
	-jobconf mapred.min.split.size="1000000000" \
	-jobconf mapred.max.split.size="10000000000" \

	-outputformat "org.apache.hadoop.mapred.TextOutputFormat" \

	-jobconf mapred.compress.map.output="true" \
	-jobconf mapred.map.output.compression.codec="org.apache.hadoop.io.compress.LzoCodec" \

	-cacheArchive "$_top_dir/$owner/common/Python-2.7.5.tar.gz#Python-2.7.5" \
	-cacheArchive "/user/img-build/wangchao34/common/Python-2.7.5.tar.gz#Python-2.7.5" \
	python="/usr/bin/python"
	python="./Python-2.7.5/python"

	-cacheArchive "$_top_dir/$owner/common/php.tar.gz#php" \
	php="php"
	php="./php/bin/php"

	-cacheArchive "$_top_dir/$owner/common/sequence_file.tar.gz#sequence_file" \
	-cacheArchive "/user/img-build/wangchao34/common/sequence_file.tar.gz#sequence_file" \
if [ $local -eq 0 -a -s data.kv ]
then
	base=`pwd`
	cd sequence_file
	cat $base/data.kv | bash writeSequence.sh > $base/data.seq
	CHK_RET FATAL "write seq error"
	cd $base
fi

	## default 1000000, almost 1MB/s
	-jobconf dfs.client.slow.write.limit=2097152 \
	-jobconf dfs.client.slow.read.limit=2097152 \

