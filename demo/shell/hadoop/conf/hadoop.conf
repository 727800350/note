#!/bin/bash

${local_hadoop} dfs -ls ${input} | awk '{if(NF == 8) print $NF}' > file.list
CHK_RET FATAL "generate file.list error"
${local_hadoop} dfs -rm ${top_dir}/file.list
${local_hadoop} dfs -put file.list ${top_dir}/file.list
CHK_RET FATAL "put file.list error"

	-input ${top_dir}/file.list \
	-reducer "NONE" \

	-partitioner "org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner" \
	-partitioner "com.baidu.sos.mapred.lib.IntHashPartitioner" \

	-input ${input_1} \
	-input ${input_2} \
	-partitioner "org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner" \
	-jobconf stream.num.map.output.key.fields=2 \
	-jobconf num.key.fields.for.partition=1 \

	-inputformat "org.apache.hadoop.mapred.lib.NLineInputFormat" \
	-jobconf "mapred.line.input.format.linespermap=1" \

	-inputformat "org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat" \
	-outputformat "org.apache.hadoop.mapred.SequenceFileAsBinaryOutputFormat" \

	-inputformat "org.apache.hadoop.mapred.CombineTextInputFormat" \
	-jobconf mapred.max.split.size="1073741824" \

	-outputformat "org.apache.hadoop.mapred.TextOutputFormat" \

	-jobconf mapred.compress.map.output="true" \
	-jobconf mapred.map.output.compression.codec="org.apache.hadoop.io.compress.LzoCodec" \

	-cacheArchive "${_top_dir}/${owner}/common/Python-2.7.5.tar.gz#Python-2.7.5" \

	## default 1000000, almost 1MB/s
	-jobconf dfs.client.slow.write.limit=2097152 \
	-jobconf dfs.client.slow.read.limit=2097152 \

