#!/bin/bash

$local_hadoop dfs -ls $input | awk '{if(NF == 8) print $NF}' > file.list
CHK_RET FATAL "generate file.list error"
$local_hadoop dfs -rm $top_dir/file.list
$local_hadoop dfs -put file.list $top_dir/file.list
CHK_RET FATAL "put file.list error"

	-input $top_dir/file.list \
	-reducer "NONE" \
	-inputformat "org.apache.hadoop.mapred.lib.NLineInputFormat" \
	-jobconf "mapred.line.input.format.linespermap=1" \

	-partitioner "org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner" \
	-partitioner "com.baidu.sos.mapred.lib.IntHashPartitioner" \

	-input $input_0 \
	-input $input_1 \
	-partitioner "org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner" \
	-jobconf stream.num.map.output.key.fields=2 \
	-jobconf num.key.fields.for.partition=1 \

	-inputformat "org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat" \
	-outputformat "org.apache.hadoop.mapred.SequenceFileAsBinaryOutputFormat" \
$hadoop_fs -rmr $output/part-* 1>&2
count=0
$local_hadoop dfs -ls $output | grep part | awk '{print $NF}' | while read line
do
	$local_hadoop dfs -rm $line &
	if [ $count -lt 100 ]
	then
		count=`expr $count + 1`
	else
		count=0
		wait
	fi
done 1>log.delete 2>&1
wait

	## min 1g, max 10g
	-inputformat "org.apache.hadoop.mapred.CombineTextInputFormat" \
	-jobconf mapred.min.split.size="1000000000" \
	-jobconf mapred.max.split.size="10000000000" \

	-jobconf mapred.compress.map.output="true" \
	-jobconf mapred.map.output.compression.codec="org.apache.hadoop.io.compress.LzoCodec" \

	-cacheArchive "/user/img-build/wangchao34/common/Python-2.7.5.tar.gz#Python-2.7.5" \
	python="/usr/bin/python"
	python="./Python-2.7.5/python"

	-cacheArchive "/user/img-build/wangchao34/common/sequence_file.tar.gz#sequence_file" \
if [ $local -eq 0 -a -s data.kv ]
then
	base=`pwd`
	cd sequence_file
	cat $base/data.kv | bash writeSequence.sh > $base/data.seq
	CHK_RET FATAL "write seq error"
	cd $base
fi

	## default 1000000, almost 1MB/s
	-jobconf dfs.client.slow.write.limit=2097152 \
	-jobconf dfs.client.slow.read.limit=2097152 \

