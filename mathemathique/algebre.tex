% !Mode:: "TeX:UTF-8"
\documentclass{book}
\input{../public/package}
\input{../public/book}
\input{../public/math}

\begin{document}
\title{Alg\`ebre}
\author{Eric Wang}

\maketitle
\tableofcontents
\newpage
\setlength{\parindent}{0pt} %% set golbal noindent

%############################################################## Algebre 1 第一章 #########################################################################
\chapter{Logique et ensembles}
La relation $1+1 \neq 2$ est fausse,la double n\'egation est la relation $1+1=2$. Elle est vraie.
\newline
disjonction 或,析取 \newline
conjonction 和,合取 \newline
axiome 公理 \newline
conjecture 猜想\newline
le quantificateur existentiel $\exists$ \newline
le quantificateur universel $\forall$ \newline
\\
Un th\'eor\`eme est simplement une phrase vraie. \newline

%当且仅当
P si et seulement si Q: \newline
P si Q, c'est si Q, alors P.\quad $Q \Rightarrow P$ \newline
P seulement si Q: c'est si P, alors Q. \quad $P \Rightarrow Q$ \\
$P \Rightarrow (Q \Rightarrow R)\Leftrightarrow (P \Rightarrow Q)\rightarrow (P \Rightarrow R)$ \newline
La diff\'erence sym\'etrique de deux ensembles $E,F$.
C'est l'ensemble des \'el\'ements qui appartient \^a exactement un des ensembles $E,F$. On le note $E \Delta F$. \\
$
E \Delta F=\{x|x\in E ~\mathrm{et} ~x \not \in F\}\cup\{x|x\in F et x \not \in E\}=(E-F)\cup(F-E)
$

Une application est la m\^eme chose qu'une famille \`a valeurs dans $F$ index\'ee par $E$.
Si on note ${\cal F}(E,F)$ l'ensemble de toutes les applications de $E$ vers $F$, on a bien s\^ur
$$
{\cal F}(E,F)=F^E
$$

Si $E$ , $F$ sont finis, alors ${\cal F}(E,F)$ et $F^E$ sont finis, et
$$
|{\cal F}(E,F)|=|F^E|=|F|^|E|
$$

%############################################################## Algebre 1 第二章 #########################################################################
\chapter{Groupes. Anneaux. Corps}
$$
\begin{array}{l|llll}
groupe & ajouter & soustraire &&\\
\hline
anneau & ajouter & soustraire & multiplication & \\
\hline
corps & ajouter & soustraire & multiplication & division
\end{array}
$$


Si A est un anneau, on appelle  {\bf groupe des inversibles de} $A$ l'ensemble des \'el\'ements inversibles de $A$, muni de la deuxi\`eme op\'eration de $A$.
On note ce groupe $A^o$

%############################################################## Algebre 1 第三章 #########################################################################
\chapter{Espace vectoriel}
\section{基坐标与坐标变换}
\begin{question}
设$V$为域$F$上的$n$维线性空间, 给定$V$的两个基$\alpha, \beta$.
$\alpha = (\alpha_1, \alpha_2, \ldots, \alpha_n)$,
$\beta = (\beta_1, \beta_2, \ldots, \beta_n)$.
设$V$中向量$v$在这两个基底的坐标分别为$X = (x_1, x_2, \ldots, x_n)^t, Y = (y_1, y_2, \ldots, y_n)^t$. \\
则$X$与$Y$之间有什么样的关系?
\end{question}
\begin{answer}
$\alpha$ 为$V$的一个基底, 因此有
$$
\begin{cases}
\begin{aligned}
\beta_1 & = a_{11} \alpha_1 + a_{21} \alpha_2 + \cdots + a_{n1} \alpha_n \\
\beta_2 & = a_{12} \alpha_1 + a_{22} \alpha_2 + \cdots + a_{n2} \alpha_n \\
		\vdots \\
\beta_n & = a_{1n} \alpha_1 + a_{2n} \alpha_2 + \cdots + a_{nn} \alpha_n \\
\end{aligned}
\end{cases}
$$
$$
\Rightarrow
(\beta_1, \beta_2, \ldots, \beta_n) = (\alpha_1, \alpha_2, \ldots, \alpha_n)
\begin{bmatrix}
a_{11} & a_{12}  & \cdots & a_{1n} \\
a_{21} & a_{22}  & \cdots & a_{2n} \\
	\vdots \\
a_{n1} & a_{n2}  & \cdots & a_{nn} \\
\end{bmatrix}
$$
最后一个大矩阵记为$P_{\alpha \rightarrow \beta}$, 意思是从$\alpha$基底变换到$\beta$基底.
$$\Rightarrow (\beta_1, \beta_2, \ldots, \beta_n) = (\alpha_1, \alpha_2, \ldots, \alpha_n) \cdot P_{\alpha \rightarrow \beta} $$

$$v = (\alpha_1, \alpha_2, \ldots, \alpha_n) \cdot X$$
$$v = (\beta_1, \beta_2, \ldots, \beta_n) \cdot Y = (\alpha_1, \alpha_2, \ldots, \alpha_n) \cdot P_{\alpha \rightarrow \beta} \cdot Y $$
$$\Rightarrow X = P_{\alpha \rightarrow \beta} Y$$
\end{answer}

\begin{question}
设$\alpha = (\alpha_1, \alpha_2, \ldots, \alpha_n)$为$V$的一个基底, 且另一向量组$\beta = (\beta_1, \beta_2, \ldots, \beta_n)$满足
$(\beta_1, \beta_2, \ldots, \beta_n) = (\alpha_1, \alpha_2, \ldots, \alpha_n) \cdot P$.\\
证明: $\beta = (\beta_1, \beta_2, \ldots, \beta_n)$为$V$的一个基底的充要条件是$P$ inversible.
\end{question}
\begin{proof}
$\beta = (\beta_1, \beta_2, \ldots, \beta_n)$ 为$V$的一个base \\
$\Leftrightarrow$ 从 $k_1 \beta_1 + k_2 \beta_2 + \cdots + k_n \beta_n = 0$ 可以推出 $k_1 = k_2 = \cdots = k_n = 0$ \\
$\Leftrightarrow$ 从 $(\beta_1, \beta_2, \ldots, \beta_n) \begin{bmatrix}k_1 \\ k_2 \\ \vdots \\ k_n\end{bmatrix} = 0$
	可以推出$\begin{bmatrix}k_1 \\ k_2 \\ \vdots \\ k_n\end{bmatrix} = 0$ \\
$\Leftrightarrow$ 从 $(\alpha_1, \alpha_2, \ldots, \alpha_n) \cdot P \cdot (k_1, k_2, \cdots, k_n)^t = 0$ 可以推出$(k_1, k_2, \cdots, k_n)^t = 0$ \\
$\Leftrightarrow$ 从 $P \cdot (k_1, k_2, \cdots, k_n)^t = 0$ 可以推出$(k_1, k_2, \cdots, k_n)^t = 0$  \eqnote{因为$\alpha$为一个base}\\
$\Leftrightarrow$ $Px = 0$只有零解 \\
$\Leftrightarrow$ $detP = 0$ \\
$\Leftrightarrow$ $P$ inversible
\end{proof}

\section{子空间的直和}
\begin{theorem}
  On a une d\'ecomposition en somme directe de deux sous-espaces $ V = W_1 \bigoplus W_2$  si et seulment si
  $$
    \begin{cases}
    V=W_1+W_2 \\
    W_1\cap W_2=\{0\}
    \end{cases}
  $$
\end{theorem}
\begin{attention}
  Si le nombre de sous-espaces est $n\geqslant 3$, on peut plus dire que
  $$
    \begin{cases}
    V=W_1+\dots+W_n \\
    \forall i \neq j,W_i\cap W_j=\{0\}
    \end{cases}
  $$
  implique $ V = W_1 \bigoplus \dots \bigoplus W_n$
\end{attention}

\chapter{线性映射 Application lin\`eaire}
$f$ application lin\`eaire de $V$ \`a $W$, alors:
\begin{enumerate}
\item $f$ injective, $\alpha_1, \alpha_2, \ldots, \alpha_r$ libre $\Rightarrow f(\alpha_1), f(\alpha_2), \ldots, f(\alpha_r)$ libre.
	$f$ n'est pas injective时, 可以把线性无关的向量组映射为现行相关的向量组
\item $f$ surjective, $\alpha_1, \alpha_2, \ldots, \alpha_r$ g\'en\'eratrice $\Rightarrow f(\alpha_1), f(\alpha_2), \ldots, f(\alpha_r)$ g\'en\'eratrice 
\item $f$ isomorphisme
	$$
	\begin{cases}
	\begin{aligned}
			\alpha_1, \alpha_2, \ldots, \alpha_r \eqnote{libre} & \Leftrightarrow f(\alpha_1), f(\alpha_2), \ldots, f(\alpha_r) \eqnote{libre} \\
	\alpha_1, \alpha_2, \ldots, \alpha_r \eqnote{li\'ees} & \Leftrightarrow f(\alpha_1), f(\alpha_2), \ldots, f(\alpha_r) \eqnote{li\'ees} \\
	\alpha_1, \alpha_2, \ldots, \alpha_r \eqnote{g\'en\'eratrice} & \Leftrightarrow f(\alpha_1), f(\alpha_2), \ldots, f(\alpha_r) \eqnote{g\'en\'eratrice} \\
	\alpha_1, \alpha_2, \ldots, \alpha_r \eqnote{base de} V & \Leftrightarrow f(\alpha_1), f(\alpha_2), \ldots, f(\alpha_r) \eqnote{base de} W
	\end{aligned}
	\end{cases}
	$$
\item $\alpha_1, \alpha_2, \ldots, \alpha_r$ li\'ees $\Rightarrow f(\alpha_1), f(\alpha_2), \ldots, f(\alpha_r)$ li\'ees
\end{enumerate}

\section{域$F$上线性空间的同构 - isomorphisme}
$\sigma: V \rightarrow V'$, $\sigma$ est un isomorphisme entre $V$ et $V'$, 记作$V \cong V'$(latex 符号 \verb+\cong+), alors
\begin{enumerate}
\item 若$\alpha = (\alpha_1, \alpha_2, \ldots, \alpha_n)$为$V$的一个base, 则$\beta = (\sigma(\alpha_1), \sigma(\alpha_2), \ldots, \sigma(\alpha_n))$为$V'$的一个base.
\item 若$v \in V$在base $\alpha$下的坐标为$(a_1, a_2, \ldots, a_n)^t$,
	那么向量$\sigma(v) \in V'$在base $\beta$下的坐标为$(\sigma(a_1), \sigma(a_2), \ldots, \sigma(a_n))^t$
\item $V$中向量组$x_1, x_2, \ldots, x_s$线性相关, 当且仅当$\sigma(x_1), \sigma(x_2), \ldots, \sigma(x_s)$线性相关.
	\begin{proof}
	$$dim(x_1, x_2, \ldots, x_s) = dim(\sigma(x_1), \sigma(x_2), \ldots, \sigma(x_s))$$
	\end{proof}
\item 若$U$为$V$的sous-espace vectoriel, 那么$\sigma(U)$为$V'$的一个sous-espace vectoriel;
		同时如果$U$为有限维的, 那么$dim \sigma(U) = dim U$
\end{enumerate}

\begin{theorem}[维数决定同构]
域$F$上两个有限维线性空间同构的充要条件是他们的维数相等.
\end{theorem}
根据上面的定理, 域$F$上任何一个$n$维线性空间$V$都与$F^n$同构, 并且可建立如下$V$到$F^n$的一个同构.\\
取$V$的一个base $(\alpha_1, \alpha_2, \ldots, \alpha_n)$, 取$F^n$的标准基base canonique, 令
$$
\begin{aligned}
\sigma: V & \rightarrow F^n \\
v = \sum_{i = 1}^n a_i \alpha_i & \rightarrow \sum_{i = 1}^n a_i \epsilon_i = \begin{bmatrix}a_1 \\ a_2 \\ \vdots \\ a_n \end{bmatrix}
\end{aligned}
$$
即把$V$中每一个向量$v$对应到它在$V$的一个base canonique下的坐标.

\begin{theorem}
\begin{enumerate}
\item Soit $V$ un espace vectoriel de dimension finie $n$ sur le corps commutatif $K$.  Alors il existe un isomorphisme de $K^n$ vers $V$.
\item S'il existe un isomorphisme de $K^n$ vers $V$, alors $V$ est de dimension $n$.
\item Soient $V$ et $W$ deux espaces vectoriels isomorphes (il existe un isomorphisme de $V$ vers $W$ et un isomorphisme de $W$ vers $V$).
	On suppose que l'un des deux espaces est de dimension finie. Alors $V$ et $W$ ont la m\^eme dimension.
\item Si $V,W$ sont de dimension finie, et $\dim V = \dim W$, alors $V$ et $W$ sont isomorphes.
\end{enumerate}
\end{theorem}

\begin{proof}
1. Soit $(e_1,\dots, e_n)$ une base de $V$. On d\'efinit l'application $u : K^n \to V$ comme suit:
$$ u(\lambda_1,\dots ,\lambda_n) = \sum_{i=1}^n \lambda_i \cdot e_i $$
Il est \'evident que $u$ est une application lin\'eaire. Comme $(e_i)$ est une famille g\'en\'eratrice de $V$, il est
\'egalement clair que $u$ est surjective. Reste \`a montrer l'injectivit\'e de $u$.
\medskip

Par lin\'earit\'e de $u$, il suffit de montrer que si $u(\lambda_1,\dots ,\lambda_n) = 0_V$, alors
tous les $\lambda_i$ sont nuls. Or cela est vrai, puisque la famille $(e_i)$ est libre.
Comme $u$ est une application lin\'eaire surjective et injective, il s'agit d'un isomorphisme d'espaces vectoriels.
\medskip

2. Soit $u: K^n \to V$ un isomorphisme. On consid\`ere une base $(e_1,\dots ,e_n)$ de $K^n$ (par exemple la
base canonique). On regarde maintenant la famille
$$ (f_1,\dots ,f_n) = (u(e_1), \dots, u(e_n)) $$
Pour montrer que $V$ est de dimension $n$, il suffit de se convaincre que la famille $(f_i)$ est une base de $V$.
Montrons d'abord qu'elle est libre.
\medskip

Si $\sum_{i=1}^n \lambda_i \cdot f_i = \sum_{i=1}^n \lambda_i \cdot u(e_i) =0_V$, alors par lin\'earit\'e de $u$ on aura
$$ u\left( \sum_{i=1}^n \lambda_i \cdot e_i \right) = 0_V.  $$
Or $u$ est un isomorphisme, donc en particulier c'est une injection. Et comme $u(0)=0$, on doit avoir
$$ \sum_{i=1}^n \lambda_i \cdot e_i  = 0 $$
Mais la famille $(e_i)$ est en particulier une famille libre, donc tous les $\lambda_i$ sont nuls.
Nous avons prouv?que la famille $(f_i)$ est libre.
\medskip

Montrons maintenant que la famille $(f_i)$ est g\'en\'eratrice. Soit $v$ vecteur quelconque de $V$. Puisque
$u$ est une surjection, il existe un vecteur $t \in K^n$ avec $u(t)=v$. Comme $(e_i)$ est une famille g\'en\'eratrice
de $K^n$, il existe des scalaires $(\lambda_1, \dots, \lambda_n)$ avec
$$ t = \sum_{i=1}^n \lambda_i \cdot e_i $$
Mais alors
$$
v
= u(t)
= u\left(\sum_{i=1}^n \lambda_i \cdot e_i \right)
= \sum_{i=1}^n \lambda_i \cdot u(e_i)
= \sum_{i=1}^n \lambda_i f_i
$$
Nous avons montr\'e que $(f_i)$ est bien une famille g\'en\'eratrice. Cette famille est bien une base de $V$.
\bigskip

3. C'est une cons\'equence imm\'ediate de 1. et 2.
\bigskip

4. Soit $n$ la dimension commune de $V$ et $W$. Par 1. les deux espaces vectoriels sont alors isomorphes
\`a l'espace vectoriel $K^n$. Puisque la composition de deux isomorphismes est encore un isomorphisme, on en d\'eduit
que $V$ et $W$ sont isomorphes.
\end{proof}

\begin{example}
设$(\alpha_1, \alpha_2, \ldots, \alpha_n)$是域$F$上$n$维线性空间$V$的一个base, $(\beta_1, \beta_2, \ldots, \beta_s)$为$V$的一个向量组, 并且
$(\beta_1, \beta_2, \ldots, \beta_s) = (\alpha_1, \alpha_2, \ldots, \alpha_n) \cdot A$ \\
证明: $dim(\beta_1, \beta_2, \ldots, \beta_s) = rank(A)$
\end{example}
\begin{proof}
$(\alpha_1, \alpha_2, \ldots, \alpha_n)$为$V$的一个base $\Rightarrow dimV = n \Rightarrow v \cong F^n$. \\
设$\sigma: \alpha = \sum_{i = 1}^n a_i \alpha_i \rightarrow (a_1, a_2, \ldots, a_n)^t$ 为$V \rightarrow F^n$的一个isomorphisme. \\
$\sigma$的作用就是取坐标.

$(\beta_1, \beta_2, \ldots, \beta_s) = (\alpha_1, \alpha_2, \ldots, \alpha_n) \cdot A$
$\Rightarrow \beta_j$在$(\alpha_1, \alpha_2, \ldots, \alpha_n)$的坐标为$A$的第$j$列$A_j$, \\
因此$\sigma(\beta_j) = A_j,~ j \in [[1, s]]$
$$ \Rightarrow \sigma(\beta_1, \beta_2, \ldots, \beta_s) = (A_1, A_2, \ldots, A_s) $$
$$ dim(\beta_1, \beta_2, \ldots, \beta_s) = dim \sigma(\beta_1, \beta_2, \ldots, \beta_s) = dim(A_1, A_2, \ldots, A_s) = rank(A) $$
\end{proof}

\begin{example}
设$A, B \in M_n(F)$ \\
证明: 若$A \sim B$, 则$C(A) \cong C(B)$, 从而 $dim C(A) = dim C(B)$.\\
$C(A)$: 与$A$可交换的矩阵.
\end{example}
\begin{proof}
\end{proof}
$A \sim B \Rightarrow \exists P \in M_n(F)$ inversible telle que $B = P^{-1} \cdot A \cdot P$. 于是
$$
\begin{aligned}
\forall x \in C(A)
& \Leftrightarrow xA = Ax \\
& \Leftrightarrow P^{-1} \cdot xA \cdot P = P^{-1} \cdot Ax \cdot P \\
& \Leftrightarrow P^{-1} \cdot x \cdot P P^{-1} \cdot A \cdot P = P^{-1} \cdot A \cdot P P^{-1} \cdot x \cdot P \\
& \Leftrightarrow P^{-1} \cdot x \cdot P B = B P^{-1} \cdot x \cdot P \\
& \Leftrightarrow (P^{-1} \cdot x \cdot P) B = B (P^{-1} \cdot x \cdot P) \\
& \Leftrightarrow (P^{-1} \cdot x \cdot P) \in C(B)
\end{aligned}
$$
所以可以令
$$
\begin{aligned}
\sigma: C(A) & \rightarrow C(B) \\
		 x   & \rightarrow P^{-1} x P
\end{aligned}
$$
因此$\sigma$是一个 isomorphisme, $C(A) \cong C(B) \Rightarrow dim C(A) = dim C(B)$

\section{正交线性映射}
\begin{definition}
线性空间$V$上的两个线性变换$A, B$, 如果满足 $AB = BA = 0$, 那么称$A$与$B$是正交的.
\end{definition}

\begin{lemma}
设$U$和$W$为域$F$上线性空间$V$的两个子空间, 且$V = U \bigoplus W$, 则
$P_U$(le projecteur sur $U$ parall\`element \`a $W$) 与
$P_W$(le projecteur sur $W$ parall\`element \`a $U$)是正交的幂等变换, 且他们的和为恒等变换, 反之也成立.
即: 若$A^2 = A, B^2 = B, A + B = I, AB = BA = 0$, 那么$A, B$分别为投影.
\end{lemma}
\begin{proof}
首先需要找到$V$的子空间$U$和$W$, 使得$V = U \bigoplus W$. \\
$\forall \alpha \in V$, 由$A + B = I$, 得到$\alpha  = I \alpha = (A + B) \alpha = A \alpha + B \alpha$. \\
因此猜想$U = ImgA, W = ImgB$. \\
由$A(0) = 0$得到 $Img A \neq \emptyset$ \\
$A \alpha + A r = A(\alpha + r) \in Img A$ \\
$k A \alpha = A(k \alpha) \in Img A$ \\
所以$ImgA$是$V$的一个sous-espace vectoriel. \\
同理$ImgB$是$V$的一个sous-espace vectoriel.

$\forall \alpha \in V$ 有 $\alpha  = A \alpha + B \alpha$. 因此$V = Img A + Img B$ \\
任取$x \in (Img A) \cap (Img B)$, 则$\exists \alpha \in Img A, \beta \in Img B$, 使得 $x = A \alpha, x = B \beta$ \\
$$x = A \alpha = A^2 \alpha = A \cdot A \alpha = A \cdot x = A \cdot B \beta = AB \beta = 0 \cdot \beta = 0 $$
$$\Rightarrow Img A \cap Img B = 0$$
结合 $V = Img A + Img B$ 得到:
$$V = Img A \bigoplus Img B$$

$\forall x \in Img A$, 从上面的结论可知: $x = A x,~ B x = 0$ \\
$\forall x \in Img B$, 从上面的结论可知: $x = B x,~ A x = 0$ \\
$$\therefore A = P_U,~ U = Img A;~ B = P_W,~ W = Img B$$
\end{proof}

\section{endormorphisme}
Si $u$ est un endomorphisme diagonalisable, alors $tr(u)$ est la some des valeurs propres de $u$ compt\'ees avec leurs multiplicit\'es.

\section{核与像 Ker et Img}
\begin{theorem}
Soit $f:V \rightarrow W$ une application lin\'eaire. Soit $w \in W$ un vecteur donn\'e. On note
$$ S=\{x \in V|f(x)=w\} $$
Alors $S$ est soit l'ensemble vide, soit un ensemble de la forme $\{x_0+v|v \in ker f\}$, o\`u $x_0$ est une solution particuli\`ere de l'\'equation $f(x_0)=w$.
\end{theorem}
\begin{note}
想一想解线性微分方程的时候,我们也用到了这种思想,先找通解,然后再找一个特解,组合在一起,就构成了微分方程的完整解.
\end{note}

\begin{theorem}
设$A: V \rightarrow W$ application lineaire, $dim V = n$.
在$KerA$中取一个base $(\alpha_1, \alpha_2, \ldots, \alpha_m)$.
然后把它扩充成为$V$的一个base,即$(\alpha_1, \alpha_2, \ldots, \alpha_m, \alpha_{m + 1}, \alpha_{m + 2}, \ldots, \alpha_n)$.
于是$Img A = Vect(A \alpha_{m + 1}, A \alpha_{m + 2}, \ldots, A \alpha_n)$.
\end{theorem}
\begin{proof}
$$Ker A = Vect(\alpha_1, \alpha_2, \ldots, \alpha_m)$$
$$V = Vect(\alpha_1, \alpha_2, \ldots, \alpha_m, \alpha_{m + 1}, \alpha_{m + 2}, \ldots, \alpha_n)$$
$$dim Ker A + dim Img A = dim V \Rightarrow dim Img A = n - m$$
$$A \alpha_i \in Img A,~ \forall i \in [[1, n]]$$
$$A \alpha_i = 0,~ \forall i \in [[1, m]]$$
$\alpha_i,~ i \in [[m + 1, n]]$ 刚好有$n - m$个, 猜测$A \alpha_i,~ i \in [[m + 1, n]]$就是$Img A$的一个base. \\
只需要证明$(A \alpha_{m + 1}, A \alpha_{m + 2}, \ldots, A \alpha_n)$是一个famille libre 就可以了. \\
也就是需要能够从 $k_{m + 1} A \alpha_{m + 1} + k_{m + 2} A \alpha_{m + 2} + \ldots + k_n A \alpha_n = 0$中推断出 $k_{m + 1} = k_{m + 2} = \cdots = k_n = 0$.

$$k_{m + 1} A \alpha_{m + 1} + k_{m + 2} A \alpha_{m + 2} + \ldots + k_n A \alpha_n = 0$$
$$A(k_{m + 1} \alpha_{m + 1} + k_{m + 2} \alpha_{m + 2} + \ldots + k_n \alpha_n) = 0$$
$$k_{m + 1} \alpha_{m + 1} + k_{m + 2} \alpha_{m + 2} + \ldots + k_n \alpha_n \in Ker A$$
$$k_{m + 1} \alpha_{m + 1} + k_{m + 2} \alpha_{m + 2} + \ldots + k_n \alpha_n \in Vect(\alpha_1, \alpha_2, \ldots, \alpha_m)$$
而$(\alpha_1, \alpha_2, \ldots, \alpha_m, \alpha_{m + 1}, \alpha_{m + 2}, \ldots, \alpha_n)$是$V$的一个base, 所以是famille libre
$$\therefore k_{m + 1} = k_{m + 2} = \cdots = k_n = 0$$
所以$Img A = Vect(A \alpha_{m + 1}, A \alpha_{m + 2}, \ldots, A \alpha_n)$.
\end{proof}

\begin{example}
设$V$和$V'$分别为域$F$上$n$维, $s$维的线性空间, $A$为$V$到$V'$的一个线性映射, 它在$V$的一个base与$V'$的一个base下的矩阵为$M(A)$.
证明:
\begin{enumerate}
\item $A$是injective $\Leftrightarrow M(A)$为列满秩矩阵
\item $A$是surjective $\Leftrightarrow M(A)$为行满秩矩阵
\end{enumerate}
\end{example}
\begin{proof}
$$
\begin{aligned}
A \eqnote{est injective} & \Leftrightarrow Ker A = 0 \\
& \Leftrightarrow dim(Img A) = dim V - dim(Ker A) = n \\
& \Leftrightarrow rang(A) = n \\
& \Leftrightarrow rang(M(A)) = n
\end{aligned}
$$
由于$M(A)$为$s \times n$矩阵, 因此$M(A)$为列满秩矩阵.

$$
\begin{aligned}
A \eqnote{est surjective} & \Leftrightarrow Img A = V' \\
& \Leftrightarrow dim(Img A) = dim V' = s \\
& \Leftrightarrow rank(A) = s \\
& \Leftrightarrow rank(M(A)) = s \\
\end{aligned}
$$
由于$M(A)$为$s \times n$矩阵, 因此$M(A)$为行满秩矩阵.
\end{proof}

\textbf{Espace m\'etrique}
On appelle $(E, d)$ un espace m\'etrique si $ E$  est un ensemble et d une distance sur $E$ .

\textbf{Espace complet}
Un espace m\'etrique $ M$  est dit complet si toute suite de Cauchy de $ M$  a une limite dans $ M$  (c'est-\`a-dire qu'elle converge dans $M$ ).\newline
Intuitivement, un espace est complet s'il n'a pas de trou, s'il n'a aucun point manquant. \newline
Par exemple, les nombres rationnels ne forment pas un espace complet,
puisque $\sqrt{2}$ n'y figure pas alors qu'il existe une suite de Cauchy de nombres rationnels ayant cette limite.

Il est toujours possible de remplir les trous amenant ainsi \`a la compl\'etion d'un espace donn\'e.

\textbf{Espace euclidien}
il est d\'efini par la donn\'ee d'un espace vectoriel sur le corps des r\'eels, de dimension finie, muni d'un produit scalaire,
qui permet de mesurer distances et angles.

\textbf{Espace hermitien}
En math\'ematiques, un espace hermitien est un espace vectoriel sur le corps commutatif des complexes de dimension finie et muni d'un produit scalaire.

La g\'eom\'etrie d'un tel espace est analogue \`a celle d'un espace euclidien

Une forme hermitienne est une application d\'efini\'e sur $E \times E$ \`a valeur dans $\mathbf{C}$ not\'ee $\langle .,.\rangle$, telle que :
\begin{itemize}
		\item pour tout y fix\'e l'application $x \mapsto \langle x,y\rangle $est $\mathbf{C}$-lin\'eaire et
		\item $\forall x,y \in E$,$\langle x,y\rangle=\overline{ \langle y,x\rangle}$.
\end{itemize}
En particulier, $\langle x,x\rangle$ est r\'eel, et $x\mapsto \langle x,x\rangle$ est une forme quadratique sur E vu comme $\mathbf{R}$-espace vectoriel.

\textbf{Espace pr\'ehilbertien}
En math\'ematiques, un espace pr\'ehilbertien est d\'efini comme un espace vectoriel r\'eel ou complexe muni d'un produit scalaire

Un espace pr\'ehilbertien $(E,\langle\cdot,\cdot\rangle)$ est alors un espace vectoriel E muni d'un produit scalaire $\langle\cdot,\cdot\rangle$.

\textbf{Espace de Hilbert}
C'est un espace pr\'ehilbertien complet, c'est-\`a-dire un espace de Banach dont
la norme $\parallel\bullet\parallel$ d\'ecoule d'un produit scalaire ou hermitien $\langle\cdot,\cdot\rangle$ par la formule
$$\parallel x\parallel = \sqrt{\langle x,x \rangle}$$
C'est la g\'en\'eralisation en dimension quelconque d'un espace euclidien ou hermitien.
\bigskip

\textbf{Application hermitien's eigenvalues are real values}.
\begin{proof}
Soient A une matrice autoadjointe (r\'eelle ou complexe), $\lambda$ une racine de son polyn\^ome caract\'eristique (il en existe au moins une,
mais a priori complexe), et $X$ une matrice colonne complexe non nulle telle que
$AX=\lambda X$. Alors
$$ \overline\lambda X^*.X=(AX)^*.X=X^*.A^*.X=X^*.A.X=\lambda X^*.X $$
or $X*.X$ est non nul, donc $\lambda$ est r\'eel.
\end{proof}

\begin{theorem}[Th\'eor\`eme spectral en dimension finie, pour les endomorphismes]
Tout endomorphisme auto-adjoint d'un espace euclidien ou hermitien est diagonalisable dans une base orthonormale et ses valeurs propres sont toutes r\'eelles.
\end{theorem}

\begin{theorem}[Th\'eor\`eme spectral pour les matrices]
Soit $A$ une matrice sym\'etrique r\'eelle(resp. hermitienne complexe),
alors il existe une matrice $P$ orthogonale (resp. unitaire) et une matrice $D$ diagonale dont tous les coefficients sont r\'eels,
telles que la matrice $A = P.D.P^{-1}$
\end{theorem}

\begin{theorem}[Diagonalisation d'un endomorphisme autoadjoint et d'une matrice autoadjointe]
Un endomorphisme d'un espace euclidien ou hermitien est autoadjoint
si et seulement s'il existe une base orthonormale de vecteurs propres, avec valeurs propres toutes r\'eelles.\\
Une matrice carr\'ee complexe $A$ est autoadjointe si et seulement s'il existe une matrice unitaire $U$ telle que $U.A.U^{-1}$ soit diagonale et r\'eelle.\\
Une matrice carr\'ee r\'eelle $A$ est \textbf{sym\'etrique} si et seulement s'il existe une \textbf{matrice orthogonale} $P$
telle que $P.A.P^{-1}$ soit diagonale et r\'eelle.
\end{theorem}
Rappel:
Une matrice carr\'ee $A$(n lignes, n colonnes) \`a coefficients r\'eels est dite \textbf{orthogonale}正交矩阵 si $A^t A = I_n$, c'est-\`a-dire que $A^t = A^{-1}$

\textbf{Th\'eor\`eme de Riesz (Fr\'echet-Riesz)}\newline
un th\'eor\`eme qui repr\'esente les \'el\'ements du dual d'un espace de Hilbert comme produit scalaire par un vecteur de l'espace.
Soient :
\begin{itemize}
	\item H un espace de Hilbert (r\'eel ou complexe) muni de son produit scalaire not\'e $<.,.>$
	\item f in H' une forme lin\'eaire continue sur H.
\end{itemize}
Alors il existe un unique $y$ dans $H$ tel que pour tout x de H on ait $f(x) = <y, x>$
$$
\exists\,!\ y \in H\,, \quad \forall x\in H\,, \quad f(x) = \langle y,x\rangle
$$
\underline{Extension aux formes bilin\'eaires}\newline
Si $a$ est une forme bilin\'eaire continu\'e sur un espace de Hilbert r\'eel $\mathcal{H}$ (ou une forme sesquilin\'eaire complexe continue sur un Hilbert complexe),
alors il existe une unique application $A$ de $\mathcal{H}$ dans $\mathcal{H}$ telle que,
pour tout $(u, v) \in \mathcal{H} \times \mathcal{H}$, on ait $a(u, v) = <Au, v>$.
De plus, A est lin\'eaire et continue, de norme \'egale \`a celle de a.
$$ \exists !\,A\in\mathcal{L}(H),\ \forall (u,v)\in H\times H,\ a(u,v)=\langle Au,v \rangle $$
Cela r\'esulte imm\'ediatement de l'isomorphisme canonique(isom\'etrique) entre l'espace norm\'e des formes bilin\'eaires continues sur
$\mathcal{H} \times \mathcal{H}$ et celui des applications lin\'eaires continues de H dans son dual, et de l'isomorphisme ci-dessus entre ce dual et H lui-m\^eme.
\bigskip

\textbf{Th\'eor\`eme de Lax-Milgram}\newline
Appliqu\'e \`a certains probl\`emes aux d\'eriv\'ees partielles exprim\'es sous une formulation faible (appel\'ee \'egalement formulation variationnelle).
Il est notamment l'un des fondements de la m\'ethode des \'el\'ements finis.
Soient :
\begin{itemize}
\item $\mathcal{H}$ un espace de Hilbert r\'eel ou complexe muni de son produit scalaire not\'e $\langle.,.\rangle$, de norme associ\'ee not\'ee $\|.\|$
\item a(.\, ,\,.) une forme bilin\'eaire (ou une forme sesquilin\'eaire si $\mathcal{H}$ est complexe) qui est
	\begin{itemize}
	\item continue sur $\mathcal{H}\times\mathcal{H} : \exists\,c>0, \forall (u,v)\in \mathcal{H}^2\,,\ |a(u,v)|\leq c\|u\|\|v\|$
	\item coercive sur $\mathcal{H}$ (certains auteurs disent plut\^ot $\mathcal{H}$-elliptique):
		$\exists\,\alpha>0, \forall u\in\mathcal{H}\,,\ a(u,u) \geq \alpha\|u\|^2$
	\end{itemize}
\item $L(.)$ une forme lin\'eaire continue sur $\mathcal{H}$
\end{itemize}
Sous ces hypoth\`eses il existe un unique $u$ de $\mathcal{H}$ tel que l'\'equation $a(u,v)=L(v)$ soit v\'erifi\'ee pour tout $v$ de $\mathcal{H}$ :
$$
\quad \exists!\ u \in \mathcal{H},\ \forall v\in\mathcal{H},\quad a(u,v)=L(v)
$$
Si de plus la forme bilin\'eaire a est sym\'etrique, alors $u$ est l'unique \'el\'ement de $\mathcal{H}$ qui
minimise la fonctionnelle $J:\mathcal{H}\rightarrow\R$ d\'efinie par $J(v) = \tfrac{1}{2}a(v,v)-L(v)$ pour tout $v$ de $\mathcal{H}$, c'est-\`a-dire :
$$
\quad \exists!\ u \in \mathcal{H},\quad J(u) = \min_{v\in\mathcal{H}}\ J(v)
$$
\bigskip

$-\laplace $ admet une base de fonctions propres $v_k$, $k \in N$,
orthonormales pour le produit scalaire de $L^2(\Omega)$

%############################################################## Algebre 2 第一章 #########################################################################
\chapter{Matrices}
\begin{definition}
  Soient $V,W$ deux espaces vectoriels sur un corps communitatif $K$ avec dim$V=n$,dim$W=p$,\newline
  soit $e=(e_1,\ldots,e_n)$ une base de $V$, et $f=(f_1,\ldots,f_n)$ une base de $W$. \newline
  On appelle {\bf matrice associ\'ee \`a $u$ dans les bases} $e,f$ et on note ${\cal M}_f^e(u)$ la matrice $p \times n$ d\'efinie par la r\`egle:
  $\forall j \in [[1,n]], u(e_j)= \sum_{i=1}^p ({\cal M}_f^e(u))_{ij} \cdot f_i $. Autrement dit,
  $\forall i \in [[1,p]],\forall j \in [[1,n]]$, le scalaire $({\cal M}_f^e(u))_{ij}$ est la i-\`eme coordonn\'ee du vecteur $u(e_j) \in W $ dans la base $f$.
\end{definition}
\begin{note}
  Les {\bf coordonn\'ees de $u(e_j)$ dans la base $f$} sont les coeficients qui se trouvent dans {\bf la colonne num\'er\'e $j$ de la matrice}
  ${\cal M}_f^e(u)$.
\end{note}

\begin{theorem}
  $ \forall v \in V, {\cal C}_f(u(v))= {\cal M}_f^e(u)\cdot {\cal C}_e(v) $
\end{theorem}

\begin{note}
 $ {\cal M}_f^e(u)\cdot {\cal P}_{e\rightarrow e'}= {\cal M}_f^e(u)\cdot {\cal M}_e^{e'}(id_V)={\cal M}_f^{e'}(u)$
\end{note}

\section{矩阵的运算}
$$
A \times B = 
\begin{bmatrix}
	A_1 & A_2
\end{bmatrix}
\times
\begin{bmatrix}
	B_1 \\
	B_2
\end{bmatrix}
=
A_1 \times B_1 + A_2 \times B_2
$$
$B$对$A$进行列变换, $A$对$B$进行行变换.

\subsection{列向量组}
若$A$的列向量组为$\alpha_1,\ldots,\alpha_n$,那么我们可以把$A$写成:$A=(\alpha_1,\ldots,\alpha_n)$ \newline
设$A=(a_{ij})_{s\times n},B=(b_{ij})_{n\times m},A$ 的列向量组为$\alpha_1,\ldots,\alpha_n$. \newline
$AB$的第$j\in [[1,m]]$列为: \newline
$$
\begin{bmatrix}
  a_{11}b_{1j}+a_{12}b_{2j}+\dots+a_{1n}b_{nj}  \\
  a_{21}b_{1j}+a_{22}b_{2j}+\dots+a_{2n}b_{nj}  \\
  \vdots \\
  a_{s1}b_{1j}+a_{s2}b_{2j}+\dots+a_{sn}b_{nj}
\end{bmatrix}
\\
=
b_{1j}\alpha_1+b_{2j}\alpha_2+\dots+b_{nj}\alpha_n
$$
于是
$$
AB=(\alpha_1,\ldots,\alpha_n)
\begin{bmatrix}
  b_{11} & b_{12} & \dots & b_{1m}  \\
  b_{21} & b_{22} & \dots & b_{2m}  \\
  \vdots & \vdots & \vdots &\vdots \\
  b_{n1} & b_{n2} & \dots & b_{nm}
\end{bmatrix}
%=(b_{11}\alpha_1+b_{21}\alpha_2+\dots+b_{n1}\alpha_n,b_{12}\alpha_1+b_{22}\alpha_2+\dots+b_{n2}\alpha_n,\ldots,b_{1m}\alpha_1+b_{2m}\alpha_2+\dots+b_{nm}\alpha_n,)
%上面的这一行显示不全,所以挪到下面了
$$
$=(b_{11}\alpha_1+b_{21}\alpha_2+\dots+b_{n1}\alpha_n,b_{12}\alpha_1+b_{22}\alpha_2+\dots+b_{n2}\alpha_n,\ldots,b_{1m}\alpha_1+b_{2m}\alpha_2+\dots+b_{nm}\alpha_n,)$
\begin{note}
B对A进行列变换 \newline
A乘以B可以看做把A的列向量组分别与B的每一列的对应元素的成绩之和作为AB的相应的列向量.
\end{note}

\subsection{行向量组}
设B的行向量组为$r_1,r_2,\ldots,r_n$,则
$$
AB=
\begin{bmatrix}
  a_{11} & a_{12} & \dots & a_{1n}  \\
  a_{21} & a_{22} & \dots & a_{2n}  \\
  \vdots & \vdots & \vdots &\vdots \\
  a_{s1} & a_{s2} & \dots & a_{sn}
\end{bmatrix}
\cdot
\begin{bmatrix}
  r_1 \\
  r_2 \\
  \vdots \\
  r_n
\end{bmatrix}
=
\begin{bmatrix}
  a_{11}r_1 + a_{12}r_2 + \dots + a_{1n}r_n  \\
  a_{21}r_1 + a_{22}r_2 + \dots + a_{2n}r_n   \\
  \vdots \\
  a_{s1}r_1 + a_{s2}r_2 + \dots + a_{sn}r_n
\end{bmatrix}
$$
$A\times B$可以看做把A的每一行元素与B的行向量组的对应的行向量的成绩作为AB相应的行向量.

\begin{question}
  设A为数域K上$s \times n$的矩阵,证明:如果对于$K^n$中任一列向量$\eta$,都有$A\eta=0$,那么$A=0$
\end{question}
\begin{proof}
  设$u:x \in K^n \rightarrow Ax \in K^s$,且$A=(\alpha_1,\ldots,\alpha_n)$ \newline
  我们求出u在base canonique下的矩阵 \newline
  $$
  u(\begin{bmatrix}
    1 \\
    0\\
    \vdots \\
    0
  \end{bmatrix})
  =A \cdot
  \begin{bmatrix}
    1 \\
    0\\
    \vdots \\
    0
  \end{bmatrix}
  =\alpha_1=0
  $$
  同理,我们可以得到$\alpha_2=\dots=\alpha_n=0$,所以$A=0$ \newline
  或者,我们可以直接写
  $$
  A=AI=A(\epsilon_1,\ldots,\epsilon_n)=(A\epsilon_1,\dots,A\epsilon_n)=(0,\ldots,0)=0
  $$
\end{proof}

\section{特殊矩阵}
\subsection{对角矩阵}
la matrice diagonale,$diag(d_1,\ldots,d_n)$ \newline
用一个对角矩阵{\bf 左(右) }乘一个矩阵A,就相当于用对角矩阵的主对角元素分别去乘A相应的{\bf 行(列)}.
%对角矩阵左乘一个矩阵
$$
\begin{bmatrix}
  d_1 & 0  & 0 & \cdots & 0 \\
  0   & d_2 & 0 & \cdots & 0 \\
  \vdots & \vdots & \ddots & \vdots \\
  0 & 0 & 0 & \cdots & d_s
\end{bmatrix}
\times
\begin{bmatrix}
  r_1  \\
  r_2 \\
  \vdots \\
  r_s \\
\end{bmatrix}
=
\begin{bmatrix}
  d_1 r_1  \\
  d_2 r_2 \\
  \vdots \\
  d_s r_s \\
\end{bmatrix}
$$

%对角矩阵右乘一个矩阵
$$
(\alpha_1,\ldots,\alpha_n)
\times
\begin{bmatrix}
  d_1 & 0  & 0 & \cdots & 0 \\
  0   & d_2 & 0 & \cdots & 0 \\
  \vdots & \vdots & \ddots & \vdots \\
  0 & 0 & 0 & \cdots & d_n
\end{bmatrix}
=
\begin{bmatrix}
  d_1 \alpha_1 & d_2 \alpha_2 & \dots & d_n \alpha_n \\
\end{bmatrix}
$$

特别的,两个n级对角矩阵的乘积还是n级对角矩阵,并且是把相应的主对角元相乘

\subsection{三角矩阵}
\noindent
上三角矩阵(上面的三角不为0,而下面的三角为零的矩阵) \newline
下三角矩阵(下面的三角不为0,而上面的三角为零的矩阵)
\begin{note}
  用初等矩阵左(右)乘一个矩阵A,就相当于对A进行了一次相应的初等行(列)变换
\end{note}

\subsection{基本矩阵}
只有一个元素为1,其余元素均为0的矩阵, (i,j)元为1的基本矩阵记为$E_{ij}$
\begin{itemize}
\item 任意一个矩阵A都可以记为$A=\sum_{i=1}^s {\sum_{j=1}^n {a_{ij}E_{ij}}}$
\item 用$E_{ij}$左乘一个矩阵A,相当于把A的第j行搬到第i行的位置,然后乘积所得矩阵的其余行均为0
\item 用$E_{ij}$右乘一个矩阵A,相当于把A的第i列搬到第j列的位置,然后乘积所得矩阵的其余列均为0
\end{itemize}

矩阵$A$的行向量组为$r_1, r_2, \ldots, r_s$, 列向量组为$c_1, c_2, \ldots, c_n$, 则
$$
E_{ij} \times A
=
\begin{bmatrix}
	0 & 0 & \cdots & 0 & 0 \\
	0 & 0 & \cdots & 0 & 0 \\
	0 & 0 & \cdots & 1_{ij} & 0\\
	0 & 0 & \cdots & 0 & 0\\
\end{bmatrix}
\times
\begin{bmatrix}
	r_1 \\
	r_2 \\
	\vdots \\
	r_s \\
\end{bmatrix}
=
\begin{bmatrix}
	0 \\
	\vdots \\
	0_{i - 1} \\
	r_j \\
	0_{i + 1} \\
	\vdots \\
	0 \\
\end{bmatrix}
$$

$$
A \times E_{ij}
=
\begin{bmatrix}
	c_1 & c_2 & \cdots & c_n
\end{bmatrix}
\times
\begin{bmatrix}
	0 & 0 & \cdots & 0 & 0 \\
	0 & 0 & \cdots & 0 & 0 \\
	0 & 0 & \cdots & 1_{ij} & 0\\
	0 & 0 & \cdots & 0 & 0\\
\end{bmatrix}
=
\begin{bmatrix}
	0 & \cdots & 0_{j - 1} & c_i & 0_{j + 1} & \cdots & 0
\end{bmatrix}
$$

$$
E_{ij}E_{kl}=
\begin{cases}
  E_{il} & \si k=j \\
  0 & \si k \neq j
\end{cases}
$$

\begin{question}
  循环移位矩阵:
  $$
  C=
  \begin{bmatrix}
    \epsilon_n & \epsilon_1 & \epsilon_2 & \epsilon_3 & \dots & \epsilon_{n-2} & \epsilon_{n-1} \\
    \hline
        0      &     1      &     0      &       0    & \dots &      0         &       0        \\
        0      &     0      &     1      &       0    & \dots &      0         &       0        \\
        0      &     0      &     0      &       1    & \dots &      0         &       0        \\
        \vdots &    \vdots  &     \vdots &     \vdots & \ddots&      \vdots    &       \vdots   \\
        0      &     0      &     0      &       0    & \dots &      0         &       1        \\
        1      &     0      &     0      &       0    & \dots &      0         &       0
  \end{bmatrix}
  $$
  (1)用C左乘A,相当于把A 的行向上移一行,第一行换到最后一行 \newline
  用C右乘B,相当于把B 的列向右移一列,最后一列换到第一列 \newline
  (2)$\sum_{l=0}^{n-1} C^l=J$,其中J 为元素全为1 的n 级矩阵
\end{question}
\begin{proof}[Demontration(2)]
  $\newline C=(\epsilon_n,\epsilon_1 , \epsilon_2 ,\epsilon_3 , \dots , \epsilon_{n-2}, \epsilon_{n-1} ) \\
    C^2=( \epsilon_{n-1},\epsilon_n,\epsilon_1 , \epsilon_2 ,\epsilon_3 , \dots , \epsilon_{n-2}  ) \\
    C^3=(\epsilon_{n-2}, \epsilon_{n-1},\epsilon_n,\epsilon_1 , \epsilon_2 ,\epsilon_3 , \dots , \epsilon_{n-3}  ) \\
    until \\
    C^{n-1}=(\epsilon_2 ,\epsilon_3 , \dots , \epsilon_n,\epsilon_1 ) \\
    \sum_{l=0}^{n-1} C^l=
    (\epsilon_n,\epsilon_1 , \epsilon_2 , \dots , \epsilon_{n-2}, \epsilon_{n-1} ) +
    ( \epsilon_{n-1},\epsilon_n,\epsilon_1 , \dots , \epsilon_{n-2}  ) +
    (\epsilon_{n-2}, \epsilon_{n-1},\epsilon_n,\epsilon_1 , \dots , \epsilon_{n-3}  ) +
    (\epsilon_2 ,\epsilon_3 , \dots , \epsilon_n,\epsilon_1 ) \\
    =(\epsilon_1+\epsilon_n+\epsilon_{n-1}+\dots+\epsilon_1,\epsilon_2+\epsilon_1+\epsilon_n+\dots+\epsilon_3,\dots,\epsilon_n+\epsilon_{n-1}+\dots+\epsilon_1)\\
    =
    \begin{bmatrix}
      1 & 1 & 1 & \cdots & 1 \\
      1 & 1 & 1 & \cdots & 1 \\
      \vdots &\vdots &\vdots &\vdots &\vdots \\
      1 & 1 & 1 & \cdots & 1 \\
    \end{bmatrix}
    =J$
\end{proof}
\section{Rang 秩}
$$rang(AB)\leqslant rang(A)$$
$$rang(A+B)\leqslant rang(A) + rang(B)$$
$$\si k \neq 0, rang(kA) = rang(A)$$

\section{D\'etermiants}

\section{可逆矩阵}
\begin{itemize}
\item Si $A$ inversible, alors $A^t$ inversible et $(A^t)^{-1} = (A^{-1})^t$
$$
A \cdot A^{-1} = 1
\Rightarrow (A \cdot A^{-1})^t = 1^t = 1
\Rightarrow (A^{-1})^t \cdot A^t = 1
\Rightarrow (A^t)^{-1} = (A^{-1})^t
$$
\item 用一个可逆矩阵左(右)乘一个矩阵$A$, 不改变$A$的rang.
\end{itemize}

\begin{question}
Si $A$ une matrice carr\'ee et $A^2 = I$, 称$A$为对合矩阵. 设$A, B \in M_{n \times n}(K)$.
证明: 若$A, B$都为对合矩阵, 且 $|A| + |B| = 0$, 那么 $A + B, I + AB$ 都不可逆. \\
\end{question}
\begin{proof}
这里的绝对值符号应该是表示 det 的意思.
$$
A^2 = I
\Rightarrow det(A^2) = det(I) = 1
\Rightarrow (det(A))^2 = 1
\Rightarrow det(A) = +1 \ou -1
$$
不妨设 $det(A) = 1, det(B) = -1$.
$$|A| \cdot |A + B| = |A(A + B)| = |A^2 + AB| = |I + AB|$$
$$|A + B| \cdot |B| = |(A + B)B| = |AB + B^2| = |AB + I|$$
因此 $$|A + B| = |A| \cdot |A + B| = |A + B| \cdot |B| = -|A + B|$$
$$\Rightarrow |A + B| = 0$$
$$\Rightarrow |I + AB| = 0$$
因此$A + B, I + AB$ 都不可逆.
\end{proof}

\begin{note}
任意一个矩阵, 若有以下三个性质的任意两个, 则第三个也必然成立
\begin{enumerate}
\item 对称矩阵: $A^t = A$
\item 正交矩阵: $A^t \cdot A = I$
\item 对合矩阵: $A^2 = I$
\end{enumerate}
\end{note}

\section{矩阵的分块}
\begin{remark}
设$A \in M_{s \times n}(K), B \in M_{n \times m}(K)$

$B$的列向量为$(\beta_1, \beta_2, \ldots, \beta_m)$, 则
$$AB = A(\beta_1, \beta_2, \ldots, \beta_m) = (A\beta_1, A\beta_2, \ldots, A\beta_m)$$

$A$的行向量组为$(r_1, r_2, \ldots, r_s)$, 则
$$
AB =
\begin{bmatrix}
r_1 \\
r_2 \\
\vdots \\
r_s
\end{bmatrix}
B =
\begin{bmatrix}
r_1 \cdot B \\
r_2 \cdot B \\
\vdots \\
r_s \cdot B
\end{bmatrix}
$$
\end{remark}

\begin{corollary}
设$A_{s \times n} \neq 0, B_{n \times m}$的列向量组$(\beta_1, \beta_2, \ldots, \beta_m)$, $C_{n \times m}$的列向量组为$(\delta_1, \delta_2, \ldots, \delta_m)$.
则
$$ AB = C \Leftrightarrow \beta_j \eqnote{为} Ax = \delta_j \eqnote{的一个解}, j = 1, 2, \ldots, m $$
\end{corollary}
\begin{proof}
$$
\begin{aligned}
		AB & = C \\
		AB & = A(\beta_1, \beta_2, \ldots, \beta_m) = (A\beta_1, A\beta_2, \ldots, A\beta_m) \\
		C & = (\delta_1, \delta_2, \ldots, \delta_m) \\
		\Rightarrow A \beta_j & = \delta_j,~ \forall j \in [[1, m]]
\end{aligned}
$$
\end{proof}

利用前面的推论, 可以使用线性方程组来求可逆矩阵的逆矩阵. \\
$A \cdot A^{-1} = I$, 设$A^{-1} = (x_1, x_2, \ldots, x_n), I = (\delta_1, \delta_2, \ldots, \delta_n)$. 则,
$$
\begin{aligned}
& A (x_1, x_2, \ldots, x_n) = (\delta_1, \delta_2, \ldots, \delta_n) \\
& \Leftrightarrow A x_i = \delta_i,~ \forall i \in [[1, n]] \\
& \Leftrightarrow x_i \eqnote{为} A x_i = \delta_i 的一个解 ,~ \forall i \in [[1, n]] \\
\end{aligned}
$$

\section{正交矩阵 matrice orthogonal}
正交矩阵的集合用$O(n)$表示.

\begin{question}
设$A \in M_{m \times n}(R), m > n, \beta \in R^m$.
若$x_o \in R^n$, 使得$|\beta - A x_0|^2 \leq |\beta - A x|^2,~ \forall x \in R^n$
那么称$x_0$为$Ax=\beta$的最小二乘解. \\
证明: $x_0$为$Ax = \beta$的最小二乘解当且仅当$x_0$ 为 $A^t \cdot Ax = A^t B$的解.
\end{question}
\begin{proof}
设$A$的列向量组为 $(\alpha_1, \alpha_2, \ldots, \alpha_n)$. \\
则$A$的列空间$U = vect(\alpha_1, \alpha_2, \ldots, \alpha_n)$. \\
则
$$
\begin{aligned}
	x_0 \eqnote{为} Ax = \beta \eqnote{的最小二乘解} & \Leftrightarrow |\beta - A x_0|^2 \leq |\beta - A x|^2,~ \forall x \in R^n \\
	& \Leftrightarrow |\beta - A x_0| \leq |\beta - A x|,~ \forall x \in R^n \\
	& \Leftrightarrow |\beta - A x_0| \leq |\beta - r|,~ \forall r \in U \\
	& \Leftrightarrow A x_0 \eqnote{为} \beta \eqnote{在} U \eqnote{上的正交投影} \\
	& \Leftrightarrow \beta - A x_0 \in U^{\perp} \\
	& \Leftrightarrow <\beta - A x_0, \alpha_i> = 0,~ \forall i \in [[1, n]] \\
	& \Leftrightarrow \alpha^t (\beta - A x_0) = 0,~ \forall i \in [[1, n]] \\
	& \Leftrightarrow A^t (\beta - A x_0) = 0 \\
	& \Leftrightarrow A^t A x_0 = A^t \beta \\
	& \Leftrightarrow x_0 \eqnote{为} A^t \cdot Ax = A^t B \eqnote{的解}
\end{aligned}
$$
\end{proof}

\subsection{最小二乘解}
最佳逼近元: 设$U$为实内积空间$V$的一个子空间, 对于$\alpha \in V$, 如果存在$\delta \in V$,使得
$$\forall r \in U, d(\alpha, \delta) \leq d(\alpha, r)$$
那么称$\delta$为$\alpha$在$U$上的最佳逼近元.
(若$U$为有限维的, 则$\delta$为$\alpha$在$U$上的正交投影)

实际应用: 在许多实际问题中, 需要研究一个变量$y$与其他一些变量$x_1, x_2, \ldots, x_n$之间的关系. \\
经过实际的观测和分析, 假定变量$y$与$x_1, x_2, \ldots, x_n$之间呈线性关系:
$y = k_1 x_1 + k_2 x_2 + \ldots + k_n x_n$.\\
为了去顶$k_1, K_2, \ldots, k_n$, 需要观测$m$次, 得到下面数值:

\begin{center}
\begin{tabular}{c|cccc}
$y$ & $x_1$ & $x_2$ & $\ldots$ & $x_n$ \\ \hline
$b_1$ & $a_{11}$ & $a_{12}$ & $\ldots$ & $a_{1n}$ \\
$b_2$ & $a_{21}$ & $a_{22}$ & $\ldots$ & $a_{2n}$ \\
		$\vdots$ \\
$b_m$ & $a_{m1}$ & $a_{m2}$ & $\ldots$ & $a_{mn}$
\end{tabular}
\end{center}
如果观测绝对精确, 只需要方程个数$m$与未知数个数$n$相等即可.
但是观测中, 肯定会有误差, 因此需要 $m > n$, 将上面的数据写成方程组的形式如下:
$$
\begin{cases}
\begin{aligned}
a_{11} k_1 + a_{12} k_2 + \ldots + a_{1n} k_n & = b_1 \\
a_{21} k_1 + a_{22} k_2 + \ldots + a_{2n} k_n & = b_2 \\
\vdots & \\
a_{m1} k_1 + a_{m2} k_2 + \ldots + a_{mn} k_n & = b_m \\
\end{aligned}
\end{cases}
$$
方程个数$m >$未知数个数$n$, 这时方程组可能无解. \\
这时, 我们就像找一组数$(c_1, c_2, \ldots, c_n)$, 使得$\forall k_1, k_2, \ldots, k_n \in R$.
\begin{equation}
\sum_{i = 1}^m (a_{i1} c_1 + a_{i2} c_2 + \ldots + a_{in} c_n - b_i)^2
\leq
\sum_{i = 1}^m (a_{i1} k_1 + a_{i2} k_2 + \ldots + a_{in} k_n - b_i)^2
\label{eq.least_square.def}
\end{equation}
此时, 我们把$(c_1, c_2, \ldots, c_n)'$称为线性方程组的最小二乘解. \\
我们将方程组的系数矩阵记为$A$, 同时令
$x = (k_1, k_2, \ldots, k_n)^t$,
$\alpha = (c_1, c_2, \ldots, c_n)^t$,
$\beta = (b_1, b_2, \ldots, b_m)^t$
$$
\begin{cases}
\begin{aligned}
a_{11} c_1 + a_{12} c_2 + \ldots + a_{1n} c_n - b_1 \\
a_{21} c_1 + a_{22} c_2 + \ldots + a_{2n} c_n - b_2 \\
\vdots \\
a_{m1} c_1 + a_{m2} c_2 + \ldots + a_{mn} c_n - b_m \\
\end{aligned}
\end{cases}
$$
上面的方程组可以写成矩阵的形式:
$$A \alpha - \beta$$
\eqref{eq.least_square.def}的左边为向量$A \alpha - \beta$的长度的平方, 也就是向量$\beta$ 与 向量 $A \alpha$的距离的平方. \\
令$U = Vect(A_1, A_2, \ldots, A_n)$, 则
$$
A \cdot \alpha
=
\begin{bmatrix}
A_1 & A_2 & \cdots & A_n
\end{bmatrix}
\cdot
\begin{bmatrix}
c_1 \\
c_2 \\
\vdots \\
c_n
\end{bmatrix}
= c_1 A_1 + c_2 A_2 + \ldots + c_n A_n \in U
$$

$$
A \cdot x
=
\begin{bmatrix}
A_1 & A_2 & \cdots & A_n
\end{bmatrix}
\cdot
\begin{bmatrix}
k_1 \\
k_2 \\
\vdots \\
k_n
\end{bmatrix}
= k_1 A_1 + k_2 A_2 + \ldots + k_n A_n \in U,~ \forall k_1, k_2, \ldots, k_n \in R
$$

于是:
$$
\begin{aligned}
\alpha \eqnote{为线性方程组} A x = \beta \eqnote{的最小二乘解}
& \Leftrightarrow \norm{A \alpha - \beta}^2 \leq \norm{A x - \beta}^2,~ \forall x \in R^m \\
& \Leftrightarrow d(A \alpha, \beta) \leq d(r, \beta),~ \forall r \in U \\
& \Leftrightarrow A \alpha \eqnote{为} \beta \eqnote{在} U \eqnote{上的正交投影} \\
& \Leftrightarrow \beta - A \alpha \in U^{\perp} \\
& \Leftrightarrow <\beta - A \alpha, \alpha_j> = 0,~ j = 1,2, \ldots, n \\
& \Leftrightarrow \alpha_j^{'} (\beta - A \alpha) = 0,~ j = 1,2, \ldots, n \\
& \Leftrightarrow A^t (\beta - A \alpha) = 0 \\
& \Leftrightarrow A^t A \alpha = A^t \beta \\
& \Leftrightarrow \alpha \eqnote{为线性方程组} A^t A x = A^t \beta \eqnote{的解}
\end{aligned}
$$
这样, 我们就把求线性方程组$A x = \beta$最小二乘解的问题化为求解线性方程组$A^t A x = A^t \beta$的解($A^t A$为方阵).

\section{$K^n$到$K^s$的线性映射}
设$A \in M_{s \times n}(K)$, $A$的列向量组为$(\alpha_1, \alpha_2, \ldots, \alpha_n)$.
则
$$
\begin{aligned}
Ax = \beta \eqnote{有解} & \Leftrightarrow \beta \in ImA \\
	& \Leftrightarrow \beta \in vect(\alpha_1, \alpha_2, \ldots, \alpha_n)
\end{aligned}
$$

$$
\begin{aligned}
A: K^n \rightarrow & K^s \\
     x \rightarrow & Ax
\end{aligned}
$$
$$ dimKerA + dimImgA = n $$
$$ dimImgA = rank(A) $$

\section{矩阵的相似}
若$A \in M_n(K), A diagonalisable$则, $A$ 与 $A^t$ 相似.
\begin{proof}
$\exists D \in M_n(K)$ diagonal et $P \in M_n(K)$ inversible telle que $P^{-1} \cdot A \cdot P = D$
$$ D^t = (P^{-1} \cdot A \cdot P)^t = P^t \cdot A^t \cdot (P^{-1})^t = P^t \cdot A^t \cdot (P^t)^{-1} $$
$$ \Rightarrow D^t \sim A^t \Rightarrow A \sim D \sim D^t \sim A^t \Rightarrow A \sim A^t $$
\end{proof}

\section{矩阵的特征值与特征向量}
设$A \in M_n(K)$, 则$A$的特征多项式为 $det(\lambda I - A)$ 为一个$n$次多项式 \\
$\lambda^n$的系数为$1$,
$\lambda^{n - 1}$的系数为$-tr(A)$,
常数项为 $(-1)^{detA}$

\begin{example}
证明: 幂零矩阵一定有特征值, 并且它的特征值一定为0
\end{example}
\begin{proof}
设幂零矩阵$A$的幂零指数为$l$, 则$A^l = 0$.

存在性:
$$
\Rightarrow det(A^l) = 0
\Rightarrow (detA)^l = 0
\Rightarrow detA = 0
\Rightarrow det(0 \cdot I - A) = (-1)^n detA = 0
\Rightarrow 0 \in SpecA
$$

唯一性:
设$\lambda \in SpecA$, 那么
$\exists x \in K^n \et x \neq 0$, 使得 $A x = \lambda x$.
$$\Rightarrow A^2 x = A \cdot \lambda x = \lambda (Ax) = \lambda^2 x$$
以此类推得到
$$ A^l x = \lambda^l x \Rightarrow \lambda^l x = 0$$
而$x \neq 0$, 所以$\lambda^l = 0 \Rightarrow \lambda = 0$
\end{proof}

\begin{example}
证明: 幂等矩阵一定有特征值, 且为1或者0.
\end{example}
\begin{proof}
幂等矩阵$A$, $A^2 = A$
$$ \lambda x = A x = A^2 x = A \cdot Ax = A \cdot \lambda x = \lambda Ax = \lambda^2 x \Rightarrow \lambda = 1 \ou 0$$
\end{proof}

\chapter{Max-plus algebra}
A max-plus algebra is a \textbf{semiring(demi-anneau):(半环是类似于环但没有加法逆元的代数结构)} over the union of real numbers and
$\varepsilon = -\infty$, equipped with maximum and addition as the two binary operations.

\textbf{Scalar operations}\\
Let a and b be real scalars or $\varepsilon$.
Then the operations maximum(implied by the max operator $\oplus$) and addition(plus operator $\otimes$) for these scalars are defined as
$$ a \oplus b = \max(a,b) $$
$$ a \otimes b = a + b $$
Similar to the conventional algebra, all $\otimes$ - operations have a higher precedence than $\oplus$ - operations.

\textbf{Matrix operations}\\
$$ [A \oplus B]_{ij} = [A]_{ij} \oplus [B]_{ij} = \max([A]_{ij} , [B]_{ij}) $$
$$ [A \otimes B]_{ij} = \bigoplus_{k = 1}^p [A]_{ik} \otimes [B]_{kj} = \max([A]_{i1} + [B]_{1j}, \dots, [A]_{ip} + [B]_{pj}) $$

\textbf{Algebra properties}\\
\begin{itemize}
\item associativity:
	\begin{itemize}
	\item $(a \oplus b) \oplus c = a \oplus (b \oplus c) $
	\item $(a\otimes b) \otimes c = a \otimes (b \otimes c)$
	\end{itemize}
\item commutativity :
	\begin{itemize}
	\item $a \oplus b = b \oplus a $
	\item $a \otimes b = b \otimes a $
	\end{itemize}
\item distributivity:
	\begin{itemize}
	\item  $(a \oplus b) \otimes c = a \otimes c \oplus b \otimes c $
	\end{itemize}
\end{itemize}

\chapter{SVD}
\begin{theorem}
\textbf{Singular value decomposition}\\
Suppose $A$ is an $m \times n$ matrix whose entries come from the field $K$, which is either the field of real numbers or the field of complex numbers.
Then there exists a factorization of the form
$$
\mathbf{A} = \mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^*
$$
where $U$ is an $m \times m$ unitary matrix over $K$ (orthogonal matrix if $K = R$),
$Σ$ is an $m \times n$ diagonal matrix with non-negative real numbers on the diagonal,
and the $n \times n$ unitary matrix $V^*$ denotes the conjugate transpose of the $n \times n$ unitary matrix $V$
\end{theorem}
\begin{proof}
$A$ is $m \times n \Rightarrow AA^T$ is $m \times m, A^T A$ is $n \times n$\\
Let $\lambda_1, \ldots, \lambda_r$ be the \textbf{nonzero} eigenvalues of $A^T A$, \\
and the corresponding unit eigenvector are $v_1, \ldots, v_r$ with $v_i \in K^n$\\
$r$ is just the rank of $A^T A$, which is also the rank of $A$.
\todo{proof it is also the rank of $A$}
$$A A^T A v_i = A \lambda_i v_i = \lambda_i (A v_i)$$
所以, $Av_i$刚好就是$A A^T$的vector propre, 相应的valeur propre也是$\lambda_i$.
$$\norm{Av_i} = \sqrt{(Av_i)^T Av_i} = \sqrt{v_i^T A^T Av_i} = \sqrt{v_i^T \lambda_i v_i} = \sqrt{\lambda_i} \equiv \sigma_i$$
Let $u_i = \dfrac{Av_i}{\sigma_i}$, 那么 $\norm{u_i} = 1$ and $\u_i \in K^m$

$$
\forall i, j \in \{1,2,\ldots, r\},\
u_i^T Av_j
= (\dfrac{Av_i}{\sigma_i})^T Av_j
= \dfrac{1}{\sigma_i} v_i^T A^T Av_j
= \dfrac{\lambda_j}{\sigma_i} v_i^T v_j
= \dfrac{\lambda_j}{\sigma_i} \delta_{ij}
=
\left\{
  \begin{array}{ll}
    \sigma_i & \si i = j \\
    0 & \sinon
  \end{array}
\right.
$$
将这$r \times r$个方程写成矩阵的形式:
$$
\begin{bmatrix}
u_1^T \\
u_2^T \\
\vdots \\
u_r^T \\
\end{bmatrix}
\cdot
A
\cdot
\begin{bmatrix}
v_1 & v_2 & \cdots & v_r
\end{bmatrix}
=\diag{\sigma_1, \sigma_2, \ldots, \sigma_r}
$$
The $u_i$ are vectors from an $m$-dimensional space, and we only have $r$ of them, so we can pick unit
vectors $u_{r+1}, \ldots, u_m$ that are pairwise orthogonal, and orthogonal to $u_1,\ldots, u_r$. Similarly we can
find $v_{r+1}, \ldots, v_n$ such that $v_1,...,v_n$ are pairwise orthogonal unit vectors. We then still have that
$u_i^T A v_j$ is $\sigma_i$ when $i = j \leq r$ and by a calculation analogous to the above it is
\textbf{zero for all other cases(因为其他的特征值都为0)}.\\
In matrix form this gives us
$$
\begin{bmatrix}
u_1^T \\
u_2^T \\
\vdots \\
u_m^T \\
\end{bmatrix}
\cdot
A
\cdot
\begin{bmatrix}
v_1 & v_2 & \cdots & v_n
\end{bmatrix}
=\Sigma
$$
where $\Sigma$ is now an $m \times n$ matrix with its first $r$ diagonal entries being the
$\sigma_1, \sigma_2, \ldots, \sigma_r$ and zeroes everywhere else.

Defining $U = [u_1, \ldots, u_m]$ and $V = [v_1, \ldots, v_n]$ column vectors.
we now have $U^TAV = \Sigma$\\
and as orthogonal matrix, $U^{-1} = U^T, V^{-1} = V^T$
$$ A = U \Sigma V^T $$
This is the famous singular value decomposition, and you have just seen a complete proof of its existence for an arbitrary $m \times n$ matrix $A$.
\end{proof}

\begin{remark}
\textbf{econonmy version of the SVD}\\
$$ A = U_r \Sigma_r V_r^T $$
\end{remark}

\begin{remark}
The $\sigma_1, \ldots, \sigma_r$ which are just the square roots of the eigenvalues of $A A^T$ or ($A^T A$), are called the singular values of A.
The columns of $U$, which are just the eigenvectors of $A A^T$, are called \textbf{left singular vectors of $A$},
and the columns of $V$ , which are just the eigenvectors of $A^T A$, are called the \textbf{right singular vectors of $A$}
\end{remark}

Any arbitrary matrix $X$ can be converted to an orthogonal matrix, a diagonal matrix and another orthogonal matrix
(or a rotation, a stretch and a second rotation)
\end{document}

