% !Mode:: "TeX:UTF-8"
\documentclass{article}
\input{../public/package}
\input{../public/article}
\input{../public/math}
\begin{document}
\title{Alg\`ebre}
\maketitle
\tableofcontents
\newpage
\section{Espace}
\textbf{Espace m\'etrique}
On appelle $(E, d)$ un espace m\'etrique si $ E$  est un ensemble et d une distance sur $E$ .

\textbf{Espace complet}
Un espace m\'etrique $ M$  est dit complet si toute suite de Cauchy de $ M$  a une limite dans $ M$  (c'est-\`a-dire qu'elle converge dans $M$ ).\newline
Intuitivement, un espace est complet s'il n'a pas de trou, s'il n'a aucun point manquant. \newline
Par exemple, les nombres rationnels ne forment pas un espace complet, puisque $\sqrt{2}$ n'y figure pas alors qu'il existe une suite de Cauchy de nombres rationnels ayant cette limite.

Il est toujours possible de remplir les trous amenant ainsi \`a la compl\'etion d'un espace donn\'e.

\textbf{Espace euclidien}
 il est d\'efini par la donn\'ee d'un espace vectoriel sur le corps des r\'eels, de dimension finie, muni d'un produit scalaire, qui permet de mesurer distances et angles.

\textbf{Espace hermitien}
En math\'ematiques, un espace hermitien est un espace vectoriel sur le corps commutatif des complexes de dimension finie et muni d'un produit scalaire.

La g\'eom\'etrie d'un tel espace est analogue \`a celle d'un espace euclidien

Une forme hermitienne est une application d\'efini\'e sur $E \times E$ \`a valeur dans $\mathbf{C}$ not\'ee $\langle .,.\rangle$, telle que :
\begin{itemize}
		\item pour tout y fix\'e l'application $x \mapsto \langle x,y\rangle $est $\mathbf{C}$-lin\'eaire et
		\item $\forall x,y \in E$,$\langle x,y\rangle=\overline{ \langle y,x\rangle}$.
\end{itemize}
En particulier, $\langle x,x\rangle$ est r\'eel, et $x\mapsto \langle x,x\rangle$ est une forme quadratique sur E vu comme $\mathbf{R}$-espace vectoriel.

\textbf{Espace pr\'ehilbertien}
En math\'ematiques, un espace pr\'ehilbertien est d\'efini comme un espace vectoriel r\'eel ou complexe muni d'un produit scalaire

Un espace pr\'ehilbertien $(E,\langle\cdot,\cdot\rangle)$ est alors un espace vectoriel E muni d'un produit scalaire $\langle\cdot,\cdot\rangle$.

\textbf{Espace de Hilbert}
C'est un espace pr\'ehilbertien complet, c'est-\`a-dire un espace de Banach dont la norme $\parallel\bullet\parallel$d\'ecoule d'un produit scalaire ou hermitien $\langle\cdot,\cdot\rangle$ par la formule
 $$\parallel x\parallel = \sqrt{\langle x,x \rangle}$$
C'est la g\'en\'eralisation en dimension quelconque d'un espace euclidien ou hermitien.
\bigskip

\textbf{Application hermitien's eigenvalues are real values}.
\begin{proof}
Soient A une matrice autoadjointe (r\'eelle ou complexe), $\lambda$ une racine de son polyn\^ome caract\'eristique (il en existe au moins une, mais a priori complexe), et $X$ une matrice colonne complexe non nulle telle que
$AX=\lambda X$. Alors
$$ \overline\lambda X^*.X=(AX)^*.X=X^*.A^*.X=X^*.A.X=\lambda X^*.X $$
or $X*.X$ est non nul, donc $\lambda$ est r\'eel.
\end{proof}

\begin{theorem}
Th\'eor\`eme spectral en dimension finie, pour les endomorphismes —  Tout endomorphisme auto-adjoint d'un espace euclidien ou hermitien est diagonalisable dans une base orthonormale et ses valeurs propres sont toutes r\'eelles.

Th\'eor\`eme spectral pour les matrices —  Soit $A$ une matrice sym\'etrique r\'eelle (resp. hermitienne complexe), alors il existe une matrice $P$ orthogonale (resp. unitaire) et une matrice $D$ diagonale dont tous les coefficients sont r\'eels, telles que la matrice $A = P.D.P^{-1}$
\end{theorem}

\begin{theorem}
Diagonalisation d'un endomorphisme autoadjoint et d'une matrice autoadjointe — \\
Un endomorphisme d'un espace euclidien ou hermitien est autoadjoint si et seulement s'il existe une base orthonormale de vecteurs propres, avec valeurs propres toutes r\'eelles.\\
Une matrice carr\'ee complexe $A$ est autoadjointe si et seulement s'il existe une matrice unitaire $U$ telle que $U.A.U^{-1}$ soit diagonale et r\'eelle.\\
Une matrice carr\'ee r\'eelle $A$ est \textbf{sym\'etrique} si et seulement s'il existe une \textbf{matrice orthogonale} P telle que $P.A.P^{-1}$ soit diagonale et r\'eelle.
\end{theorem}
Rappel:
Une matrice carr\'ee $A$ (n lignes, n colonnes) \`a coefficients r\'eels est dite \textbf{orthogonale}正交矩阵 si $A^t A = I_n$, c'est-\`a-dire que $A^t = A^{-1}$

\textbf{Th\'eor\`eme de Riesz (Fr\'echet-Riesz)}\newline
un th\'eor\`eme qui repr\'esente les \'el\'ements du dual d'un espace de Hilbert comme produit scalaire par un vecteur de l'espace.
Soient :
\begin{itemize}
	\item H un espace de Hilbert (r\'eel ou complexe) muni de son produit scalaire not\'e $<.,.>$
	\item f in H' une forme lin\'eaire continue sur H.
\end{itemize}
Alors il existe un unique $y$ dans $H$ tel que pour tout x de H on ait $f(x) = <y, x>$
$$
\exists\,!\ y \in H\,, \quad \forall x\in H\,, \quad f(x) = \langle y,x\rangle
$$
\underline{Extension aux formes bilin\'eaires}\newline
Si a est une forme bilin\'eaire continue sur un espace de Hilbert r\'eel H (ou une forme sesquilin\'eaire complexe continue sur un Hilbert complexe), alors il existe une unique application A de H dans H telle que, pour tout $(u, v) \in H \times H$, on ait $a(u, v) = <Au, v>$. De plus, A est lin\'eaire et continue, de norme \'egale \`a celle de a.
$$
\exists !\,A\in\mathcal{L}(H),\ \forall (u,v)\in H\times H,\ a(u,v)=\langle Au,v \rangle.
$$
Cela r\'esulte imm\'ediatement de l'isomorphisme canonique (isom\'etrique) entre l'espace norm\'e des formes bilin\'eaires continues sur $H \times H$ et celui des applications lin\'eaires continues de H dans son dual, et de l'isomorphisme ci-dessus entre ce dual et H lui-m\^eme.
\bigskip

\textbf{Th\'eor\`eme de Lax-Milgram}\newline
Appliqu\'e \`a certains probl\`emes aux d\'eriv\'ees partielles exprim\'es sous une formulation faible (appel\'ee \'egalement formulation variationnelle). Il est notamment l'un des fondements de la m\'ethode des \'el\'ements finis.
Soient :
\begin{itemize}
\item $\mathcal{H}$ un espace de Hilbert r\'eel ou complexe muni de son produit scalaire not\'e $\langle.,.\rangle$, de norme associ\'ee not\'ee $\|.\|$
\item a(.\, ,\,.) une forme bilin\'eaire (ou une forme sesquilin\'eaire si $\mathcal{H}$ est complexe) qui est
	\begin{itemize}
	\item continue sur $\mathcal{H}\times\mathcal{H} : \exists\,c>0, \forall (u,v)\in \mathcal{H}^2\,,\ |a(u,v)|\leq c\|u\|\|v\|$
	\item coercive sur $\mathcal{H}$ (certains auteurs disent plut\^ot $\mathcal{H}$-elliptique) : $\exists\,\alpha>0, \forall u\in\mathcal{H}\,,\ a(u,u) \geq \alpha\|u\|^2$
	\end{itemize}
\item $L(.)$ une forme lin\'eaire continue sur $\mathcal{H}$
\end{itemize}
Sous ces hypoth\`eses il existe un unique $u$ de $\mathcal{H}$ tel que l'\'equation $a(u,v)=L(v)$ soit v\'erifi\'ee pour tout $v$ de $\mathcal{H}$ :
$$
\quad \exists!\ u \in \mathcal{H},\ \forall v\in\mathcal{H},\quad a(u,v)=L(v)
$$
Si de plus la forme bilin\'eaire a est sym\'etrique, alors  $u$  est l'unique \'el\'ement de $\mathcal{H}$ qui minimise la fonctionnelle $J:\mathcal{H}\rightarrow\R$ d\'efinie par $J(v) = \tfrac{1}{2}a(v,v)-L(v)$ pour tout $v$ de $\mathcal{H}$, c'est-\`a-dire :
$$
\quad \exists!\ u \in \mathcal{H},\quad J(u) = \min_{v\in\mathcal{H}}\ J(v)
$$
\bigskip

$-\laplace $ admet une base de fonctions propres $v_k$, $k \in N$,
orthonormales pour le produit scalaire de $L^2(\Omega)$

\section{Max-plus algebra}
A max-plus algebra is a \textbf{semiring(demi-anneau):(半环是类似于环但没有加法逆元的代数结构)} over the union of real numbers and
$\varepsilon = -\infty$, equipped with maximum and addition as the two binary operations.

\textbf{Scalar operations}\\
Let a and b be real scalars or $\varepsilon$. Then the operations maximum (implied by the max operator  $\oplus$) and addition (plus operator  $\otimes$) for these scalars are defined as
$$ a \oplus b = \max(a,b) $$
$$ a \otimes b = a + b $$
Similar to the conventional algebra, all $\otimes$ - operations have a higher precedence than $\oplus$ - operations.

\textbf{Matrix operations}\\
$$ [A \oplus B]_{ij} = [A]_{ij} \oplus [B]_{ij} = \max([A]_{ij} , [B]_{ij}) $$
$$ [A \otimes B]_{ij} = \bigoplus_{k = 1}^p [A]_{ik} \otimes [B]_{kj} = \max([A]_{i1} + [B]_{1j}, \dots, [A]_{ip} + [B]_{pj}) $$

\textbf{Algebra properties}\\
\begin{itemize}
\item associativity:
	\begin{itemize}
	\item $(a \oplus b) \oplus c = a \oplus (b \oplus c) $
	\item $(a\otimes b) \otimes c = a \otimes (b \otimes c)$
	\end{itemize}
\item commutativity :
	\begin{itemize}
	\item $a \oplus b = b \oplus a $
	\item $a \otimes b = b \otimes a $
	\end{itemize}
\item distributivity:
	\begin{itemize}
	\item  $(a \oplus b) \otimes c = a \otimes c \oplus b \otimes c $
	\end{itemize}
\end{itemize}

\section{SVD}
\begin{theorem}
\textbf{Singular value decomposition}\\
Suppose $A$ is an $m \times n$ matrix whose entries come from the field $K$, which is either the field of real numbers or the field of complex numbers.
Then there exists a factorization of the form
$$
\mathbf{A} = \mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^*
$$
where $U$ is an $m \times m$ unitary matrix over $K$ (orthogonal matrix if $K = R$),
$Σ$ is an $m \times n$ diagonal matrix with non-negative real numbers on the diagonal,
and the $n \times n$ unitary matrix $V^*$ denotes the conjugate transpose of the $n \times n$ unitary matrix $V$
\end{theorem}
\begin{proof}
$A$ is $m \times n \Rightarrow AA^T$ is $m \times m, A^T A$ is $n \times n$\\
Let $\lambda_1, \ldots, \lambda_r$ be the \textbf{nonzero} eigenvalues of $A^T A$, \\
and the corresponding unit eigenvector are $v_1, \ldots, v_r$ with $v_i \in K^n$\\
$r$ is just the rank of $A^T A$, which is also the rank of $A$.
\todo{proof it is also the rank of $A$}
$$A A^T A v_i = A \lambda_i v_i = \lambda_i (A v_i)$$
所以, $Av_i$刚好就是$A A^T$的vector propre, 相应的valeur propre也是$\lambda_i$.
$$\norm{Av_i} = \sqrt{(Av_i)^T Av_i} = \sqrt{v_i^T A^T Av_i} = \sqrt{v_i^T \lambda_i v_i} = \sqrt{\lambda_i} \equiv \sigma_i$$
Let $u_i = \dfrac{Av_i}{\sigma_i}$, 那么 $\norm{u_i} = 1$ and $\u_i \in K^m$

$$
\forall i, j \in \{1,2,\ldots, r\},\
u_i^T Av_j
= (\dfrac{Av_i}{\sigma_i})^T Av_j
= \dfrac{1}{\sigma_i} v_i^T A^T Av_j
= \dfrac{\lambda_j}{\sigma_i} v_i^T v_j
= \dfrac{\lambda_j}{\sigma_i} \delta_{ij}
=
\left\{
  \begin{array}{ll}
    \sigma_i & \si i = j \\
    0 & \sinon
  \end{array}
\right.
$$
将这$r \times r$个方程写成矩阵的形式:
$$
\begin{bmatrix}
u_1^T \\
u_2^T \\
\vdots \\
u_r^T \\
\end{bmatrix}
\cdot
A
\cdot
\begin{bmatrix}
v_1 & v_2 & \cdots & v_r
\end{bmatrix}
=\diag{\sigma_1, \sigma_2, \ldots, \sigma_r}
$$
The $u_i$ are vectors from an $m$-dimensional space, and we only have $r$ of them, so we can pick unit
vectors $u_{r+1}, \ldots, u_m$ that are pairwise orthogonal, and orthogonal to $u_1,\ldots, u_r$. Similarly we can
find $v_{r+1}, \ldots, v_n$ such that $v_1,...,v_n$ are pairwise orthogonal unit vectors. We then still have that
$u_i^T A v_j$ is $\sigma_i$ when $i = j \leq r$ and by a calculation analogous to the above it is
\textbf{zero for all other cases(因为其他的特征值都为0)}.\\
In matrix form this gives us
$$
\begin{bmatrix}
u_1^T \\
u_2^T \\
\vdots \\
u_m^T \\
\end{bmatrix}
\cdot
A
\cdot
\begin{bmatrix}
v_1 & v_2 & \cdots & v_n
\end{bmatrix}
=\Sigma
$$
where $\Sigma$ is now an $m \times n$ matrix with its first $r$ diagonal entries being the
$\sigma_1, \sigma_2, \ldots, \sigma_r$ and zeroes everywhere else.

Defining $U = [u_1, \ldots, u_m]$ and $V = [v_1, \ldots, v_n]$ column vectors.
we now have $U^TAV = \Sigma$\\
and as orthogonal matrix, $U^{-1} = U^T, V^{-1} = V^T$
$$ A = U \Sigma V^T $$
This is the famous singular value decomposition, and you have just seen a complete proof of its existence for an arbitrary $m \times n$ matrix $A$.
\end{proof}

\begin{remark}
\textbf{econonmy version of the SVD}\\
$$ A = U_r \Sigma_r V_r^T $$
\end{remark}

\begin{remark}
The $\sigma_1, \ldots, \sigma_r$ which are just the square roots of the eigenvalues of $A A^T$ or ($A^T A$), are called the singular values of A.
The columns of $U$, which are just the eigenvectors of $A A^T$, are called \textbf{left singular vectors of $A$},
and the columns of $V$ , which are just the eigenvectors of $A^T A$, are called the \textbf{right singular vectors of $A$}
\end{remark}

Any arbitrary matrix $X$ can be converted to an orthogonal matrix, a diagonal matrix and another orthogonal matrix
(or a rotation, a stretch and a second rotation)

\section{Elliptic curve 椭圆曲线}
En mathématiques, une courbe elliptique est un cas particulier de courbe algébrique,
Contrairement à ce que son nom pourrait laisser croire, l'ellipse n'est pas une courbe elliptique.

L'équation d'une courbe elliptique définie sur le corps des nombres réels peut être mise sous la forme plus simple (dite équation de Weierstrass) :
$$ y^2 = x^3 + ax + b $$
où les coefficients a, b sont des nombres réels.
其是无奇点的,亦即,其图形没有尖点或自相交
Selon le choix de ces coefficients, les graphes correspondants ont essentiellement deux \href{http://upload.wikimedia.org/wikipedia/commons/d/d0/ECClines-3.svg}{formes possibles}.

Les courbes représentées ont une tangente bien définie en chaque point, n'ont ni point double, ni point de rebroussement.
Algébriquement, ceci se traduit par le fait que le discriminant de la courbe ne s'annule pas.
$$ \Delta = -16(4a^3 + 27b^2), $$
Un discriminant différent de zéro indique une courbe sans singularités (ou encore courbe non-singulière).
Le facteur -16 peut para\^itre inutile à ce stade mais il intervient dans l'étude plus avancée des courbes elliptiques.

椭圆曲线的"+" 运算
\href{http://upload.wikimedia.org/wikipedia/commons/thumb/c/c1/ECClines.svg/680px-ECClines.svg.png}{图示}

上面的群可以用代数方式定义.给定域$K$(其中$K$的特征值非$2$或者$3$)上的曲线$E: y^2 = x^3 - px - q$,及非无穷远点$P(x_P,y_P), Q(x_Q, y_Q) \in E$.
定义$R = P+Q$:
$$
\begin{aligned}
& x_R = s^2 - x_P - x_Q \\
& y_R = -y_P + s(x_P - x_R)
\end{aligned}
$$
若
$$
s =
\left\{
  \begin{array}{ll}
	\frac{y_P - y_Q}{x_P - x_Q} &  \si x_P \ne x_Q \et x_P \neq x_Q \\
	\frac{3{x_P}^2 - p}{2y_P} & \si P = Q
  \end{array}
\right.
$$

若$x_P = x_Q \et y_P = -y_Q, P+Q = 0$

\begin{theorem}
L'ensemble des points à coordonnées réelles de la courbe (en incluant le point à l'infini), muni de cette loi de composition, forme un groupe commutatif.
\end{theorem}
\end{document}
