% !Mode:: "TeX:UTF-8"
\documentclass{book}
\input{../public/package}
\input{../public/book}
\input{../public/math}

\begin{document}
\title{Alg\`ebre}
\author{Eric Wang}

\maketitle
\tableofcontents
\newpage

%############################################################## Algebre 1 第一章 #########################################################################
\chapter{Logique et ensembles}
La relation $1+1 \neq 2$ est fausse,la double n\'egation est la relation $1+1=2$. Elle est vraie.
\newline
disjonction 或,析取 \newline
conjonction 和,合取 \newline
axiome 公理 \newline
conjecture 猜想\newline
le quantificateur existentiel $\exists$ \newline
le quantificateur universel $\forall$ \newline
\\
Un th\'eor\`eme est simplement une phrase vraie. \newline

%当且仅当
P si et seulement si Q: \newline
P si Q, c'est si Q, alors P.\quad $Q \Rightarrow P$ \newline
P seulement si Q: c'est si P, alors Q. \quad $P \Rightarrow Q$ \\
$P \Rightarrow (Q \Rightarrow R)\Leftrightarrow (P \Rightarrow Q)\rightarrow (P \Rightarrow R)$ \newline
La diff\'erence sym\'etrique de deux ensembles $E,F$.
C'est l'ensemble des \'el\'ements qui appartient \^a exactement un des ensembles $E,F$. On le note $E \Delta F$. \\
$
E \Delta F=\{x|x\in E ~\mathrm{et} ~x \not \in F\}\cup\{x|x\in F et x \not \in E\}=(E-F)\cup(F-E)
$

Une application est la m\^eme chose qu'une famille \`a valeurs dans $F$ index\'ee par $E$.
Si on note ${\cal F}(E,F)$ l'ensemble de toutes les applications de $E$ vers $F$, on a bien s\^ur
$$
{\cal F}(E,F)=F^E
$$

Si $E$ , $F$ sont finis, alors ${\cal F}(E,F)$ et $F^E$ sont finis, et
$$
|{\cal F}(E,F)|=|F^E|=|F|^|E|
$$

%############################################################## Algebre 1 第二章 #########################################################################
\chapter{Groupes. Anneaux. Corps}
$$
\begin{array}{l|llll}
groupe & ajouter & soustraire &&\\
\hline
anneau & ajouter & soustraire & multiplication & \\
\hline
corps & ajouter & soustraire & multiplication & division
\end{array}
$$


Si A est un anneau, on appelle  {\bf groupe des inversibles de} $A$ l'ensemble des \'el\'ements inversibles de $A$, muni de la deuxi\`eme op\'eration de $A$.
On note ce groupe $A^o$

%############################################################## Algebre 1 第三章 #########################################################################
\chapter{Espace vectoriel}
\begin{theorem}
  On a une d\'ecomposition en somme directe de deux sous-espaces $ V = W_1 \bigoplus W_2$  si et seulment si
  $$
    \begin{cases}
    V=W_1+W_2 \\
    W_1\cap W_2=\{0\}
    \end{cases}
  $$
\end{theorem}
\begin{attention}
  Si le nombre de sous-espaces est $n\geqslant 3$, on peut plus dire que
  $$
    \begin{cases}
    V=W_1+\dots+W_n \\
    \forall i \neq j,W_i\cap W_j=\{0\}
    \end{cases}
  $$
  implique $ V = W_1 \bigoplus \dots \bigoplus W_n$
\end{attention}

\begin{theorem}
  Soit $f:V \rightarrow W$ une application lin\'eaire. Soit $w \in W$ un vecteur donn\'e. On note
  $$
  S=\{x \in V|f(x)=w\}
  $$
  Alors $S$ est soit l'ensemble vide, soit un ensemble de la forme $\{x_0+v|v \in ker f\}$, o\`u $x_0$ est une solution particuli\`ere de l'\'equation $f(x_0)=w$.
\end{theorem}
\begin{note}
  想一想解线性微分方程的时候,我们也用到了这种思想,先找通解,然后再找一个特解,组合在一起,就构成了微分方程的完整解.
\end{note}

\begin{theorem}

\begin{enumerate}
\item Soit $V$ un espace vectoriel de dimension finie $n$ sur le corps commutatif $K$.  Alors il existe un isomorphisme de $K^n$ vers $V$.
\item S'il existe un isomorphisme de $K^n$ vers $V$, alors $V$ est de dimension $n$.
\item Soient $V$ et $W$ deux espaces vectoriels isomorphes (il existe un isomorphisme de $V$ vers $W$ et un isomorphisme de $W$ vers $V$).
	On suppose que l'un des deux espaces est de dimension finie. Alors $V$ et $W$ ont la m\^eme dimension.
\item Si $V,W$ sont de dimension finie, et $\dim V = \dim W$, alors $V$ et $W$ sont isomorphes.
\end{enumerate}
\end{theorem}

\begin{proof}

1. Soit $(e_1,\dots, e_n)$ une base de $V$. On d\'efinit l'application $u : K^n \to V$ comme suit:
$$
u(\lambda_1,\dots ,\lambda_n) = \sum_{i=1}^n \lambda_i \cdot e_i
$$
Il est \'evident que $u$ est une application lin\'eaire. Comme $(e_i)$ est une famille g\'en\'eratrice de $V$, il est
\'egalement clair que $u$ est surjective. Reste \`a montrer l'injectivit\'e de $u$.
\medskip

Par lin\'earit\'e de $u$, il suffit de montrer que si $u(\lambda_1,\dots ,\lambda_n) = 0_V$, alors
tous les $\lambda_i$ sont nuls. Or cela est vrai, puisque la famille $(e_i)$ est libre.
Comme $u$ est une application lin\'eaire surjective et injective, il s'agit d'un isomorphisme d'espaces vectoriels.
\medskip

2. Soit $u: K^n \to V$ un isomorphisme. On consid\`ere une base $(e_1,\dots ,e_n)$ de $K^n$ (par exemple la
base canonique). On regarde maintenant la famille
$$
(f_1,\dots ,f_n) = (u(e_1), \dots, u(e_n))
$$
Pour montrer que $V$ est de dimension $n$, il suffit de se convaincre que la famille $(f_i)$ est une base de $V$.
Montrons d'abord qu'elle est libre.
\medskip

Si $\sum_{i=1}^n \lambda_i \cdot f_i = \sum_{i=1}^n \lambda_i \cdot u(e_i) =0_V$, alors par lin\'earit\'e de $u$ on aura
$$
u\left( \sum_{i=1}^n \lambda_i \cdot e_i \right) = 0_V.
$$
Or $u$ est un isomorphisme, donc en particulier c'est une injection. Et comme $u(0)=0$, on doit avoir
$$
 \sum_{i=1}^n \lambda_i \cdot e_i  = 0
$$
Mais la famille $(e_i)$ est en particulier une famille libre, donc tous les $\lambda_i$ sont nuls.
Nous avons prouv?que la famille $(f_i)$ est libre.
\medskip

Montrons maintenant que la famille $(f_i)$ est g\'en\'eratrice. Soit $v$ vecteur quelconque de $V$. Puisque
$u$ est une surjection, il existe un vecteur $t \in K^n$ avec $u(t)=v$. Comme $(e_i)$ est une famille g\'en\'eratrice
de $K^n$, il existe des scalaires $(\lambda_1, \dots, \lambda_n)$ avec
$$
t = \sum_{i=1}^n \lambda_i \cdot e_i
$$
Mais alors
$$
v= u(t) = u\left(  \sum_{i=1}^n \lambda_i \cdot e_i \right) = \sum_{i=1}^n \lambda_i \cdot u(e_i)
= \sum_{i=1}^n \lambda_i f_i
$$
Nous avons montr\'e que $(f_i)$ est bien une famille g\'en\'eratrice. Cette famille est bien une base de $V$.
\bigskip

3. C'est une cons\'equence imm\'ediate de 1. et 2.
\bigskip

4. Soit $n$ la dimension commune de $V$ et $W$. Par 1. les deux espaces vectoriels sont alors isomorphes
\`a l'espace vectoriel $K^n$. Puisque la composition de deux isomorphismes est encore un isomorphisme, on en d\'eduit
que $V$ et $W$ sont isomorphes.
\end{proof}

\textbf{Espace m\'etrique}
On appelle $(E, d)$ un espace m\'etrique si $ E$  est un ensemble et d une distance sur $E$ .

\textbf{Espace complet}
Un espace m\'etrique $ M$  est dit complet si toute suite de Cauchy de $ M$  a une limite dans $ M$  (c'est-\`a-dire qu'elle converge dans $M$ ).\newline
Intuitivement, un espace est complet s'il n'a pas de trou, s'il n'a aucun point manquant. \newline
Par exemple, les nombres rationnels ne forment pas un espace complet,
puisque $\sqrt{2}$ n'y figure pas alors qu'il existe une suite de Cauchy de nombres rationnels ayant cette limite.

Il est toujours possible de remplir les trous amenant ainsi \`a la compl\'etion d'un espace donn\'e.

\textbf{Espace euclidien}
il est d\'efini par la donn\'ee d'un espace vectoriel sur le corps des r\'eels, de dimension finie, muni d'un produit scalaire,
qui permet de mesurer distances et angles.

\textbf{Espace hermitien}
En math\'ematiques, un espace hermitien est un espace vectoriel sur le corps commutatif des complexes de dimension finie et muni d'un produit scalaire.

La g\'eom\'etrie d'un tel espace est analogue \`a celle d'un espace euclidien

Une forme hermitienne est une application d\'efini\'e sur $E \times E$ \`a valeur dans $\mathbf{C}$ not\'ee $\langle .,.\rangle$, telle que :
\begin{itemize}
		\item pour tout y fix\'e l'application $x \mapsto \langle x,y\rangle $est $\mathbf{C}$-lin\'eaire et
		\item $\forall x,y \in E$,$\langle x,y\rangle=\overline{ \langle y,x\rangle}$.
\end{itemize}
En particulier, $\langle x,x\rangle$ est r\'eel, et $x\mapsto \langle x,x\rangle$ est une forme quadratique sur E vu comme $\mathbf{R}$-espace vectoriel.

\textbf{Espace pr\'ehilbertien}
En math\'ematiques, un espace pr\'ehilbertien est d\'efini comme un espace vectoriel r\'eel ou complexe muni d'un produit scalaire

Un espace pr\'ehilbertien $(E,\langle\cdot,\cdot\rangle)$ est alors un espace vectoriel E muni d'un produit scalaire $\langle\cdot,\cdot\rangle$.

\textbf{Espace de Hilbert}
C'est un espace pr\'ehilbertien complet, c'est-\`a-dire un espace de Banach dont
la norme $\parallel\bullet\parallel$ d\'ecoule d'un produit scalaire ou hermitien $\langle\cdot,\cdot\rangle$ par la formule
$$\parallel x\parallel = \sqrt{\langle x,x \rangle}$$
C'est la g\'en\'eralisation en dimension quelconque d'un espace euclidien ou hermitien.
\bigskip

\textbf{Application hermitien's eigenvalues are real values}.
\begin{proof}
Soient A une matrice autoadjointe (r\'eelle ou complexe), $\lambda$ une racine de son polyn\^ome caract\'eristique (il en existe au moins une,
mais a priori complexe), et $X$ une matrice colonne complexe non nulle telle que
$AX=\lambda X$. Alors
$$ \overline\lambda X^*.X=(AX)^*.X=X^*.A^*.X=X^*.A.X=\lambda X^*.X $$
or $X*.X$ est non nul, donc $\lambda$ est r\'eel.
\end{proof}

\begin{theorem}[Th\'eor\`eme spectral en dimension finie, pour les endomorphismes]
Tout endomorphisme auto-adjoint d'un espace euclidien ou hermitien est diagonalisable dans une base orthonormale et ses valeurs propres sont toutes r\'eelles.
\end{theorem}

\begin{theorem}[Th\'eor\`eme spectral pour les matrices]
Soit $A$ une matrice sym\'etrique r\'eelle(resp. hermitienne complexe),
alors il existe une matrice $P$ orthogonale (resp. unitaire) et une matrice $D$ diagonale dont tous les coefficients sont r\'eels,
telles que la matrice $A = P.D.P^{-1}$
\end{theorem}

\begin{theorem}[Diagonalisation d'un endomorphisme autoadjoint et d'une matrice autoadjointe]
Un endomorphisme d'un espace euclidien ou hermitien est autoadjoint
si et seulement s'il existe une base orthonormale de vecteurs propres, avec valeurs propres toutes r\'eelles.\\
Une matrice carr\'ee complexe $A$ est autoadjointe si et seulement s'il existe une matrice unitaire $U$ telle que $U.A.U^{-1}$ soit diagonale et r\'eelle.\\
Une matrice carr\'ee r\'eelle $A$ est \textbf{sym\'etrique} si et seulement s'il existe une \textbf{matrice orthogonale} $P$
telle que $P.A.P^{-1}$ soit diagonale et r\'eelle.
\end{theorem}
Rappel:
Une matrice carr\'ee $A$(n lignes, n colonnes) \`a coefficients r\'eels est dite \textbf{orthogonale}正交矩阵 si $A^t A = I_n$, c'est-\`a-dire que $A^t = A^{-1}$

\textbf{Th\'eor\`eme de Riesz (Fr\'echet-Riesz)}\newline
un th\'eor\`eme qui repr\'esente les \'el\'ements du dual d'un espace de Hilbert comme produit scalaire par un vecteur de l'espace.
Soient :
\begin{itemize}
	\item H un espace de Hilbert (r\'eel ou complexe) muni de son produit scalaire not\'e $<.,.>$
	\item f in H' une forme lin\'eaire continue sur H.
\end{itemize}
Alors il existe un unique $y$ dans $H$ tel que pour tout x de H on ait $f(x) = <y, x>$
$$
\exists\,!\ y \in H\,, \quad \forall x\in H\,, \quad f(x) = \langle y,x\rangle
$$
\underline{Extension aux formes bilin\'eaires}\newline
Si $a$ est une forme bilin\'eaire continu\'e sur un espace de Hilbert r\'eel $\mathcal{H}$ (ou une forme sesquilin\'eaire complexe continue sur un Hilbert complexe),
alors il existe une unique application $A$ de $\mathcal{H}$ dans $\mathcal{H}$ telle que,
pour tout $(u, v) \in \mathcal{H} \times \mathcal{H}$, on ait $a(u, v) = <Au, v>$.
De plus, A est lin\'eaire et continue, de norme \'egale \`a celle de a.
$$ \exists !\,A\in\mathcal{L}(H),\ \forall (u,v)\in H\times H,\ a(u,v)=\langle Au,v \rangle $$
Cela r\'esulte imm\'ediatement de l'isomorphisme canonique(isom\'etrique) entre l'espace norm\'e des formes bilin\'eaires continues sur
$\mathcal{H} \times \mathcal{H}$ et celui des applications lin\'eaires continues de H dans son dual, et de l'isomorphisme ci-dessus entre ce dual et H lui-m\^eme.
\bigskip

\textbf{Th\'eor\`eme de Lax-Milgram}\newline
Appliqu\'e \`a certains probl\`emes aux d\'eriv\'ees partielles exprim\'es sous une formulation faible (appel\'ee \'egalement formulation variationnelle).
Il est notamment l'un des fondements de la m\'ethode des \'el\'ements finis.
Soient :
\begin{itemize}
\item $\mathcal{H}$ un espace de Hilbert r\'eel ou complexe muni de son produit scalaire not\'e $\langle.,.\rangle$, de norme associ\'ee not\'ee $\|.\|$
\item a(.\, ,\,.) une forme bilin\'eaire (ou une forme sesquilin\'eaire si $\mathcal{H}$ est complexe) qui est
	\begin{itemize}
	\item continue sur $\mathcal{H}\times\mathcal{H} : \exists\,c>0, \forall (u,v)\in \mathcal{H}^2\,,\ |a(u,v)|\leq c\|u\|\|v\|$
	\item coercive sur $\mathcal{H}$ (certains auteurs disent plut\^ot $\mathcal{H}$-elliptique):
		$\exists\,\alpha>0, \forall u\in\mathcal{H}\,,\ a(u,u) \geq \alpha\|u\|^2$
	\end{itemize}
\item $L(.)$ une forme lin\'eaire continue sur $\mathcal{H}$
\end{itemize}
Sous ces hypoth\`eses il existe un unique $u$ de $\mathcal{H}$ tel que l'\'equation $a(u,v)=L(v)$ soit v\'erifi\'ee pour tout $v$ de $\mathcal{H}$ :
$$
\quad \exists!\ u \in \mathcal{H},\ \forall v\in\mathcal{H},\quad a(u,v)=L(v)
$$
Si de plus la forme bilin\'eaire a est sym\'etrique, alors $u$ est l'unique \'el\'ement de $\mathcal{H}$ qui
minimise la fonctionnelle $J:\mathcal{H}\rightarrow\R$ d\'efinie par $J(v) = \tfrac{1}{2}a(v,v)-L(v)$ pour tout $v$ de $\mathcal{H}$, c'est-\`a-dire :
$$
\quad \exists!\ u \in \mathcal{H},\quad J(u) = \min_{v\in\mathcal{H}}\ J(v)
$$
\bigskip

$-\laplace $ admet une base de fonctions propres $v_k$, $k \in N$,
orthonormales pour le produit scalaire de $L^2(\Omega)$

%############################################################## Algebre 2 第一章 #########################################################################
\chapter{Matrices}
\begin{definition}
  Soient $V,W$ deux espaces vectoriels sur un corps communitatif $K$ avec dim$V=n$,dim$W=p$,\newline
  soit $e=(e_1,\ldots,e_n)$ une base de $V$, et $f=(f_1,\ldots,f_n)$ une base de $W$. \newline
  On appelle {\bf matrice associ\'ee \`a $u$ dans les bases} $e,f$ et on note ${\cal M}_f^e(u)$ la matrice $p \times n$ d\'efinie par la r\`egle:
  $\forall j \in [[1,n]], u(e_j)= \sum_{i=1}^p ({\cal M}_f^e(u))_{ij} \cdot f_i $. Autrement dit,
  $\forall i \in [[1,p]],\forall j \in [[1,n]]$, le scalaire $({\cal M}_f^e(u))_{ij}$ est la i-\`eme coordonn\'ee du vecteur $u(e_j) \in W $ dans la base $f$.
\end{definition}
\begin{note}
  Les {\bf coordonn\'ees de $u(e_j)$ dans la base $f$} sont les coeficients qui se trouvent dans {\bf la colonne num\'er\'e $j$ de la matrice}
  ${\cal M}_f^e(u)$.
\end{note}

\begin{theorem}
  $ \forall v \in V, {\cal C}_f(u(v))= {\cal M}_f^e(u)\cdot {\cal C}_e(v) $
\end{theorem}

\begin{note}
 $ {\cal M}_f^e(u)\cdot {\cal P}_{e\rightarrow e'}= {\cal M}_f^e(u)\cdot {\cal M}_e^{e'}(id_V)={\cal M}_f^{e'}(u)$
\end{note}

\section{矩阵的运算}
$$
A \times B = 
\begin{bmatrix}
	A_1 & A_2
\end{bmatrix}
\times
\begin{bmatrix}
	B_1 \\
	B_2
\end{bmatrix}
=
A_1 \times B_1 + A_2 \times B_2
$$
$B$对$A$进行列变换, $A$对$B$进行行变换.

\subsection{列向量组}
若$A$的列向量组为$\alpha_1,\ldots,\alpha_n$,那么我们可以把$A$写成:$A=(\alpha_1,\ldots,\alpha_n)$ \newline
设$A=(a_{ij})_{s\times n},B=(b_{ij})_{n\times m},A$ 的列向量组为$\alpha_1,\ldots,\alpha_n$. \newline
$AB$的第$j\in [[1,m]]$列为: \newline
$$
\begin{bmatrix}
  a_{11}b_{1j}+a_{12}b_{2j}+\dots+a_{1n}b_{nj}  \\
  a_{21}b_{1j}+a_{22}b_{2j}+\dots+a_{2n}b_{nj}  \\
  \vdots \\
  a_{s1}b_{1j}+a_{s2}b_{2j}+\dots+a_{sn}b_{nj}
\end{bmatrix}
\\
=
b_{1j}\alpha_1+b_{2j}\alpha_2+\dots+b_{nj}\alpha_n
$$
于是
$$
AB=(\alpha_1,\ldots,\alpha_n)
\begin{bmatrix}
  b_{11} & b_{12} & \dots & b_{1m}  \\
  b_{21} & b_{22} & \dots & b_{2m}  \\
  \vdots & \vdots & \vdots &\vdots \\
  b_{n1} & b_{n2} & \dots & b_{nm}
\end{bmatrix}
%=(b_{11}\alpha_1+b_{21}\alpha_2+\dots+b_{n1}\alpha_n,b_{12}\alpha_1+b_{22}\alpha_2+\dots+b_{n2}\alpha_n,\ldots,b_{1m}\alpha_1+b_{2m}\alpha_2+\dots+b_{nm}\alpha_n,)
%上面的这一行显示不全,所以挪到下面了
$$
$=(b_{11}\alpha_1+b_{21}\alpha_2+\dots+b_{n1}\alpha_n,b_{12}\alpha_1+b_{22}\alpha_2+\dots+b_{n2}\alpha_n,\ldots,b_{1m}\alpha_1+b_{2m}\alpha_2+\dots+b_{nm}\alpha_n,)$
\begin{note}
B对A进行列变换 \newline
A乘以B可以看做把A的列向量组分别与B的每一列的对应元素的成绩之和作为AB的相应的列向量.
\end{note}

\subsection{行向量组}
设B的行向量组为$r_1,r_2,\ldots,r_n$,则
$$
AB=
\begin{bmatrix}
  a_{11} & a_{12} & \dots & a_{1n}  \\
  a_{21} & a_{22} & \dots & a_{2n}  \\
  \vdots & \vdots & \vdots &\vdots \\
  a_{s1} & a_{s2} & \dots & a_{sn}
\end{bmatrix}
\cdot
\begin{bmatrix}
  r_1 \\
  r_2 \\
  \vdots \\
  r_n
\end{bmatrix}
=
\begin{bmatrix}
  a_{11}r_1 + a_{12}r_2 + \dots + a_{1n}r_n  \\
  a_{21}r_1 + a_{22}r_2 + \dots + a_{2n}r_n   \\
  \vdots \\
  a_{s1}r_1 + a_{s2}r_2 + \dots + a_{sn}r_n
\end{bmatrix}
$$
$A\times B$可以看做把A的每一行元素与B的行向量组的对应的行向量的成绩作为AB相应的行向量.

\begin{question}
  设A为数域K上$s \times n$的矩阵,证明:如果对于$K^n$中任一列向量$\eta$,都有$A\eta=0$,那么$A=0$
\end{question}
\begin{proof}
  设$u:x \in K^n \rightarrow Ax \in K^s$,且$A=(\alpha_1,\ldots,\alpha_n)$ \newline
  我们求出u在base canonique下的矩阵 \newline
  $$
  u(\begin{bmatrix}
    1 \\
    0\\
    \vdots \\
    0
  \end{bmatrix})
  =A \cdot
  \begin{bmatrix}
    1 \\
    0\\
    \vdots \\
    0
  \end{bmatrix}
  =\alpha_1=0
  $$
  同理,我们可以得到$\alpha_2=\dots=\alpha_n=0$,所以$A=0$ \newline
  或者,我们可以直接写
  $$
  A=AI=A(\epsilon_1,\ldots,\epsilon_n)=(A\epsilon_1,\dots,A\epsilon_n)=(0,\ldots,0)=0
  $$
\end{proof}

\section{特殊矩阵}
\subsection{对角矩阵}
la matrice diagonale,$diag(d_1,\ldots,d_n)$ \newline
用一个对角矩阵{\bf 左(右) }乘一个矩阵A,就相当于用对角矩阵的主对角元素分别去乘A相应的{\bf 行(列)}.
%对角矩阵左乘一个矩阵
$$
\begin{bmatrix}
  d_1 & 0  & 0 & \cdots & 0 \\
  0   & d_2 & 0 & \cdots & 0 \\
  \vdots & \vdots & \ddots & \vdots \\
  0 & 0 & 0 & \cdots & d_s
\end{bmatrix}
\times
\begin{bmatrix}
  r_1  \\
  r_2 \\
  \vdots \\
  r_s \\
\end{bmatrix}
=
\begin{bmatrix}
  d_1 r_1  \\
  d_2 r_2 \\
  \vdots \\
  d_s r_s \\
\end{bmatrix}
$$

%对角矩阵右乘一个矩阵
$$
(\alpha_1,\ldots,\alpha_n)
\times
\begin{bmatrix}
  d_1 & 0  & 0 & \cdots & 0 \\
  0   & d_2 & 0 & \cdots & 0 \\
  \vdots & \vdots & \ddots & \vdots \\
  0 & 0 & 0 & \cdots & d_n
\end{bmatrix}
=
\begin{bmatrix}
  d_1 \alpha_1 & d_2 \alpha_2 & \dots & d_n \alpha_n \\
\end{bmatrix}
$$

特别的,两个n级对角矩阵的乘积还是n级对角矩阵,并且是把相应的主对角元相乘

\subsection{三角矩阵}
\noindent
上三角矩阵(上面的三角不为0,而下面的三角为零的矩阵) \newline
下三角矩阵(下面的三角不为0,而上面的三角为零的矩阵)
\begin{note}
  用初等矩阵左(右)乘一个矩阵A,就相当于对A进行了一次相应的初等行(列)变换
\end{note}

\subsection{基本矩阵}
只有一个元素为1,其余元素均为0的矩阵, (i,j)元为1的基本矩阵记为$E_{ij}$
\begin{itemize}
\item 任意一个矩阵A都可以记为$A=\sum_{i=1}^s {\sum_{j=1}^n {a_{ij}E_{ij}}}$
\item 用$E_{ij}$左乘一个矩阵A,相当于把A的第j行搬到第i行的位置,然后乘积所得矩阵的其余行均为0
\item 用$E_{ij}$右乘一个矩阵A,相当于把A的第i列搬到第j列的位置,然后乘积所得矩阵的其余列均为0
\end{itemize}

矩阵$A$的行向量组为$r_1, r_2, \ldots, r_s$, 列向量组为$c_1, c_2, \ldots, c_n$, 则
$$
E_{ij} \times A
=
\begin{bmatrix}
	0 & 0 & \cdots & 0 & 0 \\
	0 & 0 & \cdots & 0 & 0 \\
	0 & 0 & \cdots & 1_{ij} & 0\\
	0 & 0 & \cdots & 0 & 0\\
\end{bmatrix}
\times
\begin{bmatrix}
	r_1 \\
	r_2 \\
	\vdots \\
	r_s \\
\end{bmatrix}
=
\begin{bmatrix}
	0 \\
	\vdots \\
	0_{i - 1} \\
	r_j \\
	0_{i + 1} \\
	\vdots \\
	0 \\
\end{bmatrix}
$$

$$
A \times E_{ij}
=
\begin{bmatrix}
	c_1 & c_2 & \cdots & c_n
\end{bmatrix}
\times
\begin{bmatrix}
	0 & 0 & \cdots & 0 & 0 \\
	0 & 0 & \cdots & 0 & 0 \\
	0 & 0 & \cdots & 1_{ij} & 0\\
	0 & 0 & \cdots & 0 & 0\\
\end{bmatrix}
=
\begin{bmatrix}
	0 & \cdots & 0_{j - 1} & c_i & 0_{j + 1} & \cdots & 0
\end{bmatrix}
$$

$$
E_{ij}E_{kl}=
\begin{cases}
  E_{il} & \si k=j \\
  0 & \si k \neq j
\end{cases}
$$

\begin{question}
  循环移位矩阵:
  $$
  C=
  \begin{bmatrix}
    \epsilon_n & \epsilon_1 & \epsilon_2 & \epsilon_3 & \dots & \epsilon_{n-2} & \epsilon_{n-1} \\
    \hline
        0      &     1      &     0      &       0    & \dots &      0         &       0        \\
        0      &     0      &     1      &       0    & \dots &      0         &       0        \\
        0      &     0      &     0      &       1    & \dots &      0         &       0        \\
        \vdots &    \vdots  &     \vdots &     \vdots & \ddots&      \vdots    &       \vdots   \\
        0      &     0      &     0      &       0    & \dots &      0         &       1        \\
        1      &     0      &     0      &       0    & \dots &      0         &       0
  \end{bmatrix}
  $$
  (1)用C左乘A,相当于把A 的行向上移一行,第一行换到最后一行 \newline
  用C右乘B,相当于把B 的列向右移一列,最后一列换到第一列 \newline
  (2)$\sum_{l=0}^{n-1} C^l=J$,其中J 为元素全为1 的n 级矩阵
\end{question}
\begin{proof}[Demontration(2)]
  $\newline C=(\epsilon_n,\epsilon_1 , \epsilon_2 ,\epsilon_3 , \dots , \epsilon_{n-2}, \epsilon_{n-1} ) \\
    C^2=( \epsilon_{n-1},\epsilon_n,\epsilon_1 , \epsilon_2 ,\epsilon_3 , \dots , \epsilon_{n-2}  ) \\
    C^3=(\epsilon_{n-2}, \epsilon_{n-1},\epsilon_n,\epsilon_1 , \epsilon_2 ,\epsilon_3 , \dots , \epsilon_{n-3}  ) \\
    until \\
    C^{n-1}=(\epsilon_2 ,\epsilon_3 , \dots , \epsilon_n,\epsilon_1 ) \\
    \sum_{l=0}^{n-1} C^l=
    (\epsilon_n,\epsilon_1 , \epsilon_2 , \dots , \epsilon_{n-2}, \epsilon_{n-1} ) +
    ( \epsilon_{n-1},\epsilon_n,\epsilon_1 , \dots , \epsilon_{n-2}  ) +
    (\epsilon_{n-2}, \epsilon_{n-1},\epsilon_n,\epsilon_1 , \dots , \epsilon_{n-3}  ) +
    (\epsilon_2 ,\epsilon_3 , \dots , \epsilon_n,\epsilon_1 ) \\
    =(\epsilon_1+\epsilon_n+\epsilon_{n-1}+\dots+\epsilon_1,\epsilon_2+\epsilon_1+\epsilon_n+\dots+\epsilon_3,\dots,\epsilon_n+\epsilon_{n-1}+\dots+\epsilon_1)\\
    =
    \begin{bmatrix}
      1 & 1 & 1 & \cdots & 1 \\
      1 & 1 & 1 & \cdots & 1 \\
      \vdots &\vdots &\vdots &\vdots &\vdots \\
      1 & 1 & 1 & \cdots & 1 \\
    \end{bmatrix}
    =J$
\end{proof}
\section{Rang 秩}
$$rang(AB)\leqslant rang(A)$$
$$rang(A+B)\leqslant rang(A) + rang(B)$$
$$\si k \neq 0, rang(kA) = rang(A)$$

\section{可逆矩阵}
\begin{itemize}
\item Si $A$ inversible, alors $A^t$ inversible et $(A^t)^{-1} = (A^{-1})^t$
$$
A \cdot A^{-1} = 1
\Rightarrow (A \cdot A^{-1})^t = 1^t = 1
\Rightarrow (A^{-1})^t \cdot A^t = 1
\Rightarrow (A^t)^{-1} = (A^{-1})^t
$$
\item 用一个可逆矩阵左(右)乘一个矩阵$A$, 不改变$A$的rang.
\end{itemize}

\begin{question}
Si $A$ une matrice carr\'ee et $A^2 = I$, 称$A$为对合矩阵. 设$A, B \in M_{n \times n}(K)$.
证明: 若$A, B$都为对合矩阵, 且 $|A| + |B| = 0$, 那么 $A + B, I + AB$ 都不可逆. \\
\end{question}
\begin{proof}
这里的绝对值符号应该是表示 det 的意思.
$$
A^2 = I
\Rightarrow det(A^2) = det(I) = 1
\Rightarrow (det(A))^2 = 1
\Rightarrow det(A) = +1 \ou -1
$$
不妨设 $det(A) = 1, det(B) = -1$.
$$|A| \cdot |A + B| = |A(A + B)| = |A^2 + AB| = |I + AB|$$
$$|A + B| \cdot |B| = |(A + B)B| = |AB + B^2| = |AB + I|$$
因此 $$|A + B| = |A| \cdot |A + B| = |A + B| \cdot |B| = -|A + B|$$
$$\Rightarrow |A + B| = 0$$
$$\Rightarrow |I + AB| = 0$$
因此$A + B, I + AB$ 都不可逆.
\end{proof}

%############################################################## Algebre 2 第二章 #########################################################################
\chapter{D\'etermiants}

\chapter{Max-plus algebra}
A max-plus algebra is a \textbf{semiring(demi-anneau):(半环是类似于环但没有加法逆元的代数结构)} over the union of real numbers and
$\varepsilon = -\infty$, equipped with maximum and addition as the two binary operations.

\textbf{Scalar operations}\\
Let a and b be real scalars or $\varepsilon$.
Then the operations maximum(implied by the max operator $\oplus$) and addition(plus operator $\otimes$) for these scalars are defined as
$$ a \oplus b = \max(a,b) $$
$$ a \otimes b = a + b $$
Similar to the conventional algebra, all $\otimes$ - operations have a higher precedence than $\oplus$ - operations.

\textbf{Matrix operations}\\
$$ [A \oplus B]_{ij} = [A]_{ij} \oplus [B]_{ij} = \max([A]_{ij} , [B]_{ij}) $$
$$ [A \otimes B]_{ij} = \bigoplus_{k = 1}^p [A]_{ik} \otimes [B]_{kj} = \max([A]_{i1} + [B]_{1j}, \dots, [A]_{ip} + [B]_{pj}) $$

\textbf{Algebra properties}\\
\begin{itemize}
\item associativity:
	\begin{itemize}
	\item $(a \oplus b) \oplus c = a \oplus (b \oplus c) $
	\item $(a\otimes b) \otimes c = a \otimes (b \otimes c)$
	\end{itemize}
\item commutativity :
	\begin{itemize}
	\item $a \oplus b = b \oplus a $
	\item $a \otimes b = b \otimes a $
	\end{itemize}
\item distributivity:
	\begin{itemize}
	\item  $(a \oplus b) \otimes c = a \otimes c \oplus b \otimes c $
	\end{itemize}
\end{itemize}

\chapter{SVD}
\begin{theorem}
\textbf{Singular value decomposition}\\
Suppose $A$ is an $m \times n$ matrix whose entries come from the field $K$, which is either the field of real numbers or the field of complex numbers.
Then there exists a factorization of the form
$$
\mathbf{A} = \mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^*
$$
where $U$ is an $m \times m$ unitary matrix over $K$ (orthogonal matrix if $K = R$),
$Σ$ is an $m \times n$ diagonal matrix with non-negative real numbers on the diagonal,
and the $n \times n$ unitary matrix $V^*$ denotes the conjugate transpose of the $n \times n$ unitary matrix $V$
\end{theorem}
\begin{proof}
$A$ is $m \times n \Rightarrow AA^T$ is $m \times m, A^T A$ is $n \times n$\\
Let $\lambda_1, \ldots, \lambda_r$ be the \textbf{nonzero} eigenvalues of $A^T A$, \\
and the corresponding unit eigenvector are $v_1, \ldots, v_r$ with $v_i \in K^n$\\
$r$ is just the rank of $A^T A$, which is also the rank of $A$.
\todo{proof it is also the rank of $A$}
$$A A^T A v_i = A \lambda_i v_i = \lambda_i (A v_i)$$
所以, $Av_i$刚好就是$A A^T$的vector propre, 相应的valeur propre也是$\lambda_i$.
$$\norm{Av_i} = \sqrt{(Av_i)^T Av_i} = \sqrt{v_i^T A^T Av_i} = \sqrt{v_i^T \lambda_i v_i} = \sqrt{\lambda_i} \equiv \sigma_i$$
Let $u_i = \dfrac{Av_i}{\sigma_i}$, 那么 $\norm{u_i} = 1$ and $\u_i \in K^m$

$$
\forall i, j \in \{1,2,\ldots, r\},\
u_i^T Av_j
= (\dfrac{Av_i}{\sigma_i})^T Av_j
= \dfrac{1}{\sigma_i} v_i^T A^T Av_j
= \dfrac{\lambda_j}{\sigma_i} v_i^T v_j
= \dfrac{\lambda_j}{\sigma_i} \delta_{ij}
=
\left\{
  \begin{array}{ll}
    \sigma_i & \si i = j \\
    0 & \sinon
  \end{array}
\right.
$$
将这$r \times r$个方程写成矩阵的形式:
$$
\begin{bmatrix}
u_1^T \\
u_2^T \\
\vdots \\
u_r^T \\
\end{bmatrix}
\cdot
A
\cdot
\begin{bmatrix}
v_1 & v_2 & \cdots & v_r
\end{bmatrix}
=\diag{\sigma_1, \sigma_2, \ldots, \sigma_r}
$$
The $u_i$ are vectors from an $m$-dimensional space, and we only have $r$ of them, so we can pick unit
vectors $u_{r+1}, \ldots, u_m$ that are pairwise orthogonal, and orthogonal to $u_1,\ldots, u_r$. Similarly we can
find $v_{r+1}, \ldots, v_n$ such that $v_1,...,v_n$ are pairwise orthogonal unit vectors. We then still have that
$u_i^T A v_j$ is $\sigma_i$ when $i = j \leq r$ and by a calculation analogous to the above it is
\textbf{zero for all other cases(因为其他的特征值都为0)}.\\
In matrix form this gives us
$$
\begin{bmatrix}
u_1^T \\
u_2^T \\
\vdots \\
u_m^T \\
\end{bmatrix}
\cdot
A
\cdot
\begin{bmatrix}
v_1 & v_2 & \cdots & v_n
\end{bmatrix}
=\Sigma
$$
where $\Sigma$ is now an $m \times n$ matrix with its first $r$ diagonal entries being the
$\sigma_1, \sigma_2, \ldots, \sigma_r$ and zeroes everywhere else.

Defining $U = [u_1, \ldots, u_m]$ and $V = [v_1, \ldots, v_n]$ column vectors.
we now have $U^TAV = \Sigma$\\
and as orthogonal matrix, $U^{-1} = U^T, V^{-1} = V^T$
$$ A = U \Sigma V^T $$
This is the famous singular value decomposition, and you have just seen a complete proof of its existence for an arbitrary $m \times n$ matrix $A$.
\end{proof}

\begin{remark}
\textbf{econonmy version of the SVD}\\
$$ A = U_r \Sigma_r V_r^T $$
\end{remark}

\begin{remark}
The $\sigma_1, \ldots, \sigma_r$ which are just the square roots of the eigenvalues of $A A^T$ or ($A^T A$), are called the singular values of A.
The columns of $U$, which are just the eigenvectors of $A A^T$, are called \textbf{left singular vectors of $A$},
and the columns of $V$ , which are just the eigenvectors of $A^T A$, are called the \textbf{right singular vectors of $A$}
\end{remark}

Any arbitrary matrix $X$ can be converted to an orthogonal matrix, a diagonal matrix and another orthogonal matrix
(or a rotation, a stretch and a second rotation)
\end{document}

