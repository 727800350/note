% !Mode:: "TeX:UTF-8"
\documentclass{article}
\input{../public/package}
\input{../public/article}
\input{../public/math}
\begin{document}
\title{Data Mining Concepts and Techniques\\
		\& Machine Learning\\
Eric's Notes}
\author{Eric}
\maketitle
\newpage
\tableofcontents
\newpage
\section{引论}
数据挖掘是一个多学科的交叉领域.\\
统计学, 机器学习, 神经网络, 模式识别, 知识库系统, 信息检索, 高性能计算和可视化等学科

丰富的数据类型:
流, 序列, 图, 时间序列, 符号序列, 生物学序列, 空间, 音频, 图像和视频数据

高级数据库系统: 扩充关系的, 面向对象的, 对象-关系的和演绎的模型. 包括空间的, 时间的, 多媒体的, 主动的, 流和传感器的, 科学和工程数据库, 知识库, 办公信息库在内的面向应用的数据库系统百花齐放

\textbf{数据仓库}\\
一种多个异构数据源在单个站点以统一的模式组织的存储, 以支持管理决策.\\
数据仓库技术包含数据清理, 数据集成和联机分析处理OLAP\\
OLAP是以各种分析技术, 具有汇总, 合并和具体记忆从不同的角度观察信息的能力. 尽管OLAP 工具支持多维分析和决策, 但是对于深层次的分析, 仍然需要其他分析工具, 如提供数据分类, 聚类, 离群点/异常检测和刻画数据随时间变化等特征.

OLAP 操作的列子包括下钻(drill-down) 和上卷(roll-up), 允许用户在不同的汇总级别观察数据\\
例如, 可以对按季度汇总的数据下钻, 观察按月汇总的数据. 类似的, 可以对按城市汇总的数据上卷, 观察按国家汇总的数据

\textbf{可挖掘什么类型的模式}\\
特征化与区分, 频繁模式(频繁项集, 频繁子序列, 频繁子结构), 关联和相关性挖掘, 分类和回归, 聚类分析, 离群点分析.\\
数据挖掘功能用于指定数据挖掘任务发现的模式, 一般而言, 这些任务分为: 描述性(descriptive)和预测性(predictive)

\textbf{关联规则}\\
关联规则从一个侧面揭示了事务之间的某种联系.\\
支持度和置信度总是伴随着关联规则存在的,它们是对关联规则的必要的补充\\
对某条关联规则而言,如 $A \rightarrow B (support=30\%, confidence= 60\%)$\\
其中的$support=30\%$是说, 在所有的事务中同时出现A和B的概率.\\
而$confidence=60\%$是说,所有事务中,在出现A的情况下出现B的概率,即条件概率.
$$
Support(X=>Y) = P(X \cap Y)\footnote{$P(A \cap B)$ 表示事务包含集合A 和集合B的并的概率, 不要把它与$P(A or B)$混淆} \eqspace
Confidence(X=>Y) = P(Y | X)
$$
$$Buys(X, "computer") \Rightarrow buys (X,"software") [support = 1\%, confidence = 50\%]$$
这个关联规则涉及单个重复的属性或谓词(buys), 单维关联规则 single-dimensional association rule

置信度揭示了A出现时,B是否一定会出现,如果出现则其大概有多大的可能出现.如果置信度为$100\%$,\\
则说明了A出现时,B一定出现.那么,对这种情况而言,假设A和B是市场上的两种商品,就没有理由不进行捆绑销售了.

用于预测分析的分类和回归\\
导出的模型可以用多种形式表示, 如分类规则(即if-then 规则), 决策树, 数学公式或神经网络\par
离群点分析或异常挖掘\\
数据集中可能包含一些数据对象, 他们与数据的一般行为或模型不一致. 这些数据对象是离群点(outlier).\\
大部分的数据挖掘方法都将离群点视为噪声或异常而丢弃, 然而, 在一些应用中(例如, 欺诈检测), 罕见的事件可能比正常出现的事件更令人感兴趣.

\textbf{机器学习}
\begin{itemize}
\item 监督学习Supervised learning: 基本上是分类的同义词, 回归也属于监督学习\\
	 之所以称为监督学习, 是因为这类算法必须知道预测什么, 即目标变量的分类信息.
\item 无监督学习 unsupervised learning: 基本上是聚类的同义词\\
		将数据集合分成由类似的对象组成的多个类的过程称为聚类; 将寻找描述数据统计值的过程称为\textbf{密度估计}\\
		此外, 无监督学习可以减少数据特征的维度, 以便我们可以使用二维或三维图形直观表示信息
\item 半监督学习 semi-supervised learning, 标记的实例用来学习类模型, 而未标记的实例用来进一步改进类边界
\item 主动学习
\end{itemize}

\textbf{如何选择合适的算法}\\
首先考虑使用机器学习的算法的目的. 如果想要预测目标变量的值, 可以选择监督学习算法, 否则可以选择无监督学习算法.
确定选择监督学习算法之后, 需要进一步确定目标变量的类型, 如果目标变量是离散型, 如"是/否, 1/2/3, A/B/C"等, 则可以选择分类算法;
如果目标是连续型的数值, 则需使用回归算法.

如果不想预测目标变量的值, 可以选择无监督学习算法. 进一步分析是否需要将数据划分为离散的分组. 如果这是唯一的需求, 则使用聚类算法; 
如果还需要估计数据与每个分组的相似程度, 则需使用密度估计算法.

\bigskip
数据挖掘算法的有效性和可伸缩性\par
增量数据挖掘

\section{认识数据}
\subsection{属性类型}
\begin{description}
\item[标称属性nominal attribute] 值是一些符号或事物的名称. 每个值代表某种类别, 编码或状态. 又被称为分类的(categorical) 或枚举的(enumeration). 
		This data is just named rather than numeric, as the term "nominal" indicates. There is no order to it. 
\item[二元属性binary attribute] 一种标称属性, 两个状态, 通常用0和1 或false, true表示\\
对称二元属性: 两种状态具有同等价值并且携带相同的权重\\
非对称二元属性:
\item[序数属性ordinal attribute] 其可能的值之间具有有意义的序或秩排序ranking, 但是相继值之间的差是未知的\\
例如, 成绩可以用A+, A, A-, B+ 等划分, 意味着排序, 但是划分值之间的差别未明确
\item[数值属性numeric attribute] 可度量的量\\
	区间标度属性interval-scaled 用相等的单位尺度度量, 可以比较差, 但是没有真正的零点, 例如日期, 我们可以说两个日期相差多少时间, 但是对于0日期, 没有具体的含义\\
	比率标度属性ratio-scaled: 具有固定零点的数值属性, 也就是说, 我们可以说一个值是另一个值的倍数(或比率). 此外, 这些值是有序的, 因此可以计算值之间的差, 均值, 中位数和众数\\
	开氏温标具有绝对零点\par
标称, 二元和序数属性都是定性的
\item[离散属性和连续属性] (在文献中, 术语数值属性与连续属性通常可以互换地使用)
\end{description}

Interval, ratio, and ordinal variables are also referred to as quantitative,   
while nominal variables are also called qualitative.

\subsection{基本统计描述}
\textbf{中心趋势度量}: 均值(平均值), 加权平均, 截尾均值, 中列数midrange(最大与最小值的平均值), 中位数(中间值), 众数(最常见的值)\\
	正倾斜:\\
	负倾斜
\textbf{数据的散布}: 极差, 四分位数, 四分位数极差, 无数概括和盒图, 方差和标准差

有助于在数据预处理时填补缺失值, 光滑噪声, 识别离群点.\\
数据可视化: 散点图矩阵, 树图\\
数据对象的相似性和相异性\\
最近邻分类, 邻近性度量

极差: $max – min$ \\
\textbf{分位数}quantile: 取自数据分布的每隔一定间隔上的点, 把数据划分成数量上大小基本相等的连贯集合\\
给定数据分布的第$k$ 个q-分位数是值$x$, 使得小于$x$ 的数据值最多为$k/q$, 而大于x的数据值最多为$(q-k)/q$, 其中$k$为整数, 使得$0<k<q$. 我们有$q-1$ 个q-分位数.
2-分位数是一个数据点, 把数据分布划分为高低两半, 对应于中位数\\
4-分位数是3个数据点, 他们把数据分布划分为4 个相等的部分, 使得每部分表示数据分布的四分之一. 通常把他们成为4分位数quartile\\
第一个四分位数记作$Q_1$, 第三个四分位数记为$Q_3$, 四分位数极差$IQR = Q_3 – Q_1$\\
100-分位数通常称为百分位数(percentile)

\textbf{五数概括, 盒图和离群点}\\
识别可疑的离群点的通常规则: 挑选落在第3 个四分位数之上或第1 个四分位数之下至少$1.5 * IQR$ 处的值\par
五数概括(five-number summary)由中位数$Q_2$(second quartile or median), 四分位数$Q_1$(first quartile), $Q_3$(third quartile), 最小和最大值. \\
按次序: $Minimum, Q_1, median, Q_3, maximum$

The five-number summary is a shorthand, and if you're looking at a lot of
datasets very quickly, it can provide you with a quick feel for what the set looks like.

盒图(如图\href{http://i.imgbox.com/E7zYvy7e.jpg}{盒图示例}): 
盒的端点在四分位数上, 盒的长度为IQR($IQR = Q_3-Q_1$); 中位数标在盒内; 盒外的两条线(胡须(whiskers)) 延伸到最小和最大值\\

由于现实数据中总是存在各式各样地"脏数据",也称为"离群点(outliers)",于是为了不因这些少数的离群数据导致整体特征的偏移,将这些离群点单独汇出\\
outliers 判断依据$<Q_1 - 1.5*IQR$ 或者$>Q_3 + 1.5*IQR$

盒图可在$O(nlgn)$时间内计算, 依赖于所要求的质量, 近似盒图可在线性或者子线性时间内计算.

I rarely find boxplots to be useful on their own. If I'm dealing with a single value, I'm
going to get more information out of a histogram. Boxplots become more valuable when
you start stacking bunches of them together, a situation where histograms are going to
be just too busy to be meaningfully examined.

分位数图quantile plot
直方图histogram或频率直方图frequency histogram\\
散点图scatter plot: 确定两个数值变量是否存在联系, 模式或趋势的最有效的图形方法之一\\

\subsection{数据可视化}
\begin{itemize}
\item 基于像素的可视化技术pixel-oriented technique
几何投影可视化技术
	\begin{itemize}
	\item 散点图
	\item 散点图矩阵
    \item 平行坐标parallel coordinates
	\end{itemize}
\item 基于图符icon-based的可视化技术
	\begin{itemize}
	\item 切尔诺夫脸chernoff faces
	\item 人物线条画
	\end{itemize}
\item 层次可视化技术
	\begin{itemize}
	\item 世界中的世界
	\item 树图tree-map
	\end{itemize}
\item 可视化复杂对象和关系
	\begin{itemize}
	\item 标签云tag cloud
	\end{itemize}
\end{itemize}

\subsubsection{Univariate Visualization: Histograms, QQ Plots, Boxplots, and Rank Plots}
The most basic visualizations are applied to univariate data, which consists of one observed variable per unit measured. 
Examples of univariate measurements include the number of
bytes per packet or the number of IP addresses observed over a period.

\textbf{Bar plot}\\
\href{https://www.idlcoyote.com/cg\_tips/barplot\_2.png}{Bar plot example}

In scientific visualization, bar plots are preferred over pie charts.

\textbf{The Quantile-Quantile (QQ) Plot}\\
\href{https://onlinecourses.science.psu.edu/stat414/sites/onlinecourses.science.psu.edu.stat414/files/lesson15/plot\_01.gif}{QQ plot example}
compares the distributions of two variables against each
other.  
a two-dimensional plot

\subsubsection{Bivariate Description}
\textbf{Scatterplots 散点图}\\
The primary challenge when analyzing scatter‐
plots is to identify structure among the noise. Common features in a scatterplot are
clusters, gaps, linear relationships, and outliers

\textbf{Contingency tables列联表}\\
Contingency tables are the preferred visualization when comparing categorical data
against categorical data.
\href{http://upload.wikimedia.org/wikipedia/commons/9/94/Contingency\_table.png}{Contingency table example}

\subsubsection{Multivariate Visualization}
Most multivariate visualizations are built by taking a bivariate visualization and finding a way
to add additional information. The most common approaches include colors or changing icons, plotting multiple images, and using animation.

The most basic approach for multivariate visualization is to overlay multiple datasets
on the same chart, using different tickmarks or colors to indicate the originating dataset.

\bigskip
When picking the colors or symbols to use, keep the following in mind:

\begin{itemize}
\item Don't use yellow; it looks too much like white and is often invisible on printouts and monitors.
\item Choose symbols that are very different from each other. I personally like the open circle, closed circle, triangle, and cross.
\item Choose colors that are far away from each other on the color wheel: red, green, blue, and black are my preferred choices.
\item Avoid complex symbols. Many plotting packages offer a variety of asterisk-like figures that are hard to differentiate.
\item Be consistent with your color and symbol choices, and don't overlap their domains. In other words, don't decide that red is HTTP and triangles are FTP.
\end{itemize}

\textbf{Trellis plots 格图}\\
An alternative to plotting multiple sets on the same chart is to use multiple small plots
next to each other. Commonly called \textbf{trellis plots(格图)}

each pair of variables is a distinct scatterplot. 
Each scatterplot shows the relationship between the pair. 

\href{http://i.imgbox.com/WvbltL7W.png}{trellis plots example}
as this example shows it's very easy to quickly identify that volume and articles seem to have some relationship while everything else looks unrelated.

\bigskip
\textbf{Operationalizing Security Visualization}\\
avoid flash in favor of expressiveness  

Finally, recognize that operational visualization is intended to be processed quickly and repeatedly. 
It's not a showcase for innovative graphic representation. The goal of operational visualization should be to express information quickly and clearly. 
Graphically excessive features like animation, unusual color choices, 
and the like will increase the time it takes to process the image without contributing information.

\subsection{度量数据的相似性与相异性}
相似性和相异性都成为紧邻性proximity\\
数据矩阵和相异性矩阵\\
$n$ 个对象, $p$ 个属性\\
数据矩阵data matrix $n \times p$\\
$$
\eqnote{data matrix}_{ij} = x_{ij}
$$
$x_{ij}$ 第$i$ 个对象的第$j$ 个属性的值

相异性矩阵 dissimilarity matrix; $n \times n$
$$
sim(i,j) = 1 – d(i,j)
$$
数据矩阵由两种实体(行的对象,列的属性)组成, 因而被称为二模(two-mode)矩阵\\
相异性矩阵只包含一种实体, 因而被称为单模(one-mode)矩阵\\

许多聚类和最近邻算法都在相异性矩阵上运行

\textbf{标称属性的邻近性度量}
$$
d(i,j) = \frac{p – m}{p}
$$
$m$: 匹配的数目(即对象i和j 取相同状态的属性数)\\
$p$: 属性总数

\textbf{数值属性的相异性}\par
\begin{itemize}
\item 欧几里得距离
\item 曼哈顿距离, 街区距离, 两个点在标准坐标系上的绝对轴距总和
\item 闵可夫斯基距离, $L_p$ 范数
\item 上确界距离(又称为$L_{\infty}$ 范数和切比雪夫Chebyshev距离)
\end{itemize}

\textbf{序数属性}:转化为数值属性进行度量

混合类型属性的相异性

\subsubsection{余弦相似性}
词频向量term frequency vector 通常很长, 并且是稀疏的\\
使用这种结构的应用包括信息检索, 文本文档聚类, 生物学分类和基因特征映射.\\
对于这类稀疏的数值数据, 前面介绍过的传统的距离度量效果并不好

余弦相似性是一种度量, 他可以用来比较文档, 或针对给定的查询词向量对文档排序
$$sim(x,y) = \dfrac{x \cdot y}{\norm{x} \norm{y}}$$

当属性为二元属性时, 余弦相似性衡量的是他们共享的属性

\section{数据预处理}
数据质量: 准确性, 完整性, 一致性, 时效性, 可信性和可解释性.

\subsection{数据清理data cleaning}
用来清除数据中的噪声, 填补缺失的值, 识别或删除离群点并解决不一致性.

填充缺失值的方法:忽略元组, 人工填写缺失值,使用一个全局常量填充缺失值, 使用属性的中心度量(如均值或中位数)填充缺失值, 使用与给定元组属同一类的所有样本的属性均值或中位数,使用最可能的缺失值填充缺失值(用回归,贝叶斯形式化方法的基于推理工具或决策树归纳确定)

噪声noise: 是被测量的变量的随机误差或方差.

数据光滑技术
\begin{description}
	\item [分箱binning] 通过考察数据的近邻(即周围的值)来光滑有序数据值,将数据划分到等频的箱中, 局部光滑方法, 有用箱均值光滑(用箱中数据的均值代替箱中的每一个值), 用箱中位数光滑, 用箱边界光滑
	\item [回归regression] 用一个函数拟合数据来光滑数据
	\item [离群点分析outlier analysis] 可以通过聚类来检测离群点
\end{description}

唯一性规则, 连续性规则和控制规则

数据清洗工具data srubbing tool, 数据审计工具data auditing tool, 数据迁移工具data migration tool. ETL(extraction/transformation/loading), Potter's Wheel 一个公开的数据清理工具, 集成了偏差检测和数据变换

\subsection{数据集成data integration}
将数据由多个数据源合并成一个一致的数据存储, 如数据仓库.
\subsubsection{冗余和相关分析}
一个属性如果能够由另外一个属性导出, 则这个属性可能是冗余的.

标称数据的$\chi^2$卡方检验
数值数据的相关系数, Pearson 积矩系数
数值数据的协方差

远足重复, 数据值冲突的检测与处理

\subsection{数据规约data reduction}
通过如聚集, 删除冗余特征或聚类来降低数据的规模.
\begin{itemize}
	\item 维规约dimensionality reduction: 使用数据编码方案, 例如小波分析和主成分分析, 属性子集选择(去掉不相关的属性)和属性构造(从原来的属性导出更有用的小属性集).
	\item 数值规约numerosity reduction:使用参数模型(例如,回归,多元回归和对数线性模型)和非参数模型(例如直方图, 聚类, 抽样或数据聚集), 用较小的表示取代数据, 离散化和概念分层(例如年龄的原始值可以用交高层的概念如青年, 中年和老年取代)
	\item 数据压缩data compression
\end{itemize}
\subsubsection{小波变换}
离散小波变换DWT, 离散傅里叶变换DFT. 一般的说, DWT一种更好的有损压缩
离散小波变换是一种线性信号处理技术, 用于数据向量X 时, 将它变换成不同的数值小波变换向量 $X'$. 两个向量具有相同的长度. 当这种技术用于数据规约时, 每个元祖看做一个n 维数据向量, 即 $X = (\vecteur{x})$, 描述n 个数据库属性在元组上的n 次测量.\\
如果小波变换后的数据与原数据的长度相等, 这种技术如何能够用于数据压缩? 关键在于小波变换后的数据可以截短. 仅存放一小部分最强的小波系数, 就能保留近似的压缩数据. 
例如, 保留大于用户设定的某个阀值的所有小波系数, 其他系数置为0, 这样, 结果数据表示非常稀疏, 使得如果在小波空间进行计算的话, 利用数据稀疏特点的操作计算可以非常快.
该技术也能用于消除噪声, 而不会光滑掉数据的主要特征, 使得他们也能有效的用于数据清理.\\
给定一组系数, 使用所有的DWT的逆, 可以构造原数据的近似.

流行的小波变换包括Haar$-2$, Daubechies$-4$和Daubechies$-6$. \\
离散小波变换的一般过程是使用一种层次金字塔算法pyramid algorithm, 他在每次迭代时将数据减半, 导致计算速度很快.

小波变换可以用于多维数据, 如数据立方体. 可以按照以下方法处理: 首先将变换用于第一个维, 然后第二个, 如此下去. 计算复杂性关于立方体中单元的个数是线性的.
对于稀疏或倾斜数据和具有有序属性的数据, 小波变换给出了很好的结果. 据报道, 小波变换的有损压缩优于JPEG 压缩. 
小波变换有许多实际应用, 包括指纹图像压缩, 计算机视觉, 时间序列数据分析和数据清理.

主成分分析principal components analysis(PCA):PCA通过创建一个替换的, 较小的变量集合组合属性的基本要素

\subsubsection{属性子集选择}
删除不相关或冗余的属性(或维)减少数据量. 属性子集选择的目标是找出最小属性集, 使得数据类的概率分布尽可能地接近使用所有属性得到原分布.

属性子集选择: 通常使用压缩搜索空间的启发式算法, 通常这些方法是典型的贪心算法. 在实践中, 这种贪心方法是有效的, 并且可以逼近最优解.

最好的(和最差的)属性通常使用统计显著性检验来确定. 这种检验假定属性是相互独立的, 也可以使用一些其他属性评估度量, 如建立分类决策树使用的信息增益度量.

属性子集选择的基本启发方法包括以下技术:
\begin{description}
\item [逐步向前选择] 由空属性集作为规约集开始, 确定原属性集中最好的属性, 并将它添加到规约集中. 在其后的每一次迭代中, \textbf{将剩下的原属性集中最好的属性添加到该集合中}.
\item [逐步向后删除] 由整个属性集作为规约集开始, 在每一步中, 删除尚在属性集中最差的属性.
\item [逐步向前选择和向后删除的组合]
\item [决策树归纳] 当决策树归纳用于属性子集选择时, 由给定的数据构造决策树. 不出现在树中的所有属性假定是不相关的. 出现在树中的属性形成规约后的属性子集\todo{这个方法不理解}
\end{description}

\subsection{数据变换data transformation}
(例如,规范化)可以用来把数据压缩到较小的区间, 如$0.0$ 到$1.0$, 这可以提高涉及距离度量的挖掘算法的准确率和效率.\\
规范化, 数据离散化和概念分层都是某种形式的数据变换

数据变换策略:光滑smoothing(去掉数据中的噪声, 分箱, 回归和聚类);属性构造; 聚集; 规范化; 离散化; 由标称数据产生概念分层

\subsection{规范化数据}
最小-最大规范化: 对原始数据进行线性变换, 映射到$[-1,1]$或$[0,1]$
$$
v_i' = \dfrac{v_i - min_A }{max_A - min_A}(new\_max_A - new\_min_A) + new\_min_A
$$

z分数(z-score)规范化或零均值规范化
$$
v_i' = \dfrac{ v_i - \overline{A}}{\sigma_A}
$$
其中$\overline{A}$为属性$A$的均值, $\sigma_A$的属性$A$的标准差 \\
式中的标准差可以使用均值绝对偏差替换,$A$的均值绝对偏差(mean absolute deviation)$s_A$定义为
$$
s_A = \dfrac{1}{n}( \norm{v_1 - \overline{A}}+ \norm{v_1 - \overline{A}}+ \ddots \norm{v_n - \overline{A}}+)
$$

按小数定标规范化: 移动属性$A$的值的小数点位置进行规范化
$$
v_i' = \dfrac{v_i}{10^j}
$$

\section{数据仓库和联机分析处理}
\subsection{数据仓库}
数据仓库是面向主题的subject-oriented, 集成的integrated, 时变的time-variant, 非易失的nonvolatile数据集合, 支持管理者的决策过程.

对于异构数据库的集成, 传统的数据库做法: 在多个异构数据库上,建立一个包装程序和一个集成程序(或中介程序). 当查询在客户站点提交时,首先转换成各个异构数据库上相应的查询, 然后将所有的查询结果集成为全局回答. 这是查询驱动query-driven的方法.而数据仓库采用更新驱动update-driven的方法.\par
数据仓库并不包含最近的数据.

联机操作数据库系统的主要任务是执行联机事务和查询处理. 这种系统称作联机事务处理online transaction processing,OLTP系统. \\
而数据仓库系统在数据分析和决策方面为用户和只是工人提供服务. 这种系统可以用不同的格式组织和提供数据, 以便满足不同用户的形形色色的需求. 这种系统称作联机分析处理online analytical processing,OLAP系统.\par
OLTP与OLAP的区别
\begin{itemize}
	\item 用户和系统的面向性: OLTP是面向顾客的, 用于办事员, 客户; OLAP 是面向市场的, 用于知识工人(包括经理, 主管和分析人员)的数据分析.
	\item 数据库设计: 通常OLTP系统采用实体-联系(ER)数据模型和面向应用的数据库设计. 而OLAP系统通常采用星形或雪花模型和面向主题的数据库设计.
	\item 访问模式: OLTP系统的访问主要由短的原子事务组成, 需要并发控制和恢复机制. 然后OLAP系统的访问大部分是只读操作.
\end{itemize}

\subsection{数据仓库的多维数据模型}
\textbf{事实表和维表}:事实表是用来记录具体事件的,包含了每个事件的具体要素,以及具体发生的事情,维表则是对事实表中事件的要素的描述信息.
比如一个事件会包含时间,地点,人物,事件,事实表记录了整个事件的信息,但对时间,地点和人物等要素只记录了一些关键标记,
比如事件的主角叫"Michael",那么Michael到底"长什么样",就需要到相应的维表里面去查询"Michael"的具体描述信息了.\\
基于事实表和维表就可以构建出多种多维模型,包括\textbf{星形模型,雪花模型和星座模型}P91

事实表里面主要包含两方面的信息:维和度量,维的具体描述信息记录在维表,事实表中的维属性只是一个关联到维表的键,并不记录具体信息,度量一般都会记录事件的相应数值,比如产品的销售数量,销售额等.维表中的信息一般是可以分层的,比如时间维的年月日,地域维的省市县等,这类分层的信息就是为了满足事实表中的度量可以在不同的粒度上完成聚合,比如2010年商品的销售额,来自上海市的销售额等.

还有一点需要注意的是,维表的信息更新频率不高或者保持相对的稳定,例如一个已经建立的十年的时间维在短期是不需要更新的,地域维也是,但是事实表中的数据会不断地更新或增加,因为事件一直在不断地发生,用户在不断地购买商品,接受服务.

\subsection{数据仓库的设计与使用}
设计需考虑以下四种视图: 自顶向下视图, 数据源视图, 数据仓库视图和商务查询视图.

\section{挖掘频繁模式, 关联和相关性: 基本概念和方法}
\subsection{基本概念}
\begin{itemize}
	\item 项集$X$ 在数据集$D$中是\textbf{闭的(closed)}, 如果不存在真超项集
\footnote{$Y$是$X$的真超项集, 如果$X$ 是$Y$的真子项集, 即$X \subset Y$}
$Y$使得$Y$与$X$在$D$中具有相同的支持度计数.(所谓闭项集,就是指一个项集X,它的直接超集的支持度计数都不等于它本身的支持度计数.)
	\item 项集$X$ 是数据集$D$ 中的\textbf{闭频繁项集(closed frequent itemset)}, 如果$X$在$D$是闭的和频繁的.\\
	\item 项集$X$ 是数据集$D$ 中的\textbf{极大频繁项集(maximal frequent itemset) 或极大项集(max-itemset)}, 如果$X$是频繁的, 并且不存在超项集$Y$使得$X \subset Y$在$D$ 是频繁的.
\end{itemize}

\begin{example}
例如,有交易数据库
\begin{table}[htbp]  % here, top, bottom, float page(单独的浮动页)
  \centering
  \begin{tabular}{|c|c|}
\hline
TID& item\\
\hline
1& abc\\
\hline
2& abcd\\
\hline
3& bce\\
\hline
4& acde\\
\hline
5& de\\
\hline
\end{tabular}
\end{table}\\
因为项集$\{b,c\}$出现在TID为$1,2,3$的事务中,所以$\{b,c\}$的支持度计数为$3$.而$\{b,c\}$的直接超集:$\{a,b,c\}$,$\{a,b,c,d\}$的支持度计数分别为$2,1$,都不等于$\{b,c\}$的支持度计数$3$,所以$\{b,c\}$为闭项集,如果支持度阈值为$40\%$,则$\{b,c\}$也为闭频繁项集.\\
项集$\{a,b\}$出现在TID为$1,2$的事务中,其支持度计数为$2$.而它的直接超集$\{a,b,c\}$支持度计数也为$2$,所以$\{a,b\}$不是闭项集.
\end{example}

设$\mathcal{C}$ 是数据集$D$ 中满足最小支持度阀值$min\_sup$ 的闭频繁项集的集合, $\mathcal{M}$ 是$D$ 中满足$min\_sup$的极大频繁项集的集合.
假设有$\mathcal{C}$ 和 $\mathcal{M}$ 的每个项集的支持度计数.
注意, $\mathcal{C}$ 和它的技术信息可以用来导出频繁项集的完整信息\todo{how}, 因此, 称$\mathcal{C}$ 包含了关于频繁项集的完整信息;
另一方面, $\mathcal{M}$ 只包含了极大项集的支持度信息, 通常, 他并不包含其对应的频繁项集的完整的支持度信息.

\begin{example}
假定事务数据库只有两个事务:$\{a_1,a_2,\dots,a_{100}\}$,$\{a_1,a_2,\dots,a_{50}\}$.
设最小支持度计数阀值$min\_sup = 1$.\\
两个闭频繁项集$\mathcal{C} = \{\{a_1,a_2,\dots,a_{100}\}:1;\{a_1,a_2,\dots,a_{50}\}:2\}$ \\
只有一个极大频繁项集$\mathcal{M} = \{\{a_1,a_2,\dots,a_{50}\}:1\}$ \\
注意, 我们不能断言$\{a_1,a_2,\dots,a_{50}\}$.是极大频繁项集, 因为他有一个频繁的超集$\{a_1,a_2,\dots,a_{100}\}$.\par
比频繁项集的集合包含了频繁项集的完整信息. 例如, 可以从$\mathcal{C}$推出:
\begin{enumerate}
	\item $\{\{a_2,a_{45}\}:2\}$,因为$\{a_2,a_{45}\}$ 是$\{\{a_1,a_2,\dots,a_{50}\}:2\}$的子集
	\item $\{\{a_8,a_{55}\}:1\}$
\end{enumerate}
然而, 从极大频繁项集只能断言两个项集$\{a_2,a_{45}\}$,$\{a_8,a_{55}\}$ 是频繁的, 但是不能推断他们的实际支持度计数.
\end{example}

\subsection{频繁项集挖掘方法}
\subsubsection{Apriori 算法(先验算法)}
Apriori uses breadth-first search and a Hash tree structure to count candidate item sets efficiently.

在Apriori算法中,寻找最大项目集的基本思想是:算法需要对数据集进行多步处理.
\begin{enumerate}
	\item 第一步,简单统计所有含一个元素项目集出现的频率,并找出那些不小于最小支持度的项目集,即一维最大项目集.
	\item 从第二步开始循环处理直到再没有最大项目集生成.
	\item 循环过程是:第k步中,根据第k-1步生成的(k-1)维最大项目集产生k维侯选项目集,然后对数据库进行搜索,
			得到侯选项目集的项集支持度,与最小支持度比较,从而找到k维最大项目集. 为了提高效率, 会应用下面的先验性质用于压缩搜索空间,
			也就是在搜索过程中, 会排出一些明显不可能的项集
\end{enumerate}

\textbf{先验性质}: 频繁项集的所有非空子集也一定是频繁的.\\
先验性质基于如下观察, 如果项集$I$ 不满足最小支持度阀值$min\_sup$, 则$I$ 不是频繁的, 即$P(I) < min\_sup$, 如果把项$A$添加到项集$I$ 中, 则结果项集(即$I \cup \{A\}$)不可能比$I$ 更频繁出现\footnote{因为, 添加了一项之后, 限制条件更多了, 所以支持度计数不可能会增加}, 因此, $I \cup \{A\}$ 也不是频繁的, 即$P(I \cup \{A\}) < min\_sup$

该性质是一种\textbf{反单调性 antimonotone}, 指如果一个集合不能通过测试, 则它的所有超集也都不能通过相同的测试. 称它是反单调的, 因为在不通过测试的意义下, 该性质是单调的.

算法的pseudo code见图\href{http://i.imgbox.com/ZKLH7OMB.png}{apriori algo}

\begin{example}
Let the database of transactions consist of following itemsets:
\begin{verbatim}
Itemsets
{1,2,3,4}
{1,2,4}
{1,2}
{2,3,4}
{2,3}
{3,4}
{2,4}
\end{verbatim}
the value 3 is the support threshold.
The first step of Apriori is to count up the number of occurrences, called the support, of each member item separately, by scanning the database a first time.
We obtain the following result
\begin{verbatim}
Item	Support
{1}	3
{2}	6
{3}	4
{4}	5
\end{verbatim}
All the itemsets of size 1 have a support of at least 3, so they are all frequent.
The next step is to generate a list of all pairs of the frequent items:
\begin{verbatim}
Item	Support
{1,2}	3
{1,3}	1
{1,4}	2
{2,3}	3
{2,4}	4
{3,4}	3
\end{verbatim}
The pairs $\{1,2\}, \{2,3\}, \{2,4\}$ , and $\{3,4\}$ all meet or exceed the minimum support of $3$, so they are frequent. The pairs $\{1,3\} and \{1,4\}$ are not. Now, because $\{1,3\}$ and $\{1,4\}$ are not frequent, any larger set which contains $\{1,3\}$ or $\{1,4\}$ cannot be frequent\footnote{应用先验性质}. In this way, we can prune sets: we will now look for frequent triples in the database, but we can already exclude all the triples that contain one of these two pairs:
\begin{verbatim}
Item	Support
{2,3,4}	2
\end{verbatim}
In the example, there are no frequent triplets -- $\{2,3,4\}$ is below the minimal threshold, and the other triplets were excluded because they were super sets of pairs that were already below the threshold.
\end{example}

\subsubsection{提高Apriori 算法的效率}
\begin{description}
	\item[基于散列的技术] 压缩候选$k$ 项集的集合$C_k\ (k > 1)$
	\item[事务压缩] 压缩进一步跌打扫描的事务数
	\item[划分] 为候选项集划分数据
	\item[抽样] 对给定数据的一个子集上挖掘
	\item[动态项集计数] 在不同的不同点添加候选项集
\end{description}

\subsubsection{挖掘频繁项集的的模式增长方法}
Frequent-pattern growth(FP-growth)

采用分支策略, 首先将代表频繁项集的数据压缩到一颗\textbf{频繁模式树(FP 树)}, 该树仍保留项集的关联信息. 然后把这种压缩后的数据库划分成一组条件数据库(一种特殊类型的投影数据库), 每个数据库关联一个频繁项或``模式段'', 并分别挖掘每个条件数据库. 对于每个``模式段", 只需要考察与它相关联数据集. 因此, 随着被考察的模式的``增长", 这种方法可以显著地压缩被搜索的数据集的大小.

\subsubsection{使用垂直数据格式挖掘频繁项集}
\begin{itemize}
	\item TID: itemset 被称为水平数据格式(horizontal data format)
	\item item: TID\_set 被称为垂直数据格式(vertical data format)
\end{itemize}

\subsubsection{挖掘闭模式和极大模式}

\subsection{那些模式是有去的: 模式评估方法}
不是所有满足支持度和置信度的规则都是有趣的, 所以需要采用模式评估方法对他们进行评估.

相关性度量: 提升度 ,$\chi^2$\\
$$
A \Rightarrow B[support, confidence, correlation]
$$

\textbf{提升度 lift}:一种简单的相关性度量
$$
lift(A,B) = \frac{P(A \cap B)}{P(A)P(B)} ( = \frac{P(B|A)}{P(B)})
$$
如果值小于1, 则A 的出现与B 的出现是负相关的; 如果值大于1, 则正相关; 如果值等于1, 则A 与B 是相互独立的, 之间没有相关性.

一般在数据挖掘中当提升度大于3时,我们才承认挖掘出的关联规则是有价值的.

提升度是一种比较简单地判断手段,在实际应用中它受零事务的影响较大.

\bigskip
其他的模式评估度量: 全置信度, 最大置信度, Kluc(Kulczynski), 余弦

\section{高级模式挖掘}
多层模式, 多唯模式, 连续数据中的模式, 稀有模式, 负模式, 受约束的频繁模式, 高维数据中的频繁模式, 巨型模式, 压缩和近似模式.
其他的模式包括挖掘序列模式和结构模式, 时空数据, 多媒体数据和流数据挖掘模式.

模式挖掘是一个比频繁模式挖掘更一般的术语, 因为前者还涵盖了稀有模式和负模式.

\bigskip
模式挖掘研究
\begin{itemize}
	\item 模式和规则的类型
		\begin{itemize}
			\item 基本模式
				\begin{itemize}
					\item 频繁模式
					\item 关联规则
					\item 闭/极大模式
					\item 生成元
				\end{itemize}
			\item 多层和多维模式
				\begin{itemize}
					\item 多层(一致, 变化或基于项集的支持度)
					\item 多维模式(包括高维模式)
					\item 连续数据(基于离散化或基于统计)
				\end{itemize}
			\item 扩充的模式
				\begin{itemize}
					\item 近似模式
					\item 不确定模式
					\item 压缩模式
					\item 稀有模式/负模式
					\item 高维和巨型模式
				\end{itemize}
		\end{itemize}
	\item 挖掘方法
		\begin{itemize}
			\item 基本挖掘方法
				\begin{itemize}
					\item 多候选产生(Apriori, 划分, 抽样...)
					\item 模式增长(FP-growth, HMine, FPMax, Close+, ...)
					\item 垂直格式(Eclat, CHARM, ...)
				\end{itemize}
			\item 挖掘有趣的模式
				\begin{itemize}
					\item 兴趣度(主观的与客观的)
					\item 基于约束的挖掘
					\item 相关规则
					\item 异常规则
				\end{itemize}
			\item 分布, 并行和增量的
				\begin{itemize}
					\item 分布/并性挖掘
					\item 流量挖掘
					\item 流模式
				\end{itemize}
		\end{itemize}
	\item 扩充和应用
		\begin{itemize}
			\item 扩充的数据类型
				\begin{itemize}
					\item 序列和时间序列模式
					\item 结构(例如, 树, 格, 图)模式
					\item 空间(例如, 协定位)模式
					\item 图像, 视频和多媒体模式
					\item 网络模式
				\end{itemize}
			\item 应用
				\begin{itemize}
					\item 基于模式的分类
					\item 基于模式的聚类
					\item 基于模式的语义注释
					\item 协同过滤
					\item 保护隐私
				\end{itemize}
		\end{itemize}
\end{itemize}

频繁模式挖掘常常充当中间步骤, 改善对数据的理解并进行作用更大的数据分析. 例如, 他可以作为分类的特征提取步骤使用, 这称为基于模式的分类. 类似的, 也有基于模式的聚类.

为了改善对数据的理解, 模式可以用于语义注释或语境分析. 模式分析也可以用在推荐系统中, 基于类似用户的模式, 向用户推荐他可能感兴趣的信息项(如书, 电影, Web页面). 不同的分析任务也可能需要挖掘不同的模式类型.

\textbf{多层关联规则的冗余性}

\begin{itemize}
\item 单维或维内关联规则(intradimension association rule)
\item 多维关联规则(两个或多个维或谓词) multidimensional association rule;
\item 不重复谓词的关联规则,叫做维间关联规则(interdimension association rule)
\item 重复谓词的关联规则, 叫做混合维关联规则(hybird-dimension association rule)
\end{itemize}
$$
age(X,"20...29") \cap occupation(X,"student") \Rightarrow buys(X,"laptop")
$$

在多维关联规则挖掘中, 搜索频繁\textbf{谓词集}. \textbf{k - 谓词集}是包含k 个合取谓词的集合, 例如上述规则中的谓词集$\{age, occupation, buys\}$ 是一个$3-$谓词集.

\subsection{挖掘量化关联规则}
\begin{itemize}
	\item 数据立方体方法
	\item 基于聚类的方法, 例如自顶向下的基于聚类的量化频繁模式
	\item 基于异常行为的统计方法
\end{itemize}

\subsection{挖掘稀有模式和负模式}
\begin{definition}
\label{def.negative.1}
如果项集$X$ 和$Y$ 都是频繁的, 但是很少一起出现($sup(X \cap Y) < sup(X) \times sup(Y)$), 则项集X 和Y 是负相关的, 并且模式 $X \cap Y$ 是负相关模式. 如果
$sup(X \cap Y) \ll sup(X) \times sup(Y)$, 则X 和Y 是强负相关的, 并且$X \cap Y$ 是强负相关模式.
\end{definition}
该定义容易扩展到k-项集的模式, 其中$k>2$
定义~\ref{def.negative.1} 有一个问题, 他不是\red{零不变}的, 即他的值可能被零事务(发生次数为零的事务)影响. 但是对于定量的兴趣度度量, 零不变性是至关重要的.

\begin{definition}
如果X 和Y 是强负相关的, 则
$$
sup(X \cap \stcomp{Y}) \times sup(\stcomp{X} \cap Y) \gg sup(X \cap Y) \times sup(\stcomp{X} \cap \stcomp{Y})
$$	
\end{definition}
但是这个定义又不是零不变的

定义~\ref{def.negative.3} 是基于Kulczynski 度量(条件概率的平均值), 它遵循兴趣度度量的精神实质
\begin{definition}
\label{def.negative.3}
假设项集X和Y 都是频繁的, 即
$sup(X) \geq min\_sup$,
$sup(Y) \geq min\_sup$,
其中$min\_sup$ 是最小支持度阀值.
如果 $ (P(X| Y) + P(Y|X))/2 < \epsilon $
其中 $\epsilon$ 是负模式阀值, 则 $X \cap Y$ 是负相关模式
\end{definition}

基于约束的频繁模式挖掘: 
知识类型,
数据约束,
维/层约束,
兴趣度约束,
规则约束.

基于约束的模式产生
\begin{itemize}
	\item 模式空间剪枝: 分为5类, 反单调的; 单调的; 简洁的; 不可转变的
	\item 数据空间剪枝: 数据的简洁性和反单调性
\end{itemize}

\section{分类}
构造一个模型或分类器(classifer) 来预测类标号, 如贷款申请的"安全"或"危险"等. 这些类标号可以用离散值表示, 其中值的次序没有意义.

数值预测(numeric prediction): 预测一个连续值函数或有序值, 而不是类标号. 这种模型是预测器(predictor). 回归分析(regression analysis) 是数值预测最常用的统计学方法.

分类和数值预测是预测问题的两种主要类型

数据分类是一个两阶段过程: 包括学习阶段(构建分类模型)和分类阶段(使用模型预测给定数据的类标号). 但是一般在两者之间还有一个测试阶段, 在测试数据集上对第一阶段构建的分类器进行测试, 当准确率达到要求后, 再进行第三阶段.

在学习阶段有可能出现overfit的问题, 所以要在test set 上进行test.

\bigskip
\textbf{Overfit 过度拟合}\\
overfittingt是这样一种现象:一个假设在训练数据上能够获得比其他假设更好的拟合,但是在训练数据外的数据集上却不能很好的拟合数据.此时我们就叫这个假设出现了overfitting的现象.出现这种现象的主要原因是训练数据中存在噪音或者训练数据太少.而解决overfit的方法主要有两种:提前停止树的增长或者对已经生成的树按照一定的规则进行后剪枝.\\
百度中关于overfitting的标准定义:给定一个假设空间H,一个假设h属于H,如果存在其他的假设h'属于H,使得在训练样例上h的错误率比h'小,但在整个实例分布上h'比h的错误率小,那么就说假设h过度拟合训练数据.\\
以下概念由本人摘自<数据挖掘-概念与技术>\\
P186 过分拟合 即在机器学习期间,它可能并入了训练数据中的某些特殊的异常点,这些异常不在一般数据集中出现.\\
P212 由于规则可能过分拟合这些数据,因此这种评论是乐观的.也就是说,规则可能在训练数据上行能很好,但是在以后的数据上九不那么好.

\subsection{决策树归纳}
树中包含三种结点:根结点,内部结点,叶子结点(终结点).\\
在决策树中,每一个叶结点都赋予一个类标号.非终结点(包括根节点和内部结点)包含属性测试条件.用以分开具有不同特性的记录.\\
\href{http://blog.csdn.net/raby_gyl/article/details/15034431}{决策树归纳}\\
\href{http://wenzhang.baidu.com/article/view?key=50fb6d3f2f6a2c97-1393127697}{决策树归纳-百度文章}\\

如何建立决策树?
原则上讲,给定的属性集,可以构造的决策树的数目达指数级(因为我们可以把属性集放在不同的结点处,或者构造不同结果的决策树).
尽管某些决策树比其他决策树更准确,但是由于搜索空间是指数规模的,找出最佳决策树在计算上是不可行的.
尽管如此,人们还是开发了一些有效的算法,能够在合理的时间内构造出具有一定准确率的次最优决策树.
这些算法通常都是采用贪心策略,在选择划分数据的属性时,采取一些列局部最优决策来构造决策树,Hunt算法就是这样的一种算法.
Hunt算法是许多决策树算法的基础,包括ID3,C4.5和CART(classification and regression tree),分类和回归树,OpenCV实现的就是此类算法.

\subsubsection{属性选择度量}
属性选择度量是一种选择分裂准则, 把给定标记的训练元组的数据分区D "最好地" 划分成单独类的启发式方法. 理想的情况是, 每个分区都是纯的(即落在一个给定分区的所有元组都属于相同的类).

三种常用的属性选择度量: 信息增益, 增益率和基尼指数(Gini 指数)

\textbf{信息增益}\\
基于香农的信息熵.

设节点N 代表或放在分区D 的元组. 选择具有最高信息增益的属性作为节点N的分裂属性. 该属性使结果分区中对元组分类所需要的信息量最小, 并反映这些分区中的最小随机性或不纯性.
这种方法使得对一个对象分类所需要的期望测试数目最小, 并确保找到一颗简单(但不必是最简单的)树.

对D 中的元组分类所需要的期望信息(又称为信息熵entropy)由下式给出
$$
Info(D) = - \sum_{i = 1}^m p_i \log_2 (P_i)
$$
其中$p_i$ 是D 中任意元组属于类$C_i$ 的概率, 并用 $\norm{C_{i,D}} / \norm{D}$ 估计.\par
Info(D) 是识别D 中的类标号所需要的平均信息量. 注意, 此时我们所有的信息只是每个类的元组所占的百分比.

现在, 假设我们要按照属性A 划分D中的元组, 其中属性A 根据训练数据的观测具有$v$ 个不同值
$\{ \vecteur{a} \}$. 
如果A 是离散值的, 则这些值直接对应于A 上测试的v 个输出.
可以用属性A 将D 划分为v 个分区
$\{ \vecteur{D} \}$. 
其中$D_j$ 中的元组的A 属性的值为$a_j$. 这些分区对应于从节点N 生长出来的分支. 理想情况下, 我们希望该划分产生元组的准确分类. 即我们希望每个分区都是纯的.
然而, 这些分区多半是不纯的(例如, 分区可能包含来自不同类的元组). 在此划分之后, 为了得到准确的分类, 我们还需要多少信息? 这个量由下式度量:
$$
Info_A(D) = - \sum_{j = 1}^v \frac{\norm{D_j}}{\norm{D}} \times Info(D_j)
$$
项 $\frac{\norm{D_j}}{\norm{D}}$ 充当第j 个分区的权重, $Info_A(D_j)$ 是基于按A 划分对D 的元组分类所需要的期望信息. 需要的希望信息越小, 分区的纯度越高.

信息增益定义为原来的信息需求(仅基于类比例)与新的信息需求(对A划分之后)之间的差. 即
$$
Gain(A) = Info(D) - Info_A(D)
$$
换言之, Gain(A) 告诉了我们通过A 的划分我们得到了什么. 也就是为了知道信息需求的期望, 在属性A的帮助下, 我么的需求变少了.

\textbf{增益率}\\

\textbf{基尼指数}\\

\subsubsection{树剪枝}
在决策树创建时, 由于数据中的噪声和离群点, 许多分支反映的是训练数据中的异常. 剪枝方法处理这种过分拟合数据的问题.\\
通常这种方法使用统计学度量剪掉最不可靠的分支. 剪枝后的树更小更简单, 因此更容易理解, 且一般更好.

两种常用的剪枝方法
\begin{description}
\item [先剪枝prepruning] 通过提前停止树的构建.
\item [后剪枝postpruning] 由完全生长的树减去子树. 通过删除结点的分支并用树叶替换它而剪掉给定的节点上的子树, 该树叶的类标号用子树中最频繁的类标记.
		决策树方法中的 CART,ID3,C4.5 算法主要采用后剪枝技术.后剪枝操作是一个边修剪边检验的过程,一般规则标准是:在决策树的不断剪枝操作过程中,将原样本集合或新数据集合作为测试数据,检验决策树对测试数据的预测精度,并计算出相应的错误率,如果剪掉某个子树后的决策树对测试数据的预测精度或其他测度不降低,那么剪掉该子树.
\end{description}

尽管剪枝后的树一般比未剪枝的树更紧凑, 但是仍然可能很复杂. 决策树可能受到重复和复制的困扰. 沿着一条给定的分支反复测试一个属性(如 age < 60?, 后面跟着 age < 45? 等)时
就会出现重复repetition. 复制replication 是树中存在重复的子树.\\
这些情况影响了决策树的准确率和可解释性. 使用多元划分可以防止该问题的出现.

\subsubsection{可伸缩性欲决策树归纳}
如果驻留在磁盘上的类标记元组训练集D 不能装进内存会怎样?\\
已有的决策树算法, 如ID3, C4.5, CART都是为较小数据集设计的.

\textbf{RainForest(雨林)}能适应可用的内存量, 并用于任意决策树归纳算法. 该方法在每个结点, 对每个属性维护一个AVC-集(AVC表示属性-值, 类标号), 描述该结点的训练元组.
AVC-组群可以放在内存中. 如果这个不能满足, rainforest 还有一些技术, 用于处理AVC-组群不能放在内存中的情况.

\textbf{树构造的自助乐观算法(bootstrapped optimistic algorithm for tree construction, BOAT)}: 使用一种称为自助法的统计学技术, 
创建给定训练数据的一些较小的样本(或子集), 其中每个子集都能放在内存中.
使用每个子集构造一棵树, 导致多棵树, 然后再用这些树构造一颗新树, 他非常接近于原来的所有训练数据都放在内存所产生的树.

\subsection{贝叶斯分类法}
统计学分类方法, 预测隶属关系的概率, 如一个给定的元组属于一个特定类的概率.\\
基于贝叶斯定理. 朴素贝叶斯分类法假定一个属性值在给定类上的影响独立于其他属性的值. 这一假定称为类条件独立性.

\noindent
$P(H |X)$ 后验概率(posterior probability), 或在条件$X$下, $H$的后验概率.\\
$P(H)$ 先验概率(priori probability), 或$H$的先验概率.\\
$P(X|H)$ 后验概率(posterior probability), 或在条件$H$下, $X$的后验概率.
$P(X)$ 先验概率(priori probability), 或$X$的先验概率.\\

例如, 假设数据元组由属性age和income描述的顾客, 而X是一位35岁的顾客．其收入为4万美元．令H为某种假设, 如顾客将购买计算机.\\
则, $P(H|X)$反映当我们知道顾客的收入和年龄时, 顾客X 将购买计算机的概率.\\
$P(H)$ 表示任意给定顾客将购买计算机的概率, 而不管他们的年龄, 收入等信息.\\
$P(X|H)$ 表示 已知顾客将购买计算机, 该顾客是35岁且收入为4万美元的概率.\\
$P(X)$ 表示顾客集合中的年龄为35岁且收入为4万美元的概率.

贝叶斯定理是关于随机事件A和B的条件概率(或边缘概率)的一则定理.
$$ P(A|B) = \frac{P(B | A)\, P(A)}{P(B)} $$
其中$P(A|B)$是在$B$发生的情况下$A$发生的可能性

理论上,概率模型分类器是一个条件概率模型.
$$ p(C \vert F_1,\dots,F_n)\, $$
独立的类别变量$C$有若干类别,条件依赖于若干特征变量 $F_1,F_2,...,F_n$.但问题在于如果特征数量n较大或者每个特征能取大量值时,基于概率模型列出概率表变得不现实.所以我们修改这个模型使之变得可行. 贝叶斯定理有以下式子:
$$ p(C \vert F_1,\dots,F_n) = \frac{p(C) \ p(F_1,\dots,F_n\vert C)}{p(F_1,\dots,F_n)}. \, $$
用朴素的语言可以表达为:
$$ \mbox{posterior} = \frac{\mbox{prior} \times \mbox{likelihood}}{\mbox{evidence}}.  \, $$
实际中,我们只关心分式中的分子部分,因为\textbf{分母不依赖于$C$而且特征$F_i$的值是给定的,于是分母可以认为是一个常数}.这样分子就等价于联合分布模型.
$$ p(C \vert F_1, \dots, F_n)\, $$
重复使用链式法则,可将该式写成条件概率的形式,如下所示:
$$
\begin{aligned}
& p(C \vert F_1, \dots, F_n)\,\\
& \varpropto p(C) \ p(F_1,\dots,F_n\vert C)\\
& \varpropto p(C) \ p(F_1\vert C) \ p(F_2,\dots,F_n\vert C, F_1)\\
& \varpropto p(C) \ p(F_1\vert C) \ p(F_2\vert C, F_1) \ p(F_3,\dots,F_n\vert C, F_1, F_2)\\
& \varpropto p(C) \ p(F_1\vert C) \ p(F_2\vert C, F_1) \ p(F_3\vert C, F_1, F_2) \ p(F_4,\dots,F_n\vert C, F_1, F_2, F_3)\\
& \varpropto p(C) \ p(F_1\vert C) \ p(F_2\vert C, F_1) \ p(F_3\vert C, F_1, F_2) \ \dots p(F_n\vert C, F_1, F_2, F_3,\dots,F_{n-1}).
\end{aligned}
$$
现在"朴素"的条件独立假设开始发挥作用:假设每个特征$F_i$对于其他特征$F_j,j\neq i$是条件独立的.这就意味着
$$ p(F_i \vert C, F_j) = p(F_i \vert C)\, $$
对于$i\ne j$,所以联合分布模型可以表达为
$$
\begin{aligned}
p(C \vert F_1, \dots, F_n) 
& \varpropto p(C) \ p(F_1\vert C) \ p(F_2\vert C) \ p(F_3\vert C) \ \cdots\, \\
& \varpropto p(C) \prod_{i=1}^n p(F_i \vert C).\,
\end{aligned}
$$
这意味着上述假设下,类变量C的条件分布可以表达为:
$$ p(C \vert F_1,\dots,F_n) = \frac{1}{Z}  p(C) \prod_{i=1}^n p(F_i \vert C) $$
其中$Z$(证据因子)是一个只依赖与$F_1,\dots,F_n$等的缩放因子,当特征变量的值已知时是一个常数.

如果类的先验概率未知, 则通常我们假定这些类是等概率的, 即
$ P(C_1)= P(C_2)= \cdots P(C_m) $

记$X = (\vecteur(x))$ 为n个属性 $\vecteur{F}$对元组的n 个测量.\\
若$F_k$ 是分类属性, 则$P(F_k \vert C)$ 很容易计算\\
若$F_k$ 是连续属性, 则需要多做一点工作, 通常\textbf{假定连续值属性服从均值为$\mu$, 标准差为$\sigma$的高斯分布}, 由下式定义
$$ g(x,\mu, \sigma) = {1 \over \sigma\sqrt{2\pi} }\,e^{- {{(x-\mu )^2 \over 2\sigma^2}}} $$
因此
$$ P(X_k \vert C) = g(x_k, \mu_C, \sigma_C) $$
其中$\mu_C, \sigma_C$ 分别是C类训练元组属性$F_k$的均值和标准差.

贝叶斯分类还可以用来为不直接使用贝叶斯定理的其他分类法提供理论判定. 
例如, 在某种假定下, 可以证明: 与朴素贝叶斯分类法一样, 许多神经网络和曲线拟合算法输出最大的后验假定.

\bigskip
\textbf{零概率处理}: 由于零概率的出现会将乘积中所有不为零的概率消除, 会产生问题.

一个简单的技巧来避免该问题, 可以假定训练数据库D 很大, 以至于对每个计算加1 造成的估计概率的变化可以忽略不计, 但可以方便的避免概率值为零. 这种概率估计技术成为
\textbf{拉普拉斯校准}.\\
如果对q 个计数都加上1, 则必须记住在用于计算概率的对应分母上加上q.

\subsection{模型评估和选择}
\begin{table}[htbp]
\centering
\caption{混淆矩阵}
\begin{tabular}{|lllll|}
\hline
       &      & \multicolumn{2}{c}{prediction}  &   \\
       &       & yes        & no & total \\
\multirow{2}{*}{actual}   & yes   & TP   & FN & P     \\
       & no    & FP         & TN & N     \\
       & total & P'         & N' & P+N   \\
\hline
\end{tabular}
\end{table}
\end{document}
