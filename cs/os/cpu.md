# CPU 保护模式
[CPU的保护模式](https://www.zhihu.com/question/25272611)

保护模式对应的是实模式.也就是实地址模式, 即CPU发出的访存指令所对应的地址是实际的物理地址.保护模式即然对应的是实地址模式,
那么它保护的也就是实际物理地址, 保护实际的物理地址不被非法访问. 保护模式是在80286时提出, 在80386上大成. 具体就是80286提
了的段式内存管理和80386提出的页式内存管理.

所以所谓的保护物理内存的基本措施是什么?
就是段页式内存管理方式啊.

- 使用了段式内存管理之后, cpu使用的就是逻辑地址, 要经过段描述符表翻译才能访问实际的物理地址, 而
  段描述符表只有系统内核才能修改. 这就保证了一个进程只能访问内核分配给他的段上的物理内存.
- 使用了页式内存管理之后, cpu使用的就虚拟地址, 虚拟地址要经过页表翻译之后才能访问实际的物理地址上的数据, 而段表只有系统
  内核才能修改. 这就保证了一个进程只能访问内核分配给他的页上的物理内存.

两段文字貌似长得差不多哦. 实际上段式内存管理和页式内存管理的确就是差不多的东西. 所以windows两个都用, 采用段页式内存管理,
而linux觉得完全没有必要使用段式内存管理, 页式就够了, 所以就使用了页式内存管理.

针对x86系统的话, 段式内存管理和页式内存管理是与80286提出的其他机制, 如任务门, 调用门, 中断门等结合起来起段作用的, 所以这
些门也是保护模式的一部分.

[中断门,陷阱门,调用门,任务门](https://www.cnblogs.com/diegodu/p/3949703.html)

如果在程序中需要执行一些特权指令的话, 程序必须转入到ring0. 由于用户程序执行特权指令可能会破坏系统资源, 故出于保护和稳定
的目的, 操作系统通过"门"机制向用户态程序提供必要的服务.
在x86种有四种门:中断门, 陷阱门, 调用门, 任务门, 这些是cpu从硬件层提供的支持.

这四个门就是让CPU找到到哪里去执行异常或中断的处理代码,是中断和异常处理机制
至于为什么分成四种门,是因为具体处理异常中断时有一些区别如陷阱可以嵌套发生,而中断默认情况下不可以嵌套,要要了解中断异常的
类别选择合适的门来处理.

[x86特权级别(运行级别)](https://zhuanlan.zhihu.com/p/55478625)

x86特权级别(运行级别), 即操作系统和CPU一起合作来限制用户模式程序所能做的事情.
有4个特权级别, 编号为0(特权最大) 到3(特权最小), 以及3个受保护的主要资源: 内存, I/O端口和执行某些机器指令的能力.

在任何给定的时间,x86 CPU都以特定的特权级别运行,这决定了代码可以做什么和不能做什么.
这些特权级别通常被描述为保护环,最里面的环对应于最高的特权. 大多数现代x86内核只使用两个特权级别, 0和3.

# [cpu三大架构 numa smp mpp](https://www.jianshu.com/p/81233f3c2c14)
## smp
SMP(Symmetric Multiprocessing), 对称多处理器. 顾名思义, 在SMP中所有的处理器都是对等的, 它们通过总线连接共享同一块物理内
存,这也就导致了系统中所有资源(CPU,内存,I/O等)都是共享的,
当我们打开服务器的背板盖,如果发现有多个cpu的槽位,但是却连接到同一个内存插槽的位置,那一般就是smp架构的服务器,
日常中常见的pc啊,笔记本啊,手机还有一些老的服务器都是这个架构,其架构简单,但是拓展性能非常差,
从linux 上也能看到:
```bash
ls /sys/devices/system/node/
```
如果只看到一个node0 那就是smp架构.
可以看到只有仅仅一个node,经过大神们的测试发现,2至4个CPU比较适合smp架构.

## NUMA
NUMA(Non-Uniform Memory Access), 非均匀访问存储模型,这种模型的是为了解决smp扩容性很差而提出的技术方案,
如果说smp 相当于多个cpu 连接一个内存池导致请求经常发生冲突的话,numa 就是将cpu的资源分开,以node 为单位进行切割,每个node
里有着独有的core, memory 等资源, 这也就导致了cpu在性能使用上的提升,
但是同样存在问题就是2个node 之间的资源交互非常慢,当cpu增多的情况下,性能提升的幅度并不是很高.所以可以看到很多明明有很多
core的服务器却只有2个node区.

[NUMA架构的CPU -- 你真的用好了么?](http://cenalulu.github.io/linux/numa)

NUMA中,虽然内存直接attach在CPU上,但是由于内存被平均分配在了各个die上.
只有当CPU访问自身直接attach内存对应的物理地址时,才会有较短的响应时间(后称Local Access).
而如果需要访问其他CPU attach的内存的数据时,就需要通过inter-connect通道访问,响应时间就相比之前变慢了(后称Remote Access).
所以NUMA(Non-Uniform Memory Access)就此得名.

下面这个命令可以很简单的看出cpu的架构以及配置
```plain
# lscpu
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                8
On-line CPU(s) list:   0-7
Thread(s) per core:    1 #每个core 有几个线程
Core(s) per socket:    4 #每个槽位有4个core
Socket(s):             2 #服务器面板上有2个cpu 槽位
NUMA node(s):          2 #nodes的数量
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 44
Stepping:              2
CPU MHz:               2128.025
BogoMIPS:              4256.03
Virtualization:        VT-x
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              8192K
NUMA node0 CPU(s):     0,2,4,6 #对应的core
NUMA node1 CPU(s):     1,3,5,7
```

由于numa 架构经常会有内存分配不均匀的情况,常常需要手动干预,除了代码以外,还有linux命令进行cpu的绑定:
```bash
taskset -cp 1,2 25718 #将进程ID 25718 绑定到cpu的第1和第2个core 里
```

假设你是Linux教父Linus,对于NUMA架构你会做哪些优化?下面这点是显而易见的:
既然CPU只有在Local-Access时响应时间才能有保障,那么我们就尽量把该CPU所要的数据集中在他local的内存中就OK啦.
没错,事实上Linux识别到NUMA架构后,默认的内存分配方案就是:优先尝试在请求线程当前所处的CPU的Local内存上分配空间.如果local内
存不足,优先淘汰local内存中无用的Page(Inactive,Unmapped).

## MPP
MPP (Massive Parallel Processing), 这个其实可以理解为刀片服务器,每个刀扇里的都是一台独立的smp架构服务器,且每个刀扇之间均
有高性能的网络设备进行交互,保证了smp服务器之间的数据传输性能.
相比numa 来说更适合大规模的计算,唯一不足的是,当其中的smp 节点增多的情况下,与之对应的计算管理系统也需要相对应的提高.

