\section{Logistic Regression}
For classification, not for regression

\href{http://blog.csdn.net/zouxy09/article/details/20319673}{机器学习算法与Python实践之(七)逻辑回归(Logistic Regression)}

\subsection{Notation}
$(x, y), x \in R^{n_x}, y \in {0, 1}$

$$
X_{n \times m} =
\begin{bmatrix}
	\vdots & \vdots & \ldots & \vdots \\
	x^{(1)} & x^{(2)} & \ldots & x^{(m)} \\
	\vdots & \vdots & \ldots & \vdots
\end{bmatrix}
$$

$m$ training examples, each example has a feature with $n$ dimension.

$$
Y_{1 \times m} =
\begin{bmatrix}
	y^{(1)} & y^{(2)} & \ldots & y^{(m)}
\end{bmatrix}
$$

\subsection{Problem}
Given $x \in R^{n_x}$, define $\hat{y} = P(y = 1 | x)$ \\
parameters: $w \in R^{n_x}, b \in R$ \\
output: $\hat{y} = w^{T} \cdot x + b$ \\
but we want $\hat{y} \in (0, 1)$, so we add a sigmoid function, thus $$\hat{y} = \sigma(z) = \sigma(w^T \cdot x + b)$$ with $$\sigma(z) = \dfrac{1}{1 + \exp(-z)}$$

Given ${(x^{(1)}, y^{(1)}) \ldots (x^{(m)}, y^{(m)})}$, want $\hat{y}^{(i)} \simeq y^{(i)}$

\subsection{Loss function and Cost function}
we can define $L(\hat{y}, y) = \dfrac{1}{2} (\hat{y} - y)^2$.
but people do not usually use this, because its cost function is not convex, so it will have many local minimum.

we define loss function: $$L(\hat{y}, y) = -[y \log \hat{y} + (1 - y) \log (1 - \hat{y})]$$

if $y = 1, L(\hat{y}, y) = - \log \hat{y}$,
we want loss function $L(\hat{y}, y)$ small $\Leftrightarrow \log \hat{y}$ large $\Leftrightarrow \hat{y}$ large $\Leftrightarrow \hat{y} \simeq 1$ as $\hat{y} = \sigma(z) \in (0, 1)$

if $y = 0, L(\hat{y}, y) = - \log (1 - \hat{y})$,
we want loss function $L(\hat{y}, y)$ small $\Leftrightarrow \log (1 - \hat{y})$ large $\Leftrightarrow \hat{y}$ small $\Leftrightarrow \hat{y} \simeq 0$ as $\hat{y} = \sigma(z) \in (0, 1)$

\red{Loss function is defined with respect to a single example.}

\red{Cost function is defined with respect to the entire training example set}.

Cost function:
$$
J(w, b)
=   \dfrac{1}{m} \sum_{i = 1}^m L(\hat{y}^{(i)}, y^{(i)})
= - \dfrac{1}{m} \sum_{i = 1}^m [y^{(i)} \log \hat{y}^{(i)} + (1 - y^{(i)}) \log (1 - \hat{y}^{(i)})]
$$

Want to find $w, b$ to minimize $J(w, b)$

\subsection{Gradient Descent}
one variable function $y = f(x)$.

repeat: $$x := x - \alpha \dfrac{df(x)}{dx}$$ with $\alpha$ as the learning rate.

in code, we use $dx$ to denote $\dfrac{df(x)}{dx}$, so $x := x - \alpha dx$

forward propagation, we compute the output of neural network.

backward propagation, we compute the gradients.

\subsection{Computation Graph}
$$J(a, b, c) = 3(a + bc)$$
we define:
$$
\left\{
\begin{aligned}
& u = bc \\
& v = a + u \\
& J = 3v
\end{aligned}
\right.
$$

\write18{wget https://i.imgbox.com/rV4oc2HX.png -O computation-graph.png}
\begin{figure}[htbp]
	\centering
	\includegraphics[scale = 0.4]{computation-graph}\\
	\caption{Computation Graph ex}\label{fig.computation-graph}
\end{figure}

we compute the derivatives from right to left:
$$
\left\{
\begin{aligned}
& dv = \dfrac{\partial J}{\partial v} = 3 \\
& da = \dfrac{\partial J}{\partial a} = \dfrac{\partial J}{\partial v} \times \dfrac{\partial v}{\partial a} = 3 \times 1 = 3 \\
& du = \dfrac{\partial J}{\partial u} = \dfrac{\partial J}{\partial v} \times \dfrac{\partial v}{\partial u} = 3 \times 1 = 3 \\
& db = \dfrac{\partial J}{\partial b} = \dfrac{\partial J}{\partial u} \times \dfrac{\partial u}{\partial b}= 3 \times c = 3c = 6 \\
& dc = \dfrac{\partial J}{\partial c} = \dfrac{\partial J}{\partial u} \times \dfrac{\partial u}{\partial c}= 3 \times b = 3b = 9
\end{aligned}
\right.
$$

\subsubsection{Computation Graph on Logistic Regression}
we now have:
$$
\left\{
\begin{aligned}
& z = w^T \cdot x + b \\
& \hat{y} = a = \sigma(z) \\
& L(a, y) = - [y \log a + (1 - y) \log (1 - a)]
\end{aligned}
\right.
$$

we can compute the derivatives:
$$
\dfrac{\partial L(a, y)}{\partial a}
= - [y \cdot \dfrac{1}{a} + (1 - y) \cdot \frac{1}{1 - a} \cdot -1]
= - \dfrac{y}{a} + \dfrac{1 - y}{1 - a}
$$

$$ a = \sigma(z) = \dfrac{1}{1 + \exp(-z)} = 1 - \dfrac{1}{1 + \exp(z)} $$

$$
\dfrac{da}{dz}
= -1 \cdot -1 \cdot (1 + \exp(z))^{-2} \cdot \exp(z)
= \dfrac{\exp(z)}{(1 + \exp(z))^2}
= \dfrac{\exp(-z)}{(1 + \exp(-z))^2}
= \dfrac{1 + \exp(-z) - 1}{(1 + \exp(-z))^2}
= a - a^2
$$

The computation graph of Logistic Regression is as follows:
$$
\begin{bmatrix}
x_1 \\
w_1 \\
x_2 \\
w_2 \\
b
\end{bmatrix}
\Rightarrow z = w_1 x_1 + w_2 x_2 + b
\Rightarrow a = \sigma(z)
\Rightarrow L(a, y)
$$

We compute their derivatives:
$$
\left\{
\begin{aligned}
	& \dfrac{dL}{da} = - \dfrac{y}{a} + \dfrac{1 - y}{1 - a} \\
	& \dfrac{dL}{dz} = \dfrac{dL}{da} \cdot \dfrac{da}{dz} = (- \dfrac{y}{a} + \dfrac{1 - y}{1 - a}) \cdot (a - a^2) = a - y \\
	& \dfrac{dL}{dw_1} = \dfrac{dL}{dz} \cdot \dfrac{dz}{dw_1} = x_1 \cdot \dfrac{dL}{dz} \\
	& \dfrac{dL}{dw_2} = \dfrac{dL}{dz} \cdot \dfrac{dz}{dw_2} = x_2 \cdot \dfrac{dL}{dz} \\
	& \dfrac{dL}{db} = \dfrac{dL}{dz} \cdot \dfrac{dz}{db} = \dfrac{dL}{dz}
\end{aligned}
\right.
$$

In code, we denote:
$$
\left\{
\begin{aligned}
	& dw_1 = x_1 dz \\
	& dw_2 = x_2 dz \\
	& db = dz
\end{aligned}
\right.
\Rightarrow
\left\{
\begin{aligned}
	& w_1 := w_1 - \alpha dw_1 \\
	& w_2 := w_2 - \alpha dw_2 \\
	& b := b - \alpha db
\end{aligned}
\right.
$$

\subsection{Logistic Regression on $m$ examples}
The cost function: $$ J(w, b) = \dfrac{1}{m} \sum_{i = 1}^m L(a^{(i)}, y^{(i)}) $$

The gradient on $w_1$ is $$ \dfrac{J(w, b)}{dw_1} = \dfrac{1}{m} \sum_{i = 1}^m \dfrac{\partial}{\partial w_1} L(a^{(i)}, y^{(i)}) = \dfrac{1}{m} \sum_{i = 1}^m dw^{(i)}_1 $$

pseudo code of a single step of gradient descent on $m$ examples: \\
init $J = 0, dw_1 = 0, dw_2 = 0, db = 0$ \\
For $i = 1$ to $m$ \\
\verb+    + $z^{(i)} = w^T \cdot x^{(i)} + b$ \\
\verb+    + $a^{(i)} = \sigma(z^{(i)})$ \\
\verb+    + $J += -[y^{(i)} \log a^{(i)} + (1 - y^{(i)}) \log (1 - a^{(i)})]$ \\
\verb+    + $dz^{(i)} = a^{(i)} - y^{(i)}$ \\
\verb+    + $dw_1 += x^{(i)}_1 dz^{(i)}$, suppose we are in dimention 2 \\
\verb+    + $dw_2 += x^{(i)}_2 dz^{(i)}$ \\
\verb+    + $db += dz^{(i)}$ \\
$J /= m$ \\
$dw_1 /= m$ \\
$dw_2 /= m$ \\
$db /= m$ \\
$w_1 := w_1 - \alpha dw_1$ \\
$w_2 := w_2 - \alpha dw_2$ \\
$b := b - \alpha db$

假设我们的样本是$\{x, y\}$,y是0或者1,表示正类或者负类,x是我们的m维的样本特征向量.
那么这个样本x属于正类,也就是$y = 1$的概率可以通过下面的逻辑函数来表示:
$$
P(y = 1| x; \theta) = \sigma(\theta^T x) = \frac{1}{1 + exp(- \theta^T x)}
$$
这里$\theta$是模型参数,也就是回归系数,$\sigma$是sigmoid函数.
实际上这个函数是由下面的对数几率(也就是$x$属于正类的可能性和负类的可能性的比值的对数)变换得到的:
$$
log it(x)
= ln \frac{P(y = 1 | x)}{P(y = 0 | x)}
= ln \frac{P(y = 1 | x)}{1 - P(y = 1 | x)}
= \theta^T x
= \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_m x_m
$$

模型选好了,只是模型的参数$\theta$还是未知的,我们需要用我们收集到的数据来训练求解得到它.那我们下一步要做的事情就是建立代价函数了.
LogisticRegression最基本的学习算法是最大似然.
假设我们有$n$个独立的训练样本$\{(x_1, y_1) ,(x_2, y_2), \ldots ,(x_n, y_n)\},y = \{0, 1\}$.
那每一个观察到的样本$(x_i, y_i)$出现的概率是:
$$
P(x_i, y_i) = P(y_i = 1 | x_i)^{y_i} (1 - P(y_i = 1 | x_i))^{1 - y_i}
$$
不管$y_i$是$0$还是$1$,上面得到的数,都是$(x_i, y_i)$出现的概率.

那我们的整个样本集,也就是$n$个独立的样本出现的似然函数为(因为每个样本都是独立的,所以$n$个样本出现的概率就是他们各自出现的概率相乘):
$$
L(\theta)
= \prod_{i = 1}^{n} P(x_i, y_i)
$$

那最大似然法就是求模型中使得似然函数最大的系数取值$\theta^*$.这个最大似然就是我们的代价函数(cost function)了.

先尝试对上面的代价函数求导,看导数为0的时候可不可以解出来,也就是有没有解析解,有这个解的时候,就皆大欢喜了,一步到位.如果没有就需要通过迭代了,耗时耗力.

对$L$ 取对数, 并将结果重新设为$L$.
$$
\begin{aligned}
L(\theta)
& = ln \prod_{i = 1}^{n} P(x_i, y_i) \\
& = ln \prod_{i = 1}^{n} P(y_i = 1 | x_i)^{y_i} (1 - P(y_i = 1 | x_i))^{1 - y_i} \\
& = \sum_{i = 1}^{n} (y_i ln P(y_i = 1 | x_i) + (1 - y_i) ln (1 - P(y_i = 1 | x_i))) \\
& = \sum_{i = 1}^{n} (y_i ln \frac{P(y_i = 1 | x_i)}{1 - P(y_i = 1 | x_i)} + ln (1 - P(y_i = 1 | x_i))) \\
& = \sum_{i = 1}^{n} (y_i (\theta^T x_i) + ln (1 - P(y_i = 1 | x_i))) \\
& = \sum_{i = 1}^{n} (y_i (\theta^T x_i) + ln (1 - \frac{1}{1 + exp(- \theta^T x_i)})) \\
& = \sum_{i = 1}^{n} (y_i (\theta^T x_i) - ln (1 + exp(\theta^T x_i)))
\end{aligned}
$$
这里 $\theta$是一个vector, 不能直接对它求导, 但是我们可以把它想象成一个scalaire, 求导之后得到:
$$
\frac{\partial L(\theta)}{\partial \theta}
= \sum_{i = 1}^n (y_i x_i - \frac{exp(\theta^T x_i)}{1 + exp(\theta^T x_i)} x_i)
= \sum_{i = 1}^n (y_i - \sigma(\theta^T x_i)) x_i
$$
然后我们令该导数为$0$, 你会很失望的发现, 它无法解析求解. 所以没办法了, 只能借助迭代来搞定了. 这里选用了经典的梯度下降算法.

对logistic Regression来说, 梯度下降算法如下
$$
\theta^{t + 1}
= \theta^t - \alpha \frac{\partial L(\theta)}{\partial \theta}
= \theta^t - \alpha \sum_{i = 1}^n (y_i - \sigma(\theta^T x_i)) x_i
$$
参数$\alpha$叫学习率

因为本文中是求解的logistic regression 的代价函数是似然函数, 需要最大化似然函数. 所以我们要用的是梯度上升算法, 但原理是一样的, 只是一个是找最大值, 一个是找最小值.
找最大值的方向就是梯度的方向, 最小值的方向就是梯度的负方向.

