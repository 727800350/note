<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!-- Licensed to the Apache Software Foundation (ASF) under http://www.apache.org/licenses/LICENSE-2.0 -->

<configuration>
  <property>
    <name>hbase.rootdir</name>
    <value>file:///data1/data/hbase</value>
    <description>The directory shared by region servers and into which HBase persists. The URL should be 'fully-qualified' to include the filesystem scheme.</description>
  </property>
  <property>
    <name>hbase.zookeeper.property.dataDir</name>
    <value>/data1/data/zookeeper</value>
  </property>
  <property >
    <name>hbase.cluster.distributed</name>
    <value>false</value>
    <description>The mode the cluster will be in. If false, startup will run all HBase and ZooKeeper daemons together in the one JVM.</description>
  </property>
  <property>
    <name>hbase.zookeeper.quorum</name>
    <value>localhost</value>
    <description>Comma separated list of servers in the ZooKeeper ensemble.</description>
  </property>
  <!--The above are the important configurations for getting hbase up and running -->

  <!--Master configurations-->
  <property >
    <name>hbase.master.port</name>
    <value>16000</value>
    <description>The port the HBase Master should bind to.</description>
  </property>
  <property>
    <name>hbase.master.info.port</name>
    <value>16010</value>
    <description>The port for the HBase Master web UI. Set to -1 if you do not want a UI instance run.</description>
  </property>
  <property>
    <name>hbase.master.info.bindAddress</name>
    <value>0.0.0.0</value>
    <description>The bind address for the HBase Master web UI </description>
  </property>

  <!--RegionServer configurations-->
  <property>
    <name>hbase.regionserver.port</name>
    <value>16020</value>
    <description>The port the HBase RegionServer binds to.</description>
  </property>
  <property>
    <name>hbase.regionserver.info.port</name>
    <value>16030</value>
    <description>The port for the HBase RegionServer web UI Set to -1 if you do not want the RegionServer UI to run.</description>
  </property>
  <property>
    <name>hbase.regionserver.info.bindAddress</name>
    <value>0.0.0.0</value>
    <description>The address for the HBase RegionServer web UI</description>
  </property>
  <property>
    <name>hbase.regionserver.handler.count</name>
    <value>30</value>
    <description>Count of RPC Listener instances spun up on RegionServers. Same property is used by the Master for count of master handlers.</description>
  </property>
  <property>
    <name>hbase.ipc.server.callqueue.handler.factor</name>
    <value>0.1</value>
    <description>Factor to determine the number of call queues. A value of 0 means a single queue shared between all the handlers. A value of 1 means that each handler has its own queue.</description>
  </property>
  <property>
    <name>hbase.ipc.server.callqueue.read.ratio</name>
    <value>0</value>
    <description>Split the call queues into read and write queues.
      The specified interval (which should be between 0.0 and 1.0) will be multiplied by the number of call queues.
      A value of 0 indicate to not split the call queues, meaning that both read and write requests will be pushed to the same set of queues.
      A value lower than 0.5 means that there will be less read queues than write queues.
      A value of 0.5 means there will be the same number of read and write queues.
      A value greater than 0.5 means that there will be more read queues than write queues.
      A value of 1.0 means that all the queues except one are used to dispatch read requests.

      Example: Given the total number of call queues being 10
      a read.ratio of 0 means that: the 10 queues will contain both read/write requests.
      a read.ratio of 0.3 means that: 3 queues will contain only read requests and 7 queues will contain only write requests.
      a read.ratio of 0.5 means that: 5 queues will contain only read requests and 5 queues will contain only write requests.
      a read.ratio of 0.8 means that: 8 queues will contain only read requests and 2 queues will contain only write requests.
      a read.ratio of 1 means that: 9 queues will contain only read requests and 1 queues will contain only write requests.
    </description>
  </property>
  <property>
    <name>hbase.ipc.server.callqueue.scan.ratio</name>
    <value>0</value>
    <description>Given the number of read call queues, calculated from the total number of call queues multiplied by the callqueue.read.ratio, the scan.ratio property
      will split the read call queues into small-read and long-read queues.
      A value lower than 0.5 means that there will be less long-read queues than short-read queues.
      A value of 0.5 means that there will be the same number of short-read and long-read queues.
      A value greater than 0.5 means that there will be more long-read queues than short-read queues
      A value of 0 or 1 indicate to use the same set of queues for gets and scans.

      Example: Given the total number of read call queues being 8
      a scan.ratio of 0 or 1 means that: 8 queues will contain both long and short read requests.
      a scan.ratio of 0.3 means that: 2 queues will contain only long-read requests and 6 queues will contain only short-read requests.
      a scan.ratio of 0.5 means that: 4 queues will contain only long-read requests and 4 queues will contain only short-read requests.
      a scan.ratio of 0.8 means that: 6 queues will contain only long-read requests and 2 queues will contain only short-read requests.
    </description>
  </property>
  <property>
    <name>hbase.regionserver.global.memstore.size</name>
    <value></value>
    <description>Maximum size of all memstores in a region server before new updates are blocked and flushes are forced. Defaults to 40% of heap (0.4).
      Updates are blocked and flushes are forced until size of all memstores in a region server hits hbase.regionserver.global.memstore.size.lower.limit.
      The default value in this configuration has been intentionally left emtpy in order to honor the old hbase.regionserver.global.memstore.upperLimit property if present.
    </description>
  </property>
  <property>
    <name>hbase.regionserver.global.memstore.size.lower.limit</name>
    <value></value>
    <description>Maximum size of all memstores in a region server before flushes are forced.
      Defaults to 95% of hbase.regionserver.global.memstore.size (0.95).
      A 100% value for this value causes the minimum possible flushing to occur when updates are blocked due to memstore limiting.
      The default value in this configuration has been intentionally left emtpy in order to honor the old hbase.regionserver.global.memstore.lowerLimit property if present.
    </description>
  </property>
  <property>
    <name>hbase.regionserver.region.split.policy</name>
    <value>org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy</value>
    <description>A split policy determines when a region should be split. The various other split policies that are available currently are
    ConstantSizeRegionSplitPolicy, DisabledRegionSplitPolicy, DelimitedKeyPrefixRegionSplitPolicy, KeyPrefixRegionSplitPolicy etc.
    </description>
  </property>
  <property>
    <name>hbase.regionserver.regionSplitLimit</name>
    <value>1000</value>
    <description>Limit for the number of regions after which no more region splitting should take place.
      This is not hard limit for the number of regions but acts as a guideline for the regionserver to stop splitting after a certain limit. Default is set to 1000.
    </description>
  </property>

  <!--ZooKeeper configuration-->
  <property>
    <name>zookeeper.session.timeout</name>
    <value>90000</value>
  </property>
  <property>
    <name>hbase.zookeeper.property.clientPort</name>
    <value>2181</value>
    <description>Property from ZooKeeper's config zoo.cfg. The port at which the clients will connect.</description>
  </property>
  <!-- End of properties that are directly mapped from ZooKeeper's zoo.cfg -->

  <!--Client configurations-->
  <property>
    <name>hbase.client.retries.number</name>
    <value>35</value>
  </property>
  <property>
    <name>hbase.client.max.total.tasks</name>
    <value>100</value>
    <description>The maximum number of concurrent tasks a single HTable instance will send to the cluster.</description>
  </property>
  <property>
    <name>hbase.client.max.perserver.tasks</name>
    <value>5</value>
    <description>The maximum number of concurrent tasks a single HTable instance will send to a single region server.</description>
  </property>
  <property>
    <name>hbase.client.max.perregion.tasks</name>
    <value>1</value>
    <description>The maximum number of concurrent connections the client will maintain to a single Region.
      That is, if there is already hbase.client.max.perregion.tasks writes in progress for this region, new puts won't be sent to this region until some writes finishes.
    </description>
  </property>

  <property>
    <name>hbase.server.thread.wakefrequency</name>
    <value>10000</value>
    <description>Time to sleep in between searches for work (in milliseconds). Used as sleep interval by service threads such as log roller.</description>
  </property>

  <property>
    <name>hbase.hregion.memstore.flush.size</name>
    <value>134217728</value>
    <description>Memstore will be flushed if size of the memstore exceeds this number of bytes. Value is checked by a thread that runs every hbase.server.thread.wakefrequency.</description>
  </property>
  <property>
    <name>hbase.hregion.percolumnfamilyflush.size.lower.bound</name>
    <value>16777216</value>
    <description>If FlushLargeStoresPolicy is used, then every time that we hit the total memstore limit, we find out all the column families whose memstores exceed this value,
      and only flush them, while retaining the others whose memstores are lower than this limit.
      If none of the families have their memstore size more than this, all the memstores will be flushed (just as usual).
      This value should be less than half of the total memstore threshold (hbase.hregion.memstore.flush.size).
    </description>
  </property>
  <property>
    <name>hbase.hregion.memstore.block.multiplier</name>
    <value>4</value>
    <description>Block updates if memstore has hbase.hregion.memstore.block.multiplier times hbase.hregion.memstore.flush.size bytes.</description>
  </property>
  <property>
    <name>hbase.hregion.memstore.mslab.enabled</name>
    <value>true</value>
    <description>Enables the MemStore-Local Allocation Buffer, a feature which works to prevent heap fragmentation under heavy write loads.
      This can reduce the frequency of stop-the-world GC pauses on large heaps.
    </description>
  </property>
  <property>
    <name>hbase.hregion.max.filesize</name>
    <value>10737418240</value>
    <description>Maximum HStoreFile size. If any one of a column families' HStoreFiles has grown to exceed this value, the hosting HRegion is split in two.</description>
  </property>
  <property>
    <name>hbase.hregion.majorcompaction</name>
    <value>604800000</value>
    <description>The time (in miliseconds) between 'major' compactions of all HStoreFiles in a region. Default: Set to 7 days.
      Major compactions tend to happen exactly when you need them least so enable them such that they run at off-peak for your deploy;
      or, since this setting is on a periodicity that is unlikely to match your loading, run the compactions via an external invocation out of a cron job or some such.
    </description>
  </property>
  <property>
    <name>hbase.hstore.compactionThreshold</name>
    <value>3</value>
    <description>If more than this number of HStoreFiles in any one HStore (one HStoreFile is written per flush of memstore) then a compaction is run to rewrite all HStoreFiles files as one.
      Larger numbers put off compaction but when it runs, it takes longer to complete.
    </description>
  </property>
  <property>
    <name>hbase.hstore.flusher.count</name>
    <value>2</value>
    <description>The number of flush threads. With less threads, the memstore flushes will be queued.</description>
  </property>
  <property>
    <name>hbase.hstore.blockingStoreFiles</name>
    <value>10</value>
    <description>Updates are bloked until a compaction is completed or untilblockingWaitTime has been exceeded if any store of this region has more than this number of store files.</description>
  </property>
  <property>
    <name>hbase.hstore.blockingWaitTime</name>
    <value>90000</value>
    <description>The time an HRegion will block updates for after hitting the StoreFile limit defined by hbase.hstore.blockingStoreFiles.
      After this time has elapsed, the HRegion will stop blocking updates even if a compaction has not been completed.
    </description>
  </property>
  <property>
    <name>hbase.hstore.compaction.max</name>
    <value>10</value>
    <description>Max number of HStoreFiles to compact per 'minor' compaction.</description>
  </property>
  <property>
    <name>hbase.hstore.compaction.kv.max</name>
    <value>10</value>
    <description>How many KeyValues to read and then write in a batch when flushing or compacting. Do less if big KeyValues and problems with OOME. Do more if wide, small rows.</description>
  </property>
  <property>
    <name>hbase.storescanner.parallel.seek.enable</name>
    <value>false</value>
  </property>
  <property>
    <name>hbase.storescanner.parallel.seek.threads</name>
    <value>10</value>
  </property>
  <property>
    <name>hfile.block.cache.size</name>
    <value>0.4</value>
    <description>Percentage of maximum heap (-Xmx setting) to allocate to block cache used by HFile/StoreFile. Default of 0.4 means allocate 40%.
        Set to 0 to disable but it's not recommended; you need at least enough cache to hold the storefile indices.
    </description>
  </property>
  <property>
    <name>hfile.block.index.cacheonwrite</name>
    <value>false</value>
    <description>This allows to put non-root multi-level index blocks into the block cache at the time the index is being written.</description>
  </property>
  <property>
    <name>hfile.index.block.max.size</name>
    <value>131072</value>
    <description>When the size of a leaf-level, intermediate-level, or root-level index block in a multi-level block index grows to this size,
      the block is written out and a new block is started.
    </description>
  </property>
  <property>
    <name>hfile.format.version</name>
    <value>3</value>
  </property>

  <!-- Coprocessor -->
  <property>
    <name>hbase.coprocessor.enabled</name>
    <value>true</value>
    <description>Enables or disables coprocessor loading. If 'false' (disabled), any other coprocessor related configuration will be ignored. </description>
  </property>
  <property>
    <name>hbase.coprocessor.user.enabled</name>
    <value>true</value>
    <description>Enables or disables user (aka. table) coprocessor loading. If 'false' (disabled), any table coprocessor attributes in table descriptors will be ignored.
      If "hbase.coprocessor.enabled" is 'false' this setting has no effect.
    </description>
  </property>
  <property>
    <name>hbase.coprocessor.region.classes</name>
    <value></value>
    <description>A comma-separated list of Coprocessors that are loaded by default on all tables. For any override coprocessor method, these classes will be called in order.
      After implementing your own Coprocessor, just put it in HBase's classpath and add the fully qualified class name here.
    </description>
  </property>
  <property>
    <name>hbase.coprocessor.master.classes</name>
    <value></value>
    <description>A comma-separated list of org.apache.hadoop.hbase.coprocessor.MasterObserver coprocessors that are loaded by default on the active HMaster process.</description>
  </property>
  <property>
  <property>

  <!-- Checksum -->
  <property>
    <name>hbase.regionserver.checksum.verify</name>
    <value>true</value>
    <description>If set to true (the default), HBase verifies the checksums for hfile blocks. HBase writes checksums inline with the data when it writes out hfiles.
      HDFS (as of this writing) writes checksums to a separate file than the data file necessitating extra seeks. Setting this flag saves some on i/o.
      Checksum verification by HDFS will be internally disabled on hfile streams when this flag is set.
      If the hbase-checksum verification fails, we will switch back to using HDFS checksums (so do not disable HDFS checksums! And besides this feature applies to hfiles only, not to WALs).
    </description>
  </property>
  <property>
    <name>hbase.hstore.bytes.per.checksum</name>
    <value>16384</value>
    <description>Number of bytes in a newly created checksum chunk for HBase-level checksums in hfile blocks.</description>
  </property>
  <property>
    <name>hbase.hstore.checksum.algorithm</name>
    <value>CRC32C</value>
    <description>Name of an algorithm that is used to compute checksums. Possible values are NULL, CRC32, CRC32C.</description>
  </property>
</configuration>

