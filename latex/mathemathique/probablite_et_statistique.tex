% !Mode:: "TeX:UTF-8"
\documentclass{article}
\input{../../public/package}
\input{../../public/article}
\begin{document}
\title{Probablit\'e et Statistiques}
\maketitle
\tableofcontents
\newpage

\section{Probabilit\'e}
\subsection{Loi des grands nombres}
On consid\`ere une suite $(X_n)_{n\in\N^*}$ de variables al\'eatoires ind\'ependantes d\'efinies sur un m\^eme espace probabilis\'e, ayant m\^eme variance finie et m\^eme esp\'erance not\'ees respectivement $V(X)$ et $E(X)$. 

La loi faible des grands nombres stipule que, pour tout r\'eel $\epsilon$ strictement positif, la probabilit\'e que la moyenne empirique $Y_n \equiv \bar x= \frac{1}{n} \sum_{i=1}^{n} X_i$ s'\'eloigne de l'esp\'erance d'au moins $\epsilon$ tend vers $0$ quand $n$ tend vers l'infini.

\begin{theorem}
$\forall\varepsilon>0,\quad \lim_{n \to +\infty} \mathbb{P}\left(\left|\frac{X_1+X_2+\cdots+X_n}{n} -E(X)\right| \geqslant \varepsilon\right) = 0$
\end{theorem}
Autrement dit, $(Y_n)_{n\in\N^*}$ converge en probabilit\'e vers $E(X)$. Ce r\'esultat est tr\`es important en statistique, puisqu'il assure que la moyenne empirique est un estimateur convergent de l'esp\'erance.

\subsection{Th\'eor\`eme central limite}
Le th\'eor\`eme central limite \'etablit la convergence en loi de la somme d'une suite de variables al\'eatoires vers la loi normale. Intuitivement, ce r\'esultat affirme que toute somme de variables al\'eatoires ind\'ependantes et identiquement distribu\'ees tend vers une variable al\'eatoire gaussienne.

Soit $X_1, X_2, \cdots$ une suite de variables al\'eatoires r\'eelles d\'efinies sur le m\^eme espace de probabilit\'e, ind\'ependantes et identiquement distribu\'ees suivant la m\^eme loi $D$. Supposons que l'esp\'erance $\mu$ et l'\'ecart-type $\sigma$ de $D$ existent et soient finis avec $\sigma \neq 0$.

Consid\'erons la somme
$$ S_n = X_1 + X_2 + \cdots + X_n.  $$
Alors l'esp\'erance de $S_n$ est $n\mu$ et
son \'ecart-type vaut $\sigma \sqrt{n}$.
De plus, quand $n$ est assez grand, la loi normale $\mathcal{N}(n \mu,n \sigma^2)$ est une bonne approximation de la loi de $S_n$.

Afin de formuler math\'ematiquement cette approximation, nous allons poser
$$ X_n = S_n/n = (X_1 + X_2 + \cdots + X_n)/n $$
et
$$ \mathrm{Z}_n = \frac{\mathrm{S}_n - n \mu}{\sigma \sqrt{n}} = \frac{\overline{\mathrm{X}}_n - \mu}{\sigma/\sqrt{n}}, $$
de sorte que l'esp\'erance et l'\'ecart-type de $Zn$ valent respectivement $0$ et $1$ : la variable est ainsi dite centr\'ee et r\'eduite.

Le th\'eor\`eme central limite stipule alors que la suite de variables al\'eatoires $Z1, Z2,..., Zn,... $converge en loi vers une variable al\'eatoire $Z$, d\'efinie sur le m\^eme espace probabilis\'e, et de loi normale centr\'ee r\'eduite $\mathcal{N} (0, 1)$ lorsque $n$ tend vers l'infini.

\section{Estimation}
Pour caract\'eriser l'estimation de $\theta$, on verra les crit\`eres suivants:
\begin{itemize}
\item Convergence lorsque $n \to \infty$ (forte consistence)
\item Convergence en moyenne (biais)
\item Maximisation probabiliste (maximum de vraisemblance)
\item Crit\`ere bas\'e sur la variance (risque)
\end{itemize}

\begin{definition}
\textbf{Biais d'un estimateur}:
La diff\'erence entre l'esp\'erance de l'estimateur et la vraie valeur du param\`etre estim\'e.

Si $\hat \theta$ est l'estimateur de $\theta\,,  \text{Biais}(\hat\theta)\equiv E[\hat\theta]-\theta$
\end{definition}

\subsection{Maximum de vraisemblance}
极大似然原理的直观想法是:一个随机试验如有若干个可能的结果$A,B,C,\cdots$.若在一次试验中,结果$A$出现,则一般认为试验条件对A出现有利,也即$A$出现的概率很大.

最大似然估计会寻找关于$\theta$的最可能的值(即,在所有可能的$\theta$取值中,寻找一个值使这个采样的"可能性"最大化).这种方法正好同一些其他的估计方法不同,如$\theta$的非偏估计,非偏估计未必会输出一个最可能的值,而是会输出一个既不高估也不低估的$\theta$值.

Soit une famille param\'etr\'ee de distributions de probabilit\'es $D_{\theta}$ dont les \'el\'ements sont associ\'es soit \`a une densit\'e de probabilit\'e connue (distribution continue), soit \`a une fonction de masse connue (distribution discr\`ete), not\'ee $f_{\theta}$. On tire un \'echantillon de $n$ valeurs $x_1, x_2, \cdots, x_n$:de la distribution, et l'on calcule la densit\'e de probabilit\'e associ\'ee aux donn\'ees observ\'ees

$$ f_\theta(x_1,\dots,x_n \mid \theta).\, $$
Ceci \'etant une fonction de $\theta$ avec $x_1, \cdots, x_n$ fix\'es, c'est une vraisemblance.

$$ L(\theta) = f_\theta(x_1,\dots,x_n \mid \theta).\, $$
Lorsque $\theta$ n'est pas observable, la m\'ethode du maximum de vraisemblance utilise les valeurs de $\theta$ qui maximisent $L(\theta)$ estimateur de $\theta$ : c'est l'estimateur du maximum de vraisemblance de $\theta$ not\'e $\widehat{\theta}$. Par exemple dans le cas du produit discret, on effectue un tirage de n valeurs, il faut donc trouver le param\`etre qui maximise la probabilit\'e d'avoir tir\'e ce tirage.

\bigskip
On appelle vraisemblance de $\theta$ au vu des observations $(x_1, \ldots,x_i, \ldots, x_n)$ d'un n-\'echantillon ind\'ependamment et identiquement distribu\'e selon la loi $\mathcal{D}_\theta$, le nombre :
$$ L(x_1, \ldots,x_i, \ldots,x_n;\theta) = f(x_1;\theta) \times f(x_2;\theta) \times \ldots \times f(x_n;\theta) = \prod_{i=1}^n f(x_i;\theta) $$

On cherche à trouver le maximum de cette vraisemblance pour que les probabilit\'es des r\'ealisations observ\'ees soient aussi maximum. Ceci est un probl\`eme d'optimisation. 

On utilise g\'en\'eralement le fait que si L est d\'erivable (ce qui n'est pas toujours le cas) et si L admet un maximum global en une valeur $\theta = \hat \theta$, 
alors la d\'eriv\'ee premi\`ere s'annule en $\theta = \hat \theta $et que la d\'eriv\'ee seconde est n\'egative. \\
R\'eciproquement, si la d\'eriv\'ee premi\`ere s'annule en $\theta = \hat \theta$ et que la d\'eriv\'ee seconde est n\'egative en $\theta = \hat \theta$, 
alors $\theta = \hat \theta $est un maximum local (et non global) de $L(x_1, \ldots, x_i, \ldots,x_n;\theta)$. \\
Il est alors n\'ecessaire de v\'erifier qu'il s'agit bien d'un maximum global. La vraisemblance \'etant positive et le logarithme n\'ep\'erien(naturel) une fonction croissante, il est \'equivalent et souvent plus simple de maximiser le logarithme n\'ep\'erien de la vraisemblance. 

Ainsi en pratique:

La condition n\'ecessaire
$$ \frac{\partial L(x_1, \ldots, x_i, \ldots, x_n;\theta)}{\partial \theta} = 0 $$
ou
$$ \frac{\partial \ln L(x_1, \ldots, x_i, \ldots, x_n;\theta)}{\partial \theta} = 0 $$
permet de trouver la valeur $\theta = \hat \theta$.
$\theta = \hat \theta $est un maximum local si la condition suffisante est remplie au point critique $\theta = \hat \theta $:
$$ \frac{\partial^2 L(x_1, \ldots, x_i, \ldots, x_n;\theta)}{\partial \theta^2} \le 0 $$
ou
$$ \frac{\partial^2 \ln L(x_1, \ldots, x_i, \ldots,x_n;\theta)}{\partial \theta^2} \le 0 $$

\subsubsection{Likelihood function}
The likelihood function is defined differently for discrete and continuous probability distributions.

\textbf{Discrete probability distribution}\\
Let $X$ be a random variable with a discrete probability distribution p depending on a parameter $\theta$. Then the function

$$ \mathcal{L}(\theta |x) = p_\theta (x) = P_\theta (X=x), \, $$
considered as a function of $\theta$, is called the likelihood function (of $\theta$, given the outcome $x$ of $X$). 

Sometimes the probability on the value $x$ of $X$ for the parameter value $\theta$ is written as $P(X=x|\theta)$; often written as $\mathbf{P(X=x;\theta)}$ to emphasize that this value is \textbf{not a conditional probability}, because \textbf{$\theta$ is a parameter and not a random variable}.

\textbf{Continuous probability distribution}\\
Let $X$ be a random variable with a continuous probability distribution with density function $f$ depending on a parameter $\theta$. Then the function
$$ \mathcal{L}(\theta |x) = f_{\theta} (x), \, $$
considered as a function of $\theta$, is called the likelihood function (of $\theta$, given the outcome $x$ of $X$). 

Sometimes the density function for the value $x$ of $X$ for the parameter value $\theta$ is written as $f(x|\theta)$, but should not be considered as a conditional probability density.

The actual value of a likelihood function bears no meaning. Its use lies in comparing one value with another. 
For example, one value of the parameter may be more likely than another, given the outcome of the sample. Or a specific value will be most likely: the maximum likelihood estimate.

\begin{example}
考虑投掷一枚硬币的实验.通常来说,已知投出的硬币正面朝上和反面朝上的概率各自是$p_H = 0.5$,便可以知道投掷若干次后出现各种结果的可能性.比如说,投两次都是正面朝上的概率是0.25.用条件概率表示,就是:
$$ P(\mbox{HH} \mid p_H = 0.5) = 0.5^2 = 0.25 $$
其中H表示正面朝上.

在统计学中,我们关心的是在已知一系列投掷的结果时,关于硬币投掷时正面朝上的可能性的信息.
我们可以建立一个统计模型:假设硬币投出时会有$p_H$  的概率正面朝上,而有$1 - p_H$ 的概率反面朝上.
这时,条件概率可以改写成似然函数:
$$ L(p_H =  0.5 \mid \mbox{HH}) = P(\mbox{HH}\mid p_H = 0.5) =0.25 $$
也就是说,对于取定的似然函数,在观测到两次投掷都是正面朝上时,$p_H = 0.5$ 的似然性是0.25(这并不表示当观测到两次正面朝上时$p_H= 0.5$ 的概率是0.25).

如果考虑$p_H = 0.6$,那么似然函数的值也会改变.
$$ L(p_H = 0.6 \mid \mbox{HH}) = P(\mbox{HH}\mid p_H = 0.6) =0.36 $$
注意到似然函数的值变大了.\\
这说明,如果参数$p_H$ 的取值变成0.6的话,结果观测到连续两次正面朝上的概率要比假设$p_H = 0.5$ 时更大.也就是说,参数$p_H$ 取成0.6 要比取成0.5 更有说服力,更为"合理".
\\(这里的$p_H$ 就是似然函数中的$\theta$, 而连续两次正面朝上HH 就是似然函数中随机变量$X$取得$x$.)

总之,\textbf{似然函数的重要性不是它的具体取值,而是当参数变化时函数到底变小还是变大}.对同一个似然函数,如果存在一个参数值,使得它的函数值达到最大的话,那么这个值就是最为"合理"的参数值.

在这个例子中,似然函数实际上等于:
$$ L(p_H = \theta  \mid \mbox{HH}) = P(\mbox{HH}\mid p_H = \theta) =\theta^2 , 其中0 \le p_H  \le 1.  $$
如果取$p_H = 1$,那么似然函数达到最大值1.也就是说,当连续观测到两次正面朝上时,假设硬币投掷时正面朝上的概率为1是最合理的.

类似地,如果观测到的是三次投掷硬币,头两次正面朝上,第三次反面朝上,那么似然函数将会是:
$$ L(p_H = \theta  \mid \mbox{HHT}) = P(\mbox{HHT}\mid p_H = \theta) =\theta^2(1 - \theta) , 其中T表示反面朝上,0 \le p_H  \le 1.  $$
这时候,似然函数的最大值将会在$p_H = \frac{2}{3}$的时候取到.\\
也就是说,当观测到三次投掷中前两次正面朝上而后一次反面朝上时,估计硬币投掷时正面朝上的概率$p_H = \frac{2}{3}$是最合理的.
\end{example}

\subsection{Borne Cram\'er-Rao}
la borne Cram\'er-Rao exprime une borne inf\'erieure sur la variance d'un estimateur sans biais, bas\'ee sur l'Information de Fisher.
Elle \'enonce que l'inverse de l'information de Fisher, $\mathcal{I}(\theta)$, d'un param\`etre $\theta$, est une borne inf\'erieure de la variance d'un estimateur sans biais de ce param\`etre (not\'e $\widehat{\theta}$).
$$
\mathrm{var} \left(\widehat{\theta}\right)
\geq
\mathcal{I}(\theta)^{-1}
= \mathbb{E}
 \left[
   \left(\frac{\partial}{\partial \theta} \ln L(X;\theta)\right)^2
 \right]^{-1}
$$

\begin{example}
Supposons que $X$ est un vecteur al\'eatoire qui suit une loi normale d' esp\'erance connue $\mu$ et de variance inconnue $\sigma^2$. Consid\'erons $T$ l'estimateur de $\sigma^2$:

$$ T=\frac{\sum_{i=1}^n\left(X_i-\mu\right)^2}{n}.  $$
Alors $T$ est non biais\'e pour $\sigma^2$, car $E[T]=\sigma^2$. Quelle est la variance de $T$ ?

$$
\mathrm{Var}(T) = \frac{\mathrm{var}(X-\mu)^2}{n}=\frac{1}{n}
\left[
E\left\{(X-\mu)^4\right\}-\left(E\left\{(X-\mu)^2\right\}\right)^2
\right]
$$
\todo{上式的第一个等号有点不理解}
$$(Var(X) = E(X^2) - (E(X))^2)$$
Le premier terme est le quatri\`eme moment centr\'e et vaut $3\sigma^4$; le second est le carr\'e de la variance, soit $\sigma^4$. Donc :

$$ \mathrm{var}(T)=\frac{2\sigma^4}{n}.  $$
Quelle est l'information de Fisher de cet exemple ? Le score $V$ est d\'efini par :


$$ V=\frac{\partial}{\partial\sigma^2}\log L(\sigma^2,X) $$
avec L \'etant la fonction de vraisemblance. Donc, dans ce cas,

$$
V=\frac{\partial}{\partial\sigma^2}\log\left[\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(X-\mu)^2/{2\sigma^2}}\right]
=\frac{(X-\mu)^2}{2(\sigma^2)^2}-\frac{1}{2\sigma^2}
$$
L'information de $n$ \'ev\`enements ind\'ependants \'etant seulement $n$ fois l'information d'un seul \'ev\`enement, soit $\frac{n}{2(\sigma^2)^2}$.

L'in\'egalit\'e de Cram\'er-Rao donne :
$$ \mathrm{var}(T)\geq\frac{1}{I}.$$
Dans ce cas, on a donc \'egalit\'e, ce qui montre que l'estimateur est efficace.
\end{example}

\section{Fisher information}
The Fisher information is a way of \textbf{measuring the amount of information that an observable random variable $X$ carries about an unknown parameter $\theta$ upon which the probability of $X$ depends}.

The probability function for $X$, which is also the likelihood function for $\theta$, is a function $f(X; \theta)$; 
it is the probability mass (or probability density) of the random variable $X$ conditional on the value of $\theta$. 

The partial derivative with respect to $\theta$ of the natural logarithm of the likelihood function is called the \textbf{score}.
\\(In statistics, the score, score function, efficient score or informant indicates \textbf{how sensitively a likelihood function $L(\theta; X)$ depends on its parameter $\theta$}. Explicitly, the score for $\theta$ is the gradient of the log-likelihood with respect to $\theta$: $\dfrac{\partial}{\partial\theta} \log L(\theta;X)$.)

Under certain regularity conditions, it can be shown that the first moment of the score (that is, its expected value) is $0$:
$$
\begin{aligned}
\operatorname{E} \left[\left. \frac{\partial}{\partial\theta} \log f(X;\theta)\right|\theta \right]
& = \operatorname{E} \left[\left. \frac{\frac{\partial}{\partial\theta} f(X;\theta)}{f(X; \theta)}\right|\theta \right]\\
& = \int \frac{\frac{\partial}{\partial\theta} f(x;\theta)}{f(x; \theta)} f(x;\theta)\; \mathrm{d}x\\
& = \int \frac{\partial}{\partial\theta} f(x;\theta)\; \mathrm{d}x\\
& = \frac{\partial}{\partial\theta} \int f(x; \theta)\; \mathrm{d}x\\
& = \frac{\partial}{\partial\theta} \; 1 \\
& = 0.
\end{aligned}
$$

The second moment is called the Fisher information:
$$
\mathcal{I}(\theta)=\operatorname{E} \left[\left. \left(\frac{\partial}{\partial\theta} \log f(X;\theta)\right)^2\right|\theta \right] = \int \left(\frac{\partial}{\partial\theta} \log f(x;\theta)\right)^2 f(x; \theta)\; \mathrm{d}x\,,
$$
where, for any given value of $\theta$, the expression $E[...|\theta]$ denotes the conditional expectation over values for $X$ with respect to the probability function $f(x; \theta)$ given $\theta$. Note that $0 \leq \mathcal{I}(\theta) < \infty$. 

A random variable carrying high Fisher information implies that the absolute value of the score is often high. 
The Fisher information is not a function of a particular observation, as the random variable $X$ has been averaged out.

Since the expectation of the score is zero, the Fisher information is also the variance of the score.

If $\log f(x; \theta)$ is \textbf{twice differentiable with respect to $\theta$}, and under certain regularity conditions, then the Fisher information may also be written as[10]

$$ \mathcal{I}(\theta) = - \operatorname{E} \left[\left. \frac{\partial^2}{\partial\theta^2} \log f(X;\theta)\right|\theta \right]\,, $$
since

$$
\frac{\partial^2}{\partial\theta^2} \log f(X;\theta)
= \frac{\frac{\partial^2}{\partial\theta^2} f(X;\theta)}{f(X; \theta)}
\;-\;
\left( \frac{\frac{\partial}{\partial\theta} f(X;\theta)}{f(X; \theta)} \right)^2
= \frac{\frac{\partial^2}{\partial\theta^2} f(X;\theta)}{f(X; \theta)}
\;-\;
\left( \frac{\partial}{\partial\theta} \log f(X;\theta)\right)^2
$$
and

$$
\operatorname{E} \left[\left. \frac{\frac{\partial^2}{\partial\theta^2} f(X;\theta)}{f(X; \theta)}\right|\theta \right]
= \cdots
= \frac{\partial^2}{\partial\theta^2} \int f(x; \theta)\; \mathrm{d}x
= \frac{\partial^2}{\partial\theta^2} \; 1 
= 0.
$$
Thus, the Fisher information is the negative of the expectation of the second derivative with respect to $\theta$ of the natural logarithm of $f$.

Information is \textbf{additive}, in that the information yielded by two independent experiments is the sum of the information from each experiment separately:
$$ \mathcal{I}_{X,Y}(\theta) = \mathcal{I}_X(\theta) + \mathcal{I}_Y(\theta) $$
This result follows from the elementary fact that if random variables are independent, the variance of their sum is the sum of their variances. In particular, the information in a random sample of size $n$ is $n$ times that in a sample of size 1, when observations are independent and identically distributed.
\end{document}
