% !Mode:: "TeX:UTF-8"
\documentclass{article}
\input{../public/package}
\input{../public/article}
\begin{document}
\title{Probablit\'e et Statistiques}
\maketitle
\tableofcontents
\newpage
\section{Mesure}
une tribu ou $\sigma$-algèbre (lire sigma-algèbre) ou plus rarement corps de Borel1 
sur un ensemble X est un ensemble non vide de parties de X, 
stable(closed en anglais) par passage au complémentaire et par union dénombrable (donc aussi par intersection dénombrable).\\
Les tribus permettent de définir rigoureusement la notion d'ensemble mesurable.

\begin{definition}
Soit X un ensemble. On appelle tribu (ou σ-algèbre) sur $X$, un ensemble $\mathcal{A}$ de parties de X($A \subset P(X)$) qui vérifie :
\begin{enumerate}
\item $\mathcal{A} \not=\varnothing$
\item $\forall A \in \mathcal{A}$ ,  ${}^c A \in\mathcal{A}$  (où ${}^cA$ désigne le complémentaire de $A$ dans $X$).
\item si $\forall n \in \mathbb{N}$, $A_n \in\mathcal{A}$  alors  $\bigcup_{n\in\mathbb{N} } A_n \in\mathcal{A}$  (l'union est dite dénombrable parce que l'ensemble des indices l'est).
\end{enumerate}
\end{definition}

\begin{example}
La tribu dite discrète :  $\mathcal A = \mathcal P(X)$  où  $\mathcal P(X)$ représente l'ensemble de toutes les parties de $X$.

Si $X=\{a,b,c,d\}$ alors  $\mathcal A=\{\varnothing, \{a\}, \{b, c, d\}, X\}$ est une tribu sur $X$. C'est la plus petite tribu contenant l'ensemble  $\{a\}$.
\end{example}

$A$ is closed under countable intersections.\\
as $\cap_{i=1}^{\infty} E_i = \cap(E_i^c)^c = (\cup E_i^c)^c \in A$

\begin{definition}
A \textbf{probability measure} $P$ on a measure space $(\Omega, A)$ is a function
$P: A \rightarrow [0,1]$ such that:
\begin{enumerate}
\item $P(\emptyset) = 0$, $P(\Omega) = 1$
\item $P(\cup_{i=1}^{\infty} E_i) = \sum_{i=1}^{\infty}P(E_i)$ for $\forall E_i \in A$ and pairewise disjoint.
\end{enumerate}
\end{definition}

\textbf{Property: Continuity from below}\\
If $E_i \subset E_2 \subset,\ldots $, then $P(\cup_{i=1}^{\infty} E_i) = \lim_{i \to \infty} P(E_i)$

\textbf{Property: Continuity from above}\\
If $E_i \supset E_2 \supset,\ldots $, then $P(\cap_{i=1}^{\infty} E_i) = \lim_{i \to \infty} P(E_i)$

\begin{definition}
A \textbf{CDF} is a fonction $F: R \rightarrow R$, such that:
\begin{enumerate}
\item $x \leq y \Rightarrow F(x) \leq F(y)$ with $x,y \in R$
\item $\lim_{x \to a^{+}}F(x) = F(a)$
\item $\lim_{x \to \infty}F(x) = 1$
\item $\lim_{x \to -\infty}F(x) = 0$
\end{enumerate}
\end{definition}

\begin{theorem}
$$F(x) \equiv P((-\infty, x])$$
defines an equivalence between CDFs $F$ and (Borel) probability measure $P$ on $R$.
\end{theorem}

\begin{definition}
Given $C \subset 2^{\Omega}$, the \textbf{$\sigma-algebra$ generated by $C$}, noted as $\sigma(C)$, is the \textbf{smallest} $\sigma-algebra$ containing $C$.(smallest means: $\sigma(C) = \cap_{\sigma-algebra A \supset C} A$)
\end{definition}

$\sigma(C)$ always exists, because:
\begin{enumerate}
\item $2^{\Omega}$ is a $\sigma-algebra$ $\Rightarrow \sigma(C)$ is not empty
\item Any intersections of $\sigma-algebra$ is a $\sigma-algebra$
\end{enumerate}
\begin{proof}
\end{proof}

\begin{definition}
A measure $\mu$ on $\Omega$ with $\sigma-algebra A$ is a function $\mu: A \rightarrow [0, \infty]$, s.t.
\item $\mu(\emptyset) = 0$
\item \textbf{countable additivity}: $\mu(\cup_{i=1}^{\infty} E_i) = \sum_{i=1}^{\infty}\mu(E_i)$ for $\forall E_i \in A$ and pairewise disjoint.
\end{definition}

\begin{definition}
\textbf{Kolmogorov's axioms: }
\textbf{A probability measure} is a measure $P$ s.t. $P(\Omega) = 1$
\end{definition}

\begin{definition}
\textbf{lebesgue measure} on $R$($\Omega = R, A = B(R) = Borel algebra$),s.t
$$\mu(a,b) = b -a$$ for any $a,b \in R$ and $a < b$
\end{definition}
显然, lebesgue meaure is not a prob measure.

Basic properties of a measure space $(\Omega, A, \mu)$:
\begin{enumerate}
\item $P(\emptyset) = 0$
\item $E,F \in A$ and $E \subset F$, then $\mu(E) \leq \mu(F)$
\item $P(\cup_{i=1}^{\infty} E_i) \leq \sum_{i=1}^{\infty}P(E_i)$ for $\forall E_i \in A$
\item $P(\cup_{i=1}^{\infty} E_i) = \sum_{i=1}^{\infty}P(E_i)$ for $\forall E_i \in A$ and pairewise disjoint.
\item \textbf{Continuity from below}\\
If $E_i \subset E_2 \subset,\ldots $, then $P(\cup_{i=1}^{\infty} E_i) = \lim_{i \to \infty} P(E_i)$
\item \textbf{Continuity from above}\\
If $E_i \supset E_2 \supset,\ldots $, then $P(\cap_{i=1}^{\infty} E_i) = \lim_{i \to \infty} P(E_i)$
\end{enumerate}

\begin{definition}
A \textbf{Borel measure} on $R$ is a measure on $(R, B(R))$
\end{definition}

\section{Prob}
\begin{theorem}
\textbf{Chain rule}\\
If $P(A_1 \cap A_2 \ldots A_{n-1}) > 0$
$$ P(A_1 \cap A_2 \ldots \cap A_n) = P(A_1) P(A_2 | A_2) P(A_3 | A_1 \cap A_2) \ldots P(A_n | A_1 \cap A_2 \ldots A_{n-1}) $$
\end{theorem}
Proof by induction

\begin{definition}
\textbf{Conditional measure} $Q(A) = P(A|B)$ defines a conditional prob measure $Q$ given $B$.
\end{definition}

\section{Random variable(r.v)}
\begin{definition}
Given a prob measure space $(\Omega, A, P)$, a r.v is a function $X: \Omega \rightarrow R$, s.t.
$$\{\omega \in \Omega: X(\omega) \leq x\} \in A, \forall x \in R$$ 
\end{definition}

Notation:\\
$$\{X \leq x\} = \{\omega \in \Omega: X(\omega) \leq x\}$$ 
$$P(X \leq x) = P(\{X \leq x\}) = P(\{\omega \in \Omega: X(\omega) \leq x\})$$ 

\begin{definition}
The CDF of a r.v $X$ is $F: R \rightarrow [0,1]$, s.t.
$$F(x) = P(X \leq x)$$
\end{definition}

\begin{definition}
The \textbf{distribution} of a r.v $X$ is prob measure $P_X$ on $R$ s.t.
$$P_X(B) = P(X^{-1}(B)) = P(X \in B), \forall A \in B(R)$$
\end{definition}
$R$ 是 r.v $X$所有可能的结果所在的空间, 而$B(R)$ 是可能的结果组成的所有可能的集合, 所以distribution 实际上就是r.v $X$的所有可能结果的概率.

\textbf{Types of r.v}
%% \begin{definition}
%% \end{definition}
\begin{definition}
A r.v is \textbf{discret} if $X(\Omega)( = \{X(\omega): \omega \in \Omega\})$ is countable(可以为无穷多个). (i.e $X(\Omega) = \{x_1, x_2,\ldots \}$)
\end{definition}

\begin{definition}
A r.v is has a \textbf{density} if $F(x) = \int_{-\infty}^x f(u)du, \forall x \in R$
\end{definition}

Let $Q = P_X$(distribution of $X$), let $J = \{x \in R: Q(x) > 0\}$
$$Q_d(A) = Q(A \cap J)$$
$$Q_c(A) = Q(A) - Q(A \cap J)$$
所以$Q = Q_d + Q_c$, d is for discret part(as CDF may have jumps), c for continu part

$$Q_c = Q_{ac} + Q_{sc}$$
as for absolutely continued, sc for singular continued

if $X$ discret, then $Q = Q_d$
if $X$ has a density, then $Q = Q_{ac}$

\warning{$Q = Q_c \nRightarrow X$ has a density}
反例, \href{http://en.wikipedia.org/wiki/Cantor\_function}{Cantor function}(一个连续,却不绝对连续的函数) 为 CDF.

概率质量函数(probability mass function,简写为pmf)是离散随机变量在各特定取值上的概率.

概率质量函数和概率密度函数(probability density function,简写为pdf)不同之处在于:
概率质量函数是对离散随机变量定义的,本身代表该值的概率; 
概率密度函数是对连续随机变量定义的,本身不是概率,只有对连续随机变量的概率密度函数在某区间内进行积分后才是概率.

\subsection{Random vector}
\begin{definition}
Given a prob measure space $(\Omega, A, P)$, a random vector is a (mmeasure) $X: \Omega \rightarrow R^d$, with $d \in {1, 2, \ldots}$
\end{definition}

\begin{definition}
A random vector is discret if $X(\Omega)$ is countable.
\end{definition}

Now we fix $(X,Y) \in R^2$ with $P(x,y) = P(X=x, Y=y)$
\begin{definition}
The marginal PMF of X is $P_X(x) = P(X = x)$
\end{definition}
我们可以推出: $$P_X(x) = \sum_{y} P(x,y)$$

\subsection{Independent r.v.s}
\begin{definition}
$X$ and $Y$ are independent if $P_{X,Y}(x,y) = P_X(x)P_Y(y), \forall x, y$ where $P_X(x)$ is the marginal PDF, $X$ and $Y$ are discret.
\end{definition}

\begin{definition}
$X$ and $Y$ are independent if $f_{X,Y}(x,y) = f_X(x)f_Y(y), \forall x, y$ where $f_X(x)$ is the marginal PDF, $X$ and $Y$ have densities.
\end{definition}

\section{指数分布(Exponential distribution)}
\label{sec.distribution.exponential}
一种\textbf{连续概率分布}.指数分布可以用来表示独立随机事件发生的时间间隔,比如旅客进机场的时间间隔,中文维基百科新条目出现的时间间隔等等.

\textbf{概率密度函数}
一个指数分布的概率密度函数是
$$f(x;\lambda )=\left\{{\begin{matrix}\lambda e^{{-\lambda x}}&,\;x\geq 0,\\0&,\;x<0.\end{matrix}}\right.$$
其中$\lambda  > 0$是分布的一个参数,常被称为率参数(rate parameter).即每单位时间发生该事件的次数.
指数分布的区间是$[0,\infty)$. 如果一个随机变量X 呈指数分布,则可以写作:$X \sim Exponential(\lambda )$.
$$
\int_0^{\infty} f(x)dx = 1
$$
\href{http://upload.wikimedia.org/wikipedia/commons/thumb/b/b1/Exponential\_distribution\_pdf.png/800px-Exponential\_distribution\_pdf.png}{Exponential 概率密度函数}
	
\textbf{累积分布函数}\\
累积分布函数可以写成:
$$F(x;\lambda )=\left\{{\begin{matrix}1-e^{{-\lambda x}}&,\;x\geq 0,\\0&,\;x<0.\end{matrix}}\right.$$
\href{http://upload.wikimedia.org/wikipedia/commons/thumb/b/b1/Exponential\_distribution\_pdf.png/800px-Exponential\_distribution\_pdf.png}{Exponential 累积分布函数}
	
随机变量$X$ (概率参数是$\lambda$ ) 的期望值是:
$$
{\mathbf  {E}}[X]=\int_0^{\infty} xf(x)dx = {\frac  {1}{\lambda }}
$$
比方说:如果你平均每个小时接到2次电话,那么你预期等待每一次电话的时间是半个小时.
$X$ 的方差是:
$${\mathbf  {D}}[X]={\frac{1}{\lambda ^{2}}}
$$
$X$ 的偏离系数是: $V[X] = 1$

\textbf{与泊松过程的关系}\\
泊松过程是一种重要的随机过程.泊松过程中,第k次随机事件与第$k+1$次随机事件出现的时间间隔服从指数分布.这是因为,第$k$次随机事件之后长度为$t$的时间段内,第$k+1$次随机事件出现的概率等于$1$减去这个时间段内没有随机事件出现的概率.而根据泊松过程的定义,长度为$t$的时间段内没有随机事件出现的概率等于
$$
{\frac  {e^{{-\lambda t}}(\lambda t)^{0}}{0!}}=e^{{-\lambda t}}.
$$
所以第k次随机事件之后长度为t的时间段内,第$k+1$次随机事件出现的概率等于$1-e^{{-\lambda t}}$,这是指数分布.这还表明了泊松过程的无记忆性.

\section{Poisson分布}
\label{sec.distribution.poisson}
\textbf{离散分布}

泊松分布适合于描述单位时间内随机事件发生的次数的概率分布.如某一服务设施在一定时间内受到的服务请求的次数,电话交换机接到呼叫的次数,汽车站台的候客人数,机器出现的故障数,自然灾害发生的次数,DNA序列的变异数,放射性原子核的衰变数等等.

\bigskip
$X$ 表示在给定的时间间隔或指定区域$t$内结果的发生数量, 则泊松随机变量$X$的概率分布为:
$$
P(x,\lambda t)={\frac  {e^{-\lambda t}(\lambda t) ^{x}}{x!}}, \eqspace x=0,1,2,\cdots
$$
其中$\lambda$ 是单位时间(或单位面积)内随机事件的平均发生率,单位时间内得到结果的数量.

\href{http://upload.wikimedia.org/wikipedia/commons/thumb/c/c1/Poisson\_distribution\_PMF.png/325px-Poisson\_distribution\_PMF.png}{Poisson 概率分布}\label{fig.distribution.poisson}

\href{http://upload.wikimedia.org/wikipedia/commons/thumb/8/84/Poisson\_distribution\_CMF.png/325px-Poisson\_distribution\_CMF.png}{Poisson 累积分布函数}\label{fig.distribution.poisson.pmf}

\begin{example}
一个试验中在$1 \mu s$ 内通过一计数器的平均辐射粒子数为4 个, 在某$1\mu s$ 中有6 个粒子通过计数器的概率为是多少?\\
根据参数, $x=6, \lambda t = 4 \times 1=4$, 得到
$$ p(6,4) = \frac{e^{-4} 4^6}{6!} = 0.1042 $$
\end{example}

\subsection{Features}	
\noindent
1.$E(X)=V(X)=\lambda$ \\
2.两个独立且服从泊松分布的随机变量,其和仍然服从泊松分布 (更精确地说:若$X \sim Poisson(\lambda_1)$且$Y \sim Poisson(\lambda_2)$,则 $X+Y \sim Poisson(\lambda_1+\lambda_2))$\\
3.与许多离散分布和连续分布一样, 随着均值越来越大, 泊松分布的形式越来越对称, 如图Figure~\ref{fig.distribution.poisson}.

\textbf{泊松过程的特点}
\begin{enumerate}
\item Les nombres d'occurrences dans des intervalles de temps disjoints sont ind\'ependants
\item La probabilit\'e d'une occurrence dans un petit intervalle de temps est proportionnelle \`a la longueur de cet intervalle, le coefficient de proportionnalit\'e \'etant$ \lambda$ 
\item La probabilit\'e qu'il y ait plus d'une occurrence dans un petit intervalle de temps est n\'egligeable
\end{enumerate}
Ces deux derni\`eres conditions forment la propri\'et\'e dite des \textbf{\'ev\'enements rares}.

Math\'ematiquement, ces propri\'et\'es se traduisent, si l'on note $(N_{t})_{{t\in {\mathbb  {R}}^{+}}}$ le processus de Poisson et ${\mathbb  {P}}$ la probabilit\'e, par:
\begin{enumerate}
\item $\forall t_{0}=0\leq t_{1}<\dots <t_{k}$, les variables al\'eatoires $(N_{{t_{k}}}-N_{{t_{{k-1}}}}),\dots (N_{{t_{1}}}-N_{{t_{0}}})$ sont ind\'ependantes
\item ${\mathbb{P}}(N_{{t+h}}-N_{t}=1)=\lambda h+o(h)$ lorsque $h\to 0+$ (t \'etant fix\'e)
\item ${\mathbb{P}}(N_{{t+h}}-N_{t}>1)=o(h)$ lorsque $h\to 0+$ (t \'etant fix\'e)
\end{enumerate}

\subsection{Poisson 分布与二项分布的关系}
%% http://my.oschina.net/u/347414/blog/129195
在二项分布的伯努利试验中,如果试验次数$n$很大,二项分布的概率$p$很小,且乘积$\lambda = n$ \\$p$比较适中,则事件出现的次数的概率可以用泊松分布来逼近.事实上,\textbf{二项分布可以看作泊松分布在离散时间上的对应物}.
\begin{proof}
证明如下.首先,回顾e的定义:
$$
\lim _{{n\to \infty }}\left(1-{\lambda  \over n}\right)^{n}=e^{{-\lambda }},
$$
二项分布的定义:
$$
P(X=k)={n \choose k}p^{k}(1-p)^{{n-k}}.
$$
如果令$p=\lambda /n$, $n$趋于无穷时$P$的极限:
$$
\begin{aligned}
\lim_{{n \to \infty }}P(X=k) & =\lim _{{n\to \infty }}{n \choose k}p^{k}(1-p)^{{n-k}}\\
& = \lim _{{n\to \infty }}{n! \over (n-k)!k!}\left({\lambda \over n}\right)^{k}\left(1-{\lambda \over n}\right)^{{n-k}}\\
& = \lim _{{n\to \infty }}\underbrace {\left[{\frac {n!}{n^{k}\left(n-k\right)!}}\right]}_{F}\left({\frac {\lambda ^{k}}{k!}}\right)\underbrace {\left(1-{\frac {\lambda }{n}}\right)^{n}}_{{\to \exp \left(-\lambda \right)}}\underbrace {\left(1-{\frac {\lambda }{n}}\right)^{{-k}}}_{{\to 1}}\\
& = \lim _{{n\to \infty }}\underbrace {\left[\left(1-{\frac {1}{n}}\right)\left(1-{\frac {2}{n}}\right)\ldots \left(1-{\frac {k-1}{n}}\right)\right]}_{{\to 1}}\left({\frac {\lambda ^{k}}{k!}}\right)\underbrace {\left(1-{\frac {\lambda }{n}}\right)^{n}}_{{\to \exp \left(-\lambda \right)}}\underbrace {\left(1-{\frac {\lambda }{n}}\right)^{{-k}}}_{{\to 1}}\\
& = \left({\frac {\lambda ^{k}}{k!}}\right)\exp \left(-\lambda \right)
\end{aligned}
$$
\end{proof}

\section{正态分布}
\textbf{连续概率分布}\\
若随机变量X服从一个位置参数为$\mu$ ,尺度参数为$\sigma$ 的概率分布,记为:$X\sim N(\mu ,\sigma ^{2})$

则其概率密度函数为
$$f(x)=\frac{1}{\sqrt{2\pi} \sigma} \exp({- \dfrac{(x-\mu )^{2}}{2\sigma ^{2}}})$$

如果$\sigma = 0$, 那么$X$ is contant and $X = \mu$

\begin{definition}
A r.v $X \in R^n$ is multivariant Gaussian if any linear combination of its compoents is uni Gaussian, i.e.
$$a^T X = \sum_{i=1}^n a_x X_i$$ is Gaussian
\end{definition}

$X \sicksim N(\mu, C)$ means $X$ is multi-variant Gaussian with $E(X_i) = \mu_i$ and $Cov(X_i, X_j) = C_{ij}$

\begin{definition}
$X \thicksim N(\mu, C)$ is degenerate if $\det{C} = 0$
\end{definition}

\href{http://www.ryanzhang.info/wp-content/uploads/2013/07/QQ\%E6\%88\%AA\%E5\%9B\%BE20130706200349.png}{协方差矩阵对多元高斯分布的影响}\\
上图是5个不同的模型,从左往右依次分析:
\begin{enumerate}
\item 是一个一般的二元高斯分布模型(independent, 同时expectation 相等)
\item 通过协方差矩阵,令特征1拥有较小的偏差,同时保持特征2的偏差
\item 通过协方差矩阵,令特征2拥有较大的偏差,同时保持特征1的偏差
\item 通过协方差矩阵,在不改变两个特征的原有偏差的基础上,增加两者之间的正相关性
\item 通过协方差矩阵,在不改变两个特征的原有偏差的基础上,增加两者之间的负相关性
\end{enumerate}

\begin{fact}
(independent compoents) $\vector{X}$ are independent with $X_i \thicksim N(\mu_i, \sigma^2)$ iff $X = (\vector) \thicksim N(\mu, C)$ where
$$\mu = (\vector{\mu}) \hbox{ and } C = \diag{\vector{\sigma^2}}$$
\end{fact}

\begin{fact}
If $X \in R^n$ is Gaussian then 
$X_i, X_j$ are indenpendent iff $Cov(X_i, X_j) = 0$
\end{fact}

\warning{This does not hold for Non-Gaussian}

\warning{$\vector{X}$ each uni Gaussian $\nRightarrow X = (\vector{X})$ is multi Gaussian}

\subsection{中心极限定理}
正态分布有一个非常重要的性质:
在特定条件下,大量统计独立的随机变量的平均值的分布趋于正态分布,这就是中心极限定理.中心极限定理的重要意义在于,根据这一定理的结论,其他概率分布可以用正态分布作为近似.

Le th\'eor\`eme central limite \'etablit la convergence en loi de la somme d'une suite de variables al\'eatoires vers la loi normale. 
Intuitivement, ce r\'esultat affirme que toute somme de variables al\'eatoires ind\'ependantes et identiquement distribu\'ees tend vers 
une variable al\'eatoire gaussienne.

Soit $X_1, X_2, \cdots$ une suite de variables al\'eatoires r\'eelles d\'efinies sur le m\^eme espace de probabilit\'e, ind\'ependantes et identiquement distribu\'ees suivant la m\^eme loi $D$. Supposons que l'esp\'erance $\mu$ et l'\'ecart-type $\sigma$ de $D$ existent et soient finis avec $\sigma \neq 0$.

Consid\'erons la somme
$$S_n = X_1 + X_2 + \cdots + X_n$$
Alors l'esp\'erance de $S_n$ est $n\mu$ et
son \'ecart-type vaut $\sigma \sqrt{n}$.
De plus, quand $n$ est assez grand, la loi normale $\mathcal{N}(n \mu,n \sigma^2)$ est une bonne approximation de la loi de $S_n$.

Afin de formuler math\'ematiquement cette approximation, nous allons poser
$$ X_n = S_n/n = (X_1 + X_2 + \cdots + X_n)/n $$
et
$$ \mathrm{Z}_n = \frac{\mathrm{S}_n - n \mu}{\sigma \sqrt{n}} = \frac{\overline{\mathrm{X}}_n - \mu}{\sigma/\sqrt{n}}, $$
de sorte que l'esp\'erance et l'\'ecart-type de $Zn$ valent respectivement $0$ et $1$ : la variable est ainsi dite centr\'ee et r\'eduite.

Le th\'eor\`eme central limite stipule alors que la suite de variables al\'eatoires $Z1, Z2,..., Zn,... $converge en loi vers une variable al\'eatoire $Z$, 
d\'efinie sur le m\^eme espace probabilis\'e, et de loi normale centr\'ee r\'eduite $\mathcal{N} (0, 1)$ lorsque $n$ tend vers l'infini.

参数为$n$和$p$的二项分布,在$n$相当大而且$p$接近$0.5$时近似于正态分布(有的参考书建议仅在$np$与$n(1-p)$至少为5时才能使用这一近似).
近似正态分布平均数为$\mu =np$且方差为$\sigma ^{2}=np(1-p)$.

泊松分布带有参数$\lambda$ 当取样样本数很大时将近似正态分布$\lambda$ .
近似正态分布平均数为$\mu =\lambda$ 且方差为$\sigma ^{2}=\lambda$ .

这些近似值是否完全充分正确取决于使用者的使用需求.

\section{重尾分布}
在机率论中,重尾分布(英语:Heavy-tailed distribution)是一种机率分布的模型,它的尾部比指数分布还要厚.
在许多状况中,通常右边尾部的分布会比较受到重视,但左边尾部比较厚,或是两边尾部都很厚的状况,也会被认为是一种重尾分布.

重尾分布之中,又有两个子类型,分别称为长尾分布(long-tailed distributions)以及次指数分布(subexponential distributions).

在一个累积分布函数中,一个随机变量 $X$　的分布状况,在以下状况时,被称为是一个重尾分布.假设:
$$ \lim_{x \to \infty} e^{\lambda x}\Pr[X>x] = \infty \quad \mbox{for all } \lambda>0 $$

在一个累积分布函数中,一个随机变量 $X$的分布,出现以下状况时,被称为是一个长尾分布.假设对所有$t > 0$ :
$$ \lim_{x \to \infty} \Pr[X>x+t|X>x] =1$$
对一个右尾部形成长尾分布的状况,我们可以做一个直观的解释:假如一个长尾分布的尾部数量超过某个很高的水平,它超过另一个更高水平的机率会接近于一.
也就是说,如果你发现状况很糟,它可能会比你想像的还要糟.\\
长尾分布是重尾分布中的一个特例.所有的长尾分布都是重尾分布,但反之则不然,也就是说,我们可以找出某一个重尾分布,它不是长尾分布.

\subsection{Loi des grands nombres}
On consid\`ere une suite $(X_n)_{n\in\N^*}$ de variables al\'eatoires ind\'ependantes d\'efinies sur un m\^eme espace probabilis\'e, 
ayant m\^eme variance finie et m\^eme esp\'erance not\'ees respectivement $V(X)$ et $E(X)$. 

La loi faible des grands nombres stipule que, pour tout r\'eel $\epsilon$ strictement positif, la probabilit\'e que la moyenne empirique 
$Y_n \equiv \bar x= \frac{1}{n} \sum_{i=1}^{n} X_i$ s'\'eloigne de l'esp\'erance d'au moins $\epsilon$ tend vers $0$ quand $n$ tend vers l'infini.

\begin{theorem}
$\forall\varepsilon>0,\quad \lim_{n \to +\infty} \mathbb{P}\left(\left|\frac{X_1+X_2+\cdots+X_n}{n} -E(X)\right| \geqslant \varepsilon\right) = 0$
\end{theorem}

Autrement dit, $(Y_n)_{n\in\N^*}$ converge en probabilit\'e vers $E(X)$. Ce r\'esultat est tr\`es important en statistique, 
puisqu'il assure que la moyenne empirique est un estimateur convergent de l'esp\'erance.

\section{Estimation}
Pour caract\'eriser l'estimation de $\theta$, on verra les crit\`eres suivants:
\begin{itemize}
\item Convergence lorsque $n \to \infty$ (forte consistence)
\item Convergence en moyenne (biais)
\item Maximisation probabiliste (maximum de vraisemblance)
\item Crit\`ere bas\'e sur la variance (risque)
\end{itemize}

\begin{definition}
\textbf{Biais d'un estimateur}:
La diff\'erence entre l'esp\'erance de l'estimateur et la vraie valeur du param\`etre estim\'e.

Si $\hat \theta$ est l'estimateur de $\theta\,,  \text{Biais}(\hat\theta)\equiv E[\hat\theta]-\theta$
\end{definition}

\subsection{Maximum de vraisemblance}
极大似然原理的直观想法是:一个随机试验如有若干个可能的结果$A,B,C,\cdots$.若在一次试验中,结果$A$出现,则一般认为试验条件对A出现有利,也即$A$出现的概率很大.

最大似然估计会寻找关于$\theta$的最可能的值(即,在所有可能的$\theta$取值中,寻找一个值使这个采样的"可能性"最大化).
这种方法正好同一些其他的估计方法不同,如$\theta$的非偏估计,非偏估计未必会输出一个最可能的值,而是会输出一个既不高估也不低估的$\theta$值.

Soit une famille param\'etr\'ee de distributions de probabilit\'es $D_{\theta}$ dont les \'el\'ements sont associ\'es soit 
\`a une densit\'e de probabilit\'e connue (distribution continue), soit \`a une fonction de masse connue (distribution discr\`ete), not\'ee $f_{\theta}$. 
On tire un \'echantillon de $n$ valeurs $x_1, x_2, \cdots, x_n$:de la distribution,
et l'on calcule la densit\'e de probabilit\'e associ\'ee aux donn\'ees observ\'ees

$$ f_\theta(x_1,\dots,x_n \mid \theta).\, $$
Ceci \'etant une fonction de $\theta$ avec $x_1, \cdots, x_n$ fix\'es, c'est une vraisemblance.

$$ L(\theta) = f_\theta(x_1,\dots,x_n \mid \theta).\, $$
Lorsque $\theta$ n'est pas observable, la m\'ethode du maximum de vraisemblance utilise les valeurs de $\theta$ qui 
maximisent $L(\theta)$ estimateur de $\theta$: c'est l'estimateur du maximum de vraisemblance de $\theta$ not\'e $\widehat{\theta}$. 
Par exemple dans le cas du produit discret, on effectue un tirage de n valeurs, 
il faut donc trouver le param\`etre qui maximise la probabilit\'e d'avoir tir\'e ce tirage.

\bigskip
On appelle vraisemblance de $\theta$ au vu des observations $(x_1, \ldots,x_i, \ldots, x_n)$ d'un \'echantillon ind\'ependamment et identiquement distribu\'e 
selon la loi $\mathcal{D}_\theta$, le nombre :
$$ L(x_1, \ldots,x_i, \ldots,x_n;\theta) = f(x_1;\theta) \times f(x_2;\theta) \times \ldots \times f(x_n;\theta) = \prod_{i=1}^n f(x_i;\theta) $$

On cherche à trouver le maximum de cette vraisemblance pour que les probabilit\'es des r\'ealisations observ\'ees soient aussi maximum. 
Ceci est un probl\`eme d'optimisation. 

On utilise g\'en\'eralement le fait que si L est d\'erivable (ce qui n'est pas toujours le cas) et 
si L admet un maximum global en une valeur $\theta = \hat \theta$, 
alors la d\'eriv\'ee premi\`ere s'annule en $\theta = \hat \theta $et que la d\'eriv\'ee seconde est n\'egative. \\
R\'eciproquement, si la d\'eriv\'ee premi\`ere s'annule en $\theta = \hat \theta$ et que la d\'eriv\'ee seconde est n\'egative en $\theta = \hat \theta$, 
alors $\theta = \hat \theta $est un maximum local (et non global) de $L(x_1, \ldots, x_i, \ldots,x_n;\theta)$. \\
Il est alors n\'ecessaire de v\'erifier qu'il s'agit bien d'un maximum global. 
La vraisemblance \'etant positive et le logarithme n\'ep\'erien(naturel) une fonction croissante, 
il est \'equivalent et souvent plus simple de maximiser le logarithme n\'ep\'erien de la vraisemblance. 

Ainsi en pratique:

La condition n\'ecessaire
$$ \frac{\partial L(x_1, \ldots, x_i, \ldots, x_n;\theta)}{\partial \theta} = 0 $$
ou
$$ \frac{\partial \ln L(x_1, \ldots, x_i, \ldots, x_n;\theta)}{\partial \theta} = 0 $$
permet de trouver la valeur $\theta = \hat \theta$.
$\theta = \hat \theta $est un maximum local si la condition suffisante est remplie au point critique $\theta = \hat \theta $:
$$ \frac{\partial^2 L(x_1, \ldots, x_i, \ldots, x_n;\theta)}{\partial \theta^2} \le 0 $$
ou
$$ \frac{\partial^2 \ln L(x_1, \ldots, x_i, \ldots,x_n;\theta)}{\partial \theta^2} \le 0 $$

\subsubsection{Likelihood function}
The likelihood function is defined differently for discrete and continuous probability distributions.

\textbf{Discrete probability distribution}\\
Let $X$ be a random variable with a discrete probability distribution p depending on a parameter $\theta$. Then the function

$$ \mathcal{L}(\theta |x) = p_\theta (x) = P_\theta (X=x), \, $$
considered as a function of $\theta$, is called the likelihood function (of $\theta$, given the outcome $x$ of $X$). 

Sometimes the probability on the value $x$ of $X$ for the parameter value $\theta$ is written as $P(X=x|\theta)$; often written as $\mathbf{P(X=x;\theta)}$ to emphasize that this value is \textbf{not a conditional probability}, because \textbf{$\theta$ is a parameter and not a random variable}.

\textbf{Continuous probability distribution}\\
Let $X$ be a random variable with a continuous probability distribution with density function $f$ depending on a parameter $\theta$. Then the function
$$ \mathcal{L}(\theta |x) = f_{\theta} (x), \, $$
considered as a function of $\theta$, is called the likelihood function (of $\theta$, given the outcome $x$ of $X$). 

Sometimes the density function for the value $x$ of $X$ for the parameter value $\theta$ is written as $f(x|\theta)$, 
but should not be considered as a conditional probability density.

The actual value of a likelihood function bears no meaning. Its use lies in comparing one value with another. 
For example, one value of the parameter may be more likely than another, given the outcome of the sample. 
Or a specific value will be most likely: the maximum likelihood estimate.

\begin{example}
考虑投掷一枚硬币的实验.通常来说,已知投出的硬币正面朝上和反面朝上的概率各自是$p_H = 0.5$,便可以知道投掷若干次后出现各种结果的可能性.
比如说,投两次都是正面朝上的概率是0.25.用条件概率表示,就是:
$$ P(\mbox{HH} \mid p_H = 0.5) = 0.5^2 = 0.25 $$
其中H表示正面朝上.

在统计学中,我们关心的是在已知一系列投掷的结果时,关于硬币投掷时正面朝上的可能性的信息.
我们可以建立一个统计模型:假设硬币投出时会有$p_H$  的概率正面朝上,而有$1 - p_H$ 的概率反面朝上.
这时,条件概率可以改写成似然函数:
$$ L(p_H =  0.5 \mid \mbox{HH}) = P(\mbox{HH}\mid p_H = 0.5) =0.25 $$
也就是说,对于取定的似然函数,在观测到两次投掷都是正面朝上时,$p_H = 0.5$ 的似然性是0.25(这并不表示当观测到两次正面朝上时$p_H= 0.5$ 的概率是0.25).

如果考虑$p_H = 0.6$,那么似然函数的值也会改变.
$$ L(p_H = 0.6 \mid \mbox{HH}) = P(\mbox{HH}\mid p_H = 0.6) =0.36 $$
注意到似然函数的值变大了.\\
这说明,如果参数$p_H$ 的取值变成0.6的话,结果观测到连续两次正面朝上的概率要比假设$p_H = 0.5$ 时更大.也就是说,参数$p_H$ 取成0.6 要比取成0.5 更有说服力,更为"合理".
\\(这里的$p_H$ 就是似然函数中的$\theta$, 而连续两次正面朝上HH 就是似然函数中随机变量$X$取得$x$.)

总之,\textbf{似然函数的重要性不是它的具体取值,而是当参数变化时函数到底变小还是变大}.
对同一个似然函数,如果存在一个参数值,使得它的函数值达到最大的话,那么这个值就是最为"合理"的参数值.

在这个例子中,似然函数实际上等于:
$$ L(p_H = \theta  \mid \mbox{HH}) = P(\mbox{HH}\mid p_H = \theta) =\theta^2 , 其中0 \le p_H  \le 1.  $$
如果取$p_H = 1$,那么似然函数达到最大值1.也就是说,当连续观测到两次正面朝上时,假设硬币投掷时正面朝上的概率为1是最合理的.

类似地,如果观测到的是三次投掷硬币,头两次正面朝上,第三次反面朝上,那么似然函数将会是:
$$ L(p_H = \theta  \mid \mbox{HHT}) = P(\mbox{HHT}\mid p_H = \theta) =\theta^2(1 - \theta) , 其中T表示反面朝上,0 \le p_H  \le 1.  $$
这时候,似然函数的最大值将会在$p_H = \frac{2}{3}$的时候取到.\\
也就是说,当观测到三次投掷中前两次正面朝上而后一次反面朝上时,估计硬币投掷时正面朝上的概率$p_H = \frac{2}{3}$是最合理的.
\end{example}

\subsection{Borne Cram\'er-Rao}
la borne Cram\'er-Rao exprime une borne inf\'erieure sur la variance d'un estimateur sans biais, bas\'ee sur l'Information de Fisher.
Elle \'enonce que l'inverse de l'information de Fisher, $\mathcal{I}(\theta)$, d'un param\`etre $\theta$, 
est une borne inf\'erieure de la variance d'un estimateur sans biais de ce param\`etre (not\'e $\widehat{\theta}$).
$$
\mathrm{var} \left(\widehat{\theta}\right)
\geq
\mathcal{I}(\theta)^{-1}
= \mathbb{E}
 \left[
   \left(\frac{\partial}{\partial \theta} \ln L(X;\theta)\right)^2
 \right]^{-1}
$$

\begin{example}
Supposons que $X$ est un vecteur al\'eatoire qui suit une loi normale d' esp\'erance connue $\mu$ et de variance inconnue $\sigma^2$. 
Consid\'erons $T$ l'estimateur de $\sigma^2$:

$$ T=\frac{\sum_{i=1}^n\left(X_i-\mu\right)^2}{n}.  $$
Alors $T$ est non biais\'e pour $\sigma^2$, car $E[T]=\sigma^2$. Quelle est la variance de $T$ ?

$$
\mathrm{Var}(T) = \frac{\mathrm{var}(X-\mu)^2}{n}=\frac{1}{n}
\left[
E\left\{(X-\mu)^4\right\}-\left(E\left\{(X-\mu)^2\right\}\right)^2
\right]
$$
\todo{上式的第一个等号有点不理解}
$$(Var(X) = E(X^2) - (E(X))^2)$$
Le premier terme est le quatri\`eme moment centr\'e et vaut $3\sigma^4$; le second est le carr\'e de la variance, soit $\sigma^4$. Donc :

$$ \mathrm{var}(T)=\frac{2\sigma^4}{n}.  $$
Quelle est l'information de Fisher de cet exemple ? Le score $V$ est d\'efini par :


$$ V=\frac{\partial}{\partial\sigma^2}\log L(\sigma^2,X) $$
avec L \'etant la fonction de vraisemblance. Donc, dans ce cas,

$$
V=\frac{\partial}{\partial\sigma^2}\log\left[\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(X-\mu)^2/{2\sigma^2}}\right]
=\frac{(X-\mu)^2}{2(\sigma^2)^2}-\frac{1}{2\sigma^2}
$$
L'information de $n$ \'ev\`enements ind\'ependants \'etant seulement $n$ fois l'information d'un seul \'ev\`enement, soit $\frac{n}{2(\sigma^2)^2}$.

L'in\'egalit\'e de Cram\'er-Rao donne :
$$ \mathrm{var}(T)\geq\frac{1}{I}.$$
Dans ce cas, on a donc \'egalit\'e, ce qui montre que l'estimateur est efficace.
\end{example}

\section{Fisher information}
The Fisher information is a way of \textbf{measuring the amount of information that an observable random variable $X$ 
carries about an unknown parameter $\theta$ upon which the probability of $X$ depends}.

The probability function for $X$, which is also the likelihood function for $\theta$, is a function $f(X; \theta)$; 
it is the probability mass (or probability density) of the random variable $X$ conditional on the value of $\theta$. 

The partial derivative with respect to $\theta$ of the natural logarithm of the likelihood function is called the \textbf{score}.\\
(In statistics, the score, score function, efficient score or informant indicates \textbf{how sensitively a likelihood function $L(\theta; X)$ 
depends on its parameter $\theta$}. 
Explicitly, the score for $\theta$ is the gradient of the log-likelihood with respect to $\theta$: $\dfrac{\partial}{\partial\theta} \log L(\theta;X)$.)

Under certain regularity conditions, it can be shown that the first moment of the score (that is, its expected value) is $0$:
$$
\begin{aligned}
\operatorname{E} \left[\left. \frac{\partial}{\partial\theta} \log f(X;\theta)\right|\theta \right]
& = \operatorname{E} \left[\left. \frac{\frac{\partial}{\partial\theta} f(X;\theta)}{f(X; \theta)}\right|\theta \right]\\
& = \int \frac{\frac{\partial}{\partial\theta} f(x;\theta)}{f(x; \theta)} f(x;\theta)\; \mathrm{d}x\\
& = \int \frac{\partial}{\partial\theta} f(x;\theta)\; \mathrm{d}x\\
& = \frac{\partial}{\partial\theta} \int f(x; \theta)\; \mathrm{d}x\\
& = \frac{\partial}{\partial\theta} \; 1 \\
& = 0.
\end{aligned}
$$

The second moment is called the Fisher information:
$$
\mathcal{I}(\theta)=\operatorname{E} \left[\left. \left(\frac{\partial}{\partial\theta} \log f(X;\theta)\right)^2\right|\theta \right] = \int \left(\frac{\partial}{\partial\theta} \log f(x;\theta)\right)^2 f(x; \theta)\; \mathrm{d}x\,,
$$
where, for any given value of $\theta$, the expression $E[...|\theta]$ denotes the conditional expectation over values for $X$ with respect to the probability function $f(x; \theta)$ given $\theta$. Note that $0 \leq \mathcal{I}(\theta) < \infty$. 

A random variable carrying high Fisher information implies that the absolute value of the score is often high. 
The Fisher information is not a function of a particular observation, as the random variable $X$ has been averaged out.

Since the expectation of the score is zero, the Fisher information is also the variance of the score.

If $\log f(x; \theta)$ is \textbf{twice differentiable with respect to $\theta$}, and under certain regularity conditions, 
then the Fisher information may also be written as[10]

$$ \mathcal{I}(\theta) = - \operatorname{E} \left[\left. \frac{\partial^2}{\partial\theta^2} \log f(X;\theta)\right|\theta \right]\,, $$
since

$$
\frac{\partial^2}{\partial\theta^2} \log f(X;\theta)
= \frac{\frac{\partial^2}{\partial\theta^2} f(X;\theta)}{f(X; \theta)}
\;-\;
\left( \frac{\frac{\partial}{\partial\theta} f(X;\theta)}{f(X; \theta)} \right)^2
= \frac{\frac{\partial^2}{\partial\theta^2} f(X;\theta)}{f(X; \theta)}
\;-\;
\left( \frac{\partial}{\partial\theta} \log f(X;\theta)\right)^2
$$
and

$$
\operatorname{E} \left[\left. \frac{\frac{\partial^2}{\partial\theta^2} f(X;\theta)}{f(X; \theta)}\right|\theta \right]
= \cdots
= \frac{\partial^2}{\partial\theta^2} \int f(x; \theta)\; \mathrm{d}x
= \frac{\partial^2}{\partial\theta^2} \; 1 
= 0.
$$
Thus, the Fisher information is the negative of the expectation of the second derivative with respect to $\theta$ of the natural logarithm of $f$.

Information is \textbf{additive}, in that the information yielded by two independent experiments is the sum of the information from each experiment separately:
$$ \mathcal{I}_{X,Y}(\theta) = \mathcal{I}_X(\theta) + \mathcal{I}_Y(\theta) $$
This result follows from the elementary fact that if random variables are independent, the variance of their sum is the sum of their variances. 
In particular, the information in a random sample of size $n$ is $n$ times that in a sample of size 1, 
when observations are independent and identically distributed.

\section{PCA: Principal component analysis}
降维技术使得数据更易使用, 并且他们能够去除数据中的噪声, 使得其他机器学习任务更加精确. 降维往往作为预处理步骤, 在数据应用其他算法之前清洗数据.\\
有很多技术可以用于数据降维, 独立成分分析(Independent Component Analysis, ICA), 因子分析(Factor Analysis)和主成分分析比较流行, 其中又以主成分分析应用最广泛.\\
机器学习实战一书中的python 实例是将数据集都调入内存, 如果无法做到, 就需要采用其他的方法, 可以参考一篇优秀的论文 - Incremental Eigenanalysis for Classification

a useful statistical technique

wiki: the frist principal component has the largest possible variance.\\
\noindent
用covariance matrix 矩阵来讲解的话, 说是最大的eigenvalue 对应的eigenvector所在方向为the most principal component方向\\
\red{机器学习实战中说, 求得的特征值若为零, 则意味着这些特征值是其他特征的副本, 也就是, 他们可以通过其他特征来表示, 没有理解todo}

\subsection{Idea}
我们以二元变量$X = (X_1, X_2)$为例, 说明主成分分析的思想. 对此二维变量进行了$n$ 次观测, 得到数据$x_i = (x_{i1},x_{i2}) \eqspace (i=1,2,\dots,n))$.假设它们在二维平面x1ox2上的分布如Figure
\href{http://upload.wikimedia.org/wikipedia/commons/thumb/1/15/GaussianScatterPCA.png/220px-GaussianScatterPCA.png}{PCA idea}
所示.

考虑如下一种极端情况,即$X_1$和$X_2$的相关系数的绝对值为$1$,则$(x(i_1),x(i_2))(i=1,2,...n)$以概率$1$分布在一条直线$L$上,若将原坐标系沿逆时针方向旋转一个角度$\theta $得到新的直角坐标系$y_1oy_2$,使坐标轴$oy_1$与$L$重合,这时观测点$(x(i_1),x(i_2))(i=1,2,...n)$则可由它们在$oy_1$上的坐标所决定,这些观测点在$oy_1$上的坐标为
$$ y(i_1)=x(i_1)cos\theta +x(i_2)sin\theta ,    i=1,2,...n $$
它们是原观测数据的线性组合且在$oy_1$轴上的分散性(即样本方差)达到最大. 这相当于对原变量$(X_1,X_2)$作适当的线性变换得新的变量$Y_1$,即
$$ Y_1=X_1cos\theta +X_2sin\theta , $$
其中$\theta $的选择使得$Var(Y_1)$最大且$Y_1$的相应观测值完全可以反映原二元变量$(X_1,X_2)$的观测值的分布状况. 一般情况下,将$ox_1$轴沿逆时针旋转到观测点具有最大分散性的方向$oy_1$上(观测点在$oy_1$轴上的投影到均值点的距离大于在$ox_1$上投影到均值点的距离),使该方向所含的数据间的差异的信息最多. 同样的,再旋转$ox_2$到$oy_2$. 我们将相应的变量
$$ Y_1=X_1cos\theta +X_2sin\theta , $$
$$ Y_2=X_1sin\theta +X_2cos\theta , $$
分别称为$X_1$和$X_2$的第一和第二主成分. 设想数据在$oy_2$方向上的分散性很小,因而用一元数据便可以反映二元数据的绝大部分信息,即达到了降维的目的. 
综上所述,主成分分析是研究如何通过少数几个主成分来解释多变量的方差-协方差结构的分析方法,也就是求出少数几个主成分(变量) ,使它们尽可能多地保留原始变量的信息,且彼此不相关. 它是一种变换方法,即把给定的一组变量通过线性变换,转换为一组不相关的变量,在这种变换中,\textbf{保持变量的总方差不变},同时具有最大方差,称为第一主成分,具有次大方差,称为第二主成分. 依次类推. 

find application in face recognition and image compression, and is a common technique for finding patterns in data of high dimension.

\subsection{总体主成分的定义}
设$\vecteur{X}$ 为某实际问题涉及的n 个随机变量, 记$\mathrm{X} = \vecteur{X}^t$, 其协方差矩阵为
$$
\Sigma = (\sigma)_{n \times n} = E[(\mathrm{X} - E(\mathrm{X}))(\mathrm{X} - E(\mathrm{X}))^t]
$$
设$l_i = (l_{i1},l_{i2},\dots,l_{in})^t \eqspace (i=1,2,\dots,n)$, 考虑下列线性组合
$$
\left\{
  \begin{array}{ll}
	  Y_1 = l_1^t \mathrm{X} & = l_{11}X_1 + l_{12}X_2 + \cdots + l_{1n}X_n \\
	  Y_2 = l_2^t \mathrm{X} & = l_{21}X_1 + l_{22}X_2 + \cdots + l_{2n}X_n \\
		  \vdots\\
	  Y_n = l_n^t \mathrm{X} & = l_{n1}X_1 + l_{n2}X_2 + \cdots + l_{nn}X_n
  \end{array}
\right.
$$
有
$$
\begin{aligned}
E(Y_i)
& = E(l_i^t \mathrm{X}) \\
& = E(l_{i1}X_1 + l_{i2}X_2 + \cdots + l_{in}X_n) \\
& = l_{i}^t E(X)
\end{aligned}
$$
$$
\begin{aligned}
Cov(Y_i,Y_j)
& = E[(Y_i - E(Y_i))(Y_j - E(Y_j))^t] \\
& = E[(l_i^tX - l_i^tE(X_i))(l_j^tX - l_j^tE(X))^t] \\
& = E[l_i^t (X-E(X)) (X-E(X))^t l_j]\\
& = l_i^t E((X-E(X))(X-E(X))^t) l_j \\
& = l_i^t \Sigma l_j
\end{aligned}
$$
$$Var(Y_i) = Cov(Y_i,Y_i) = l_i^t \Sigma l_i$$

\subsection{求法}
$\Sigma$ 的特征值及相应的正交单位化向量分别为$\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n \geq 0$ and $e_i,e_2,\cdots,e_n$, 则$X$ 的第$i$ 个主成分为
$$ Y_i = e_i^t \mathrm{X} = e_{i1}X_i + e_{i2}X_2 + \cdots + e_{in}X_n, \eqspace i=1,2,\cdots,n $$
其中$e_i = (e_{i1}, e_{i2}, \cdots+ e_{in})^t $\\
这时易见
$$
\left\{
	\begin{array}{l}
		Var(Y_i) = e_i^t \Sigma e_i = \lambda_i e_i^t e_i = \lambda_i, \eqspace i = 1,2,\cdots,n\\
		Cov(Y_i,Y_k) = e_i^t \Sigma e_k = \lambda_k e_i^t e_k = 0, \eqspace i \neq k
	\end{array}
\right.
$$

\subsection{性质}
\subsubsection{主成分的协方差及总方差}
令 $P = \vecteur{e}$, 则$P$ 为一正交矩阵, 记 $Y=\vecteur{Y}^t$ 为主成分向量, 则 $Y=P^t X$, 且
$$
Cov(Y) = Cov(P^t X) = P^t \Sigma P = Diag \vecteur{\lambda}
$$
由此得到, 主成分的总方差为
$$
\sum_{i=1}^n Var(Y_i) = \sum_{i=1}^n \lambda_i = tr(P^t \sigma P) = tr(\Sigma) = \sum_{i=1}^n Var(X_i)
$$

\subsubsection{主成分$Y_i$与$X_j$的相关系数}
由于$Y=P^t X$, 故 $X=PY$, 从而
$$
\left\{
  \begin{array}{l}
		  X_j = e_{1j}Y_1 + e_{2j}Y_2 + \cdots + e_{nj}Y_n \\
		  Cov(Y_i,X_j) = \lambda_i e_{ij}
  \end{array}
\right.
$$
由此可得$Y_i$与$X_j$ 的相关系数为
$$
\rho_{Y_i,X_j} = \frac{Cov(Y_i,X_j)}{\sqrt{Var(Y_i)} \sqrt{Var(X_j)}} = \frac{\lambda_i e_{ij}}{\sqrt{\lambda_i} \sqrt{\sigma_{jj}}} = \frac{\sqrt{\lambda_i}}{\sigma_{jj}} e_{ij}
$$

\subsubsection{标准化变量的主成分}
为了避免不同的物理量的不同量纲对计算的权重的影响, 需要normaliser.\\
令 $X_i^* = \dfrac{X_i - \mu_i}{\sqrt{\sigma_{ii}}}$, where $\mu_i = E(X_i), \sigma_{ii} = Var(X_i)$

\subsection{样本主成分: 以统计学的方式讲解}
\subsubsection{Standard Deviation}
$$
s = \sqrt{\dfrac{\sum_{i=1}^n (X_i - \overline{X})^2}{n-1}}
$$
\textbf{Why are you using $n-1$ and not $n$?}\\
Well, the answer is a bit complicated, but in general, if your data set is a sample data set, 
ie. you have taken a subset of the real-world (like surveying $500$ people about the election) then you must use $n-1$ 
because it turns out that this gives you an answer that is closer to the standard deviation that would result if you had used the entire population, than if you'd used$n$.
If, however, you are not calculating the standard deviation for a sample, but for an entire population, then you should divide by $n$ instead of $n-1$.\\
For further information, refer to \href{http://mathcentral.uregina.ca/RR/database/RR.09.95/weston2.html}{when $n$ or $n-1$ in computing the standard deviation}

Standard deviation and variance only operate on 1 dimension.\\
Covariance is always measured between 2 dimensions. If you calculate the covariance between one dimension and itself, you get the variance
$$ \operatorname{Var}(X) = \operatorname{E}\left[(X - E(x))^2 \right] = \dfrac{\sum_{i=1}^n (X_i - \overline{X})^2}{n-1} $$
$$ \operatorname{Cov}(X,Y) = \operatorname{E}{\big[(X - \operatorname{E}[X])(Y - \operatorname{E}[Y])\big]} =  \dfrac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{n-1} $$

The exact value is not as important as it's sign (ie. positive or negative).
\begin{itemize}
\item If the value is positive,then that indicates that both dimensions increase together.
\item If the value is negative, then as one dimension increases, the other decreases.
\item In the last case, if the covariance is zero, it indicates that the two dimensions are independent of each other.
\end{itemize}

\subsubsection{The covariance Matrix}
the covariance matrix $\Sigma$ is the matrix whose $(i, j)$ entry is the covariance
$$
\Sigma_{ij}
= \mathrm{cov}(X_i, X_j) = \mathrm{E}\begin{bmatrix}
(X_i - \mu_i)(X_j - \mu_j)
\end{bmatrix}
$$
where $\mu_i = \mathrm{E}(X_i)$ is the expected value of the ith entry in the vector X. In other words, we have

$$
\Sigma
= \begin{bmatrix}
 \mathrm{E}[(X_1 - \mu_1)(X_1 - \mu_1)] & \mathrm{E}[(X_1 - \mu_1)(X_2 - \mu_2)] & \cdots & \mathrm{E}[(X_1 - \mu_1)(X_n - \mu_n)] \\ \\
 \mathrm{E}[(X_2 - \mu_2)(X_1 - \mu_1)] & \mathrm{E}[(X_2 - \mu_2)(X_2 - \mu_2)] & \cdots & \mathrm{E}[(X_2 - \mu_2)(X_n - \mu_n)] \\ \\
 \vdots & \vdots & \ddots & \vdots \\ \\
 \mathrm{E}[(X_n - \mu_n)(X_1 - \mu_1)] & \mathrm{E}[(X_n - \mu_n)(X_2 - \mu_2)] & \cdots & \mathrm{E}[(X_n - \mu_n)(X_n - \mu_n)]
\end{bmatrix}.
$$
The inverse of this matrix, $\Sigma^{-1}$ is the inverse covariance matrix, also known as the concentration matrix or precision matrix

If you ever need to find the eigenvectors of a matrix in a program, just find a maths library that does it all for you.  A useful maths package, called newmat, is available at \href{http://webnz.com/robert/}{newmat}

\subsubsection{PCA 的求解过程}
\begin{enumerate}
\item Get some data
\item Subtract the mean
\item Calculate the covariance matrix
\item Calculate the unit eigenvectors and eigenvalues of the covariance matrix
\item Choosing components and forming a feature vector
\item Deriving the new data set
\end{enumerate}
\end{document}

