% !Mode:: "TeX:UTF-8"
\documentclass{article}
\input{../../public/package}   %导入需要用到的package
\input{../../public/article}   %导入需要用到的package
\begin{document}
\title{Introduction to algorithm \\Eric's Notes}
\author{Eric}
\maketitle
\newpage
\tableofcontents
\newpage

\section{课程简介及算法分析}
\subsection{Insertion sort}
Pseudocode
\begin{verbatim}
INSERTION-SORT(A)
1 for j ← 2 to length[A]  #1是第一个元素
2 	do key ← A[j] //将将要插入的数据保存下来
3 	//Insert A[j] into the sorted sequence A[1 _ j - 1].
4	 i ← j - 1
5	 while i > 0 and A[i] > key
6		 do A[i + 1] ← A[i]
7		 i ← i - 1
8	 A[i + 1] ← key
\end{verbatim}
\begin{figure}[htbp]
  \centering
  \includegraphics[scale = 0.7]{sort_insertion}\\
  \caption{Insertion sort}\label{fig.sort.insertion}
\end{figure}

ex:
\begin{verbatim}
8 2 4 9 3 6
2 8 4 9 3 6 // 2 goes before 8
2 4 8 9 3 6 // 4 goes before 8
2 4 8 9 3 6 // 9 stays there
2 3 4 8 9 6 // 3 goes before 4
2 3 4 6 8 9 // 6 goes before 8
\end{verbatim}

\begin{figure}[htbp]
  \centering
  \includegraphics[scale = 0.7]{sort_insertion_example}\\
  \caption{Insertion sort}\label{fig.sort.insertion.example}
\end{figure}

python code
\begin{verbatim}
def insertionSort(L):
	#in place algorithem, 升序
	for j in range(1,len(L)):
		#将将要排序的数据保存下来, 开始对第j 个元素进行排序
		key=L[j]
		i=j-1
		#找到存放key的位置
		while i >= 0 and L[i]>key:
			L[i+1]=L[i] ## 将大数往后移动
			i=i-1
		##循环结束,说明L[i] <= key,所以key应该放在i+1处
		L[i+1]=key
	return L
\end{verbatim}

running time\\
1 already sorted: 最理想情况\\
2 reverse sorted: 最差情况

we want upper bounds 上界

kinds of analysis\\
worst-case $T(n)=max$ time of any input of size n\\
average-case $T(n)=expected time$期望时间\\
(need assumption of statistical  distribution of inputs)\\
best-case (bogus假象)

BIG IDEA: \textbf{asymptotic analysis渐进分析}\\
not the the exact running time of an algorithm\\
the order of growth of the running time

the insertion sort analysis\\
worst-case: input reverse sorted
$$T(n)=\sum_{j=2}^{j=n} \theta(j)=\theta(n^2) \eqnote{算术级数arithmetic series}$$

\subsection{Merge sort}
算法\\
T(n) merge sort A[1...n]\\
$\theta(1)$	1 if n=1, done\\
$2 \times \theta(n/2)$	2 Recursively sort A[1...upper(n/2)] and A[upper(n/2)+1...n]    向上取整\\
$\theta(n)$	3 merge 2 sorted  list

Merge\\
Where is the smallest element of any two lists that are already sorted?\\
It is in one of two places, the head of the first list or the head of the second list

\textbf{Key subroutine Merge}
\begin{verbatim}
2 7 13 20
1 9 11 12
\end{verbatim}
在两个list head中,1最小,所以1是n个元素中最小的,排在最终的list的第一个位置,现在总list和两个子list成为:
\begin{verbatim}
1
2 7 13 20
9 11 12
\end{verbatim}
然后在比较两个子list 中head位置那个更小,把它放在总list的第二个位置
\begin{verbatim}
1 2
7 13 20
9 11 12
\end{verbatim}
一直这么继续下去,
这里的每一步都是固定数目的操作,和每一步中的数组的尺寸无关,每一步总,我们只关注两个head,并挑出最小的,再把数组指针推进一位,所以我知道当前的标头在哪里.
所以,对于总数为n的输入,时间是$\theta(n)$的
所以把两个list遍历和排序的时间是$\theta(n)$,有时我们称之为线性时间

merge sort的例子
\begin{figure}[htbp]
  \centering
  \includegraphics[scale = 0.7]{sort_merge_example}\\
  \caption{Merge sort example}\label{fig.sort.merge.example}
\end{figure}

时间复杂度
$$
T(n) =
\left\{
  \begin{array}{ll}
	\theta(1) \si n=1 \\
	2 \times T(n/2) + \theta(n) \si n>1
  \end{array}
\right.
$$

\subsection{Recursion tree}
一直做下去,得到,每一行的和都为$cn$,数的深度为$\lg n$,树的level是$\lg n+1$,最后一层的叶节点有$n$个,每一个都是$\theta(1)$

\begin{figure}[htbp]
  \centering
  \includegraphics[scale = 0.7]{recursion_tree}\\
  \caption{Recursion tree example}\label{fig.compute.recursion_tree}
\end{figure}

为了便于计算,我们在这里设$\theta(1)=c$;
先把每一层的加起来,得到都是$cn$,然后再把所有层加起来,得到
$$T(n)=(\lg n +1)cn=\Theta(n\lg n )$$
So merge sort beats insertion sort.
$$ \theta(n\lg n )<\theta(n^2) $$

\section{渐进符号,递归及解法}
\begin{definition}
\begin{figure}[htbp]
  \centering
  \includegraphics[scale = 0.7]{notation_asymptotical}\\
  \caption{Asymptotical notation}\label{fig.notation.asymptotical}
\end{figure}
\textbf{BIG O:上界 <=}\\
$O(g(n)) = \{f(n): $there exist positive constants $c$ and $n_0$ such that $0 \leq f(n) \leq cg(n)$ for all $n \geq n_0\}$\\
We say that $g(n)$ is an asymptotically upper bound for $f(n)$

\textbf{BIG $\Omega$下界 >=}\\
$\Omega(g(n)) = \{f(n): $there exist positive constants $c$ and $n_0$ such that $0\leq cg(n)\leq f(n)$ for all $n geq n_0\}$\\
We say that $g(n)$ is an asymptotically lower bound for $f(n)$.

\textbf{BIG $\Theta$ =}\\
$\Theta(g(n)) = \{f(n) : $there exist positive constants $c_1, c_2$, and $n_0$ such that $0\leq c\lg n\leq f(n)\leq c_2g(n)$ for all $n geq n_0\}$\\
$$\Theta(g(n))=O(g(n)) \cap \Omega(g(n)) $$\\
We say that $g(n)$ is an asymptotically tight bound for $f(n)$.

The definition of $\Theta(g(n))$ requires that every member $f(n) in \Theta(g(n))$ be asymptotically nonnegative, that is, that $f(n)$ be nonnegative whenever $n$ is sufficiently large.
\end{definition}

Ex:
$2n^2 + \Theta(n) = \Theta(n^2)$
for any function $f(n) in \Theta(n)$, there is some function $g(n) in \Theta(n^2)$ such that $2n^2 + f(n) = g(n)$ for all $n$. In other words, the right-hand side of an equation provides a coarser level of detail than the left-hand side.

o-notation
to denote an upper bound that is not asymptotically tight.\\
$o(g(n)) = \{f(n) : $for any positive constant $c > 0$, there exists a constant $n_0 > 0$ such that $0 \leq f(n) < cg(n)$ for all $n geq n_0\}$\\
For example, $2n = o(n^2)$, but $2n^2 \neq o(n^2)$

o-notation表示的是一种相差比较大的\\
in the o-notation, the function $f(n)$ becomes insignificant relative to $g(n)$ as $n$ approaches infinity; that is,
$$\lim_{n \to \infty} \frac{f(n)}{g(n)} = 0 $$

ω-notation
By analogy, ω-notation is to ?-notation as o-notation is to O-notation.
$$ \lim_{n \to \infty} \frac{f(n)}{g(n)} = \infty $$

\textbf{Comparison of functions}\\
与数的比较类比
\begin{itemize}
	\item $f(n) = O(g(n))	\approx	a \leq b$
	\item $f(n) = \Omega(g(n))	\approx a \geq b$
	\item $f(n) = \Theta(g(n))	\approx	a = b$
	\item $f(n) = o(g(n))	\approx	a \ll b$
	\item $f(n) = \omega(g(n))	\approx	a \gg b$
\end{itemize}

\subsection{Standard notations and common functions}
函数的单调性\\
monotonically increasing单调增\\
monotonically decreasing\\
strictly increasing严格递增\\
strictly decreasing

Floors and ceilings
$$
x -1 < \lfloor x \rfloor \leq x \leq \lceil x \rceil < x + 1
$$

For any integer $n, \lceil n/2 \rceil + \lfloor n/2 \rfloor = n$

And for any real number $n \geq 0$ and integer $a,b >0$
\begin{enumerate}
	\item $\lceil \lceil n/a \rceil /b \rceil = \lceil n/(ab) \rceil$
	\item $\lfloor \lfloor n/a \rfloor /b \rfloor = \lfloor n/(ab) \rfloor$
	\item $\lceil a/b \rceil \leq (a+(b-1))b$
	\item $\lfloor a/b \rfloor \geq (a-(b-1))/b$
	\item $a \mod n = a - \lfloor a/n \rfloor n$
\end{enumerate}

$\lim_{n \to \infty} \dfrac{n^b}{a^n} = 0$
from which we can conclude that
$n^b = o(a^n)$.
Thus, any exponential function with a base strictly greater than $1$ grows faster than any polynomial function.

$$
\lim_{n \to \infty} \frac{\lg ^b n}{(2^a)^{\lg n}} = \lim_{n \to \infty}\frac{\lg ^b n}{n^a} = 0
$$
$\lg^bn = o(n^a)$,
for any constant $a > 0$. Thus, any positive polynomial function grows faster than any polylogarithmic function.
$$
n! = \sqrt{2\pi n} (\frac{n}{e})^n (1 + \Theta(\frac{1}{n}))
$$
Functional iteration\\
We use the notation $f(i)(n)$ to denote the function $f(n)$ iteratively applied $i$ times to an initial value of $n$. Formally, let $f(n)$ be a function over the reals. For nonnegative integers $i$, we recursively define
$$
f^{(i)}(n) =
\left\{
  \begin{array}{ll}
		  n & \si i = 0\\
		  f(f^{(i-1)}(n)) & \si i >0
  \end{array}
\right.
$$
For example, if $f(n) = 2n$, then $f^{(i)}(n) = 2^in$.

\subsection{Substitution method}
Substitution method for solving recurrences entails two steps:\\
Guess the form of the solution.\\
Use mathematical induction(数学归纳法) to find the constants and show that the solution works.

Ex:
$T(n) = 2T(\lfloor n/2 \rfloor) + n$\\
猜测$T(n) = O(n \lg n)$,然后归纳法证明

$T(n) = 2T(n/2 + 17) + n$
与上一式相差$17$,但是当$n$很大时,$17$可以忽略掉,所以仍然猜测$T(n) = O(n \lg n)$,然后尝试用归纳法证明,发现是正确的

\subsubsection{Subtleties}
$T(n) = T(n/2) + T(n/2) + 1$.\\
我们猜测$T(n) \leq cn$\\
$T(n) \leq c n/2 + c n/2 + 1 =cn + 1$ ,wrong,但是只差了一个常数\\
we're only off by the constant 1, a lower-order term,加上一个lower-order term,猜测$T(n) \leq cn - b$\\
$T(n) \leq (c n/2 - b) + (c n/2- b) + 1 = cn - 2b + 1 \leq cn - b$\\
imply $b \geq 1$,所以当$b \geq 1$时,$T(n) \leq cn - b$

\subsubsection{Changing variables}
$T(n) = 2T(\lfloor \sqrt{n} \rfloor) +\lg n$\\
Renaming $m = \lg n$ yields  $T(2^m) = 2T(2^m/2) + m$.\\
We can now rename $S(m) = T(2^m)$ to produce the new recurrence
$S(m) = 2S(m/2) + m$,\\
这个见过,$S(m) = O(m \lg m)$.\\
Changing back from $S(m)$ to $T(n)$, we obtain
$$
T(n) = T(2^m) = S(m) = O(m \lg m) = O(\lg n \lg \lg n).
$$

\subsection{Recursion-tree method}
In a recursion tree, each node represents the cost of a single subproblem somewhere in the set of recursive function invocations.

We sum the costs within each level of the tree to obtain a set of per-level costs, and then we sum all the per-level costs to
determine the total cost of all levels of the recursion.

Recursion trees are particularly useful when the recurrence describes the running time of a divide-and-conquer algorithm.

A recursion tree is best used to generate a good guess,
which is then verified by the substitution method. So we can often tolerate a small amount of "sloppiness", since you will be verifying your guess later on.

Ex:
$T(n) = 3T(n/4) + \Theta(n^2)$\\
We create a recursion tree for the recurrence: $T(n) = 3T(n/4) + cn^2$,如图\ref{fig.compute.recursion_tree2}所示
\begin{figure}[htbp]
  \centering
  \includegraphics[scale = 0.7]{recursion_tree2}\\
  \caption{Recursion tree example}\label{fig.compute.recursion_tree2}
\end{figure}

Assume that n is an exact power of 4 (another example of tolerable sloppiness)\\
Its height is $\log_4 n$ (it has $\log_4 n + 1$ levels ($0, 1, 2,..., \log_4 n$))\\
The subproblem size for a node at depth i is $n/4^i$.\\
Each level has three times more nodes than the level above, and so the number of nodes at depth $i$ is $3i$.

深度为 $i = 0, 1, 2,..., \log_4 n - 1$,的nodes加起来 is $3^i c(n/4^i)^2 = (3/16)^icn^2$.\\
最后一层,at depth $\log_4n$, has $3^{\log_4 n} = n^{\log_4 3}$ nodes, each contributing cost $T(1)$, for a total cost of $n^{\log_4 3}T(1)$ , which is $\Theta(n^{\log_4 3})$.\\
把所有层的加起来,得到整个树的时间
\begin{equation}
\begin{split}
		T(n) & = cn^2 + \frac{3}{16} cn^2 + (\frac{3}{16})^2 cn^2 + \dots + (\frac{3}{16})^{\log_4 n - 1}cn^2 + \Theta(n^{\log_4 3}) \\
				   & = \sum_{i=0}^{\log_4 n - 1}(\frac{3}{16})^i cn^2 + \Theta(n^{\log_4 3}) \\
				   & = \frac{(3/16)^{\log_4 n} - 1}{(3/16)-1} cn^2 + \Theta(n^{\log_4 3})
\end{split}
\end{equation}
保留高阶项,得到$T(n)=O(n^2)$,
然后再用归纳法证明,发现是正确的

\subsection{Master method}
Theorem:Let $a \geq 1$ and $b > 1$ be constants, let $f(n)$ be a asymptotically positive function, and let $T(n)$ be defined on the nonnegative integers by the recurrence
$$T(n) = aT(n/b) + f(n)$$
Where we interpret $n/b$ to mean either $\lceil n/b \rceil$ or $\lfloor n/b \rfloor$. Then $T(n)$ can be bounded asymptotically as follows.
\begin{enumerate}
	\item If $f(n) = O(n^{\log_b a - \epsilon})$ for some constant $\epsilon > 0$, then $T(n) = \Theta(n^{\log_b a})$
	\item If $f(n) = n^{\log_{b} a}$, then $T(n) = \Theta(n^{\log_b a} \lg n)$
	\item If $f(n) = n^{\log_{b} a + \epsilon}$ for some constant $\epsilon > 0$, and if $a f(n/b) \leq cf(n)$ for some constant $c < 1$ and all sufficiently large $n$, then $T(n) = \Theta(f(n))$.
\end{enumerate}

总结规律如下:
Compare the function $f(n)$ with the function  $n^{\log_b a}$\\
if $f(n)$ is polynomial smaller, case $1$\\
if $f(n)$ is polynomial lager, case $3$

It is important to realize that \textbf{these three cases do not cover all the possibilities for} $f(n)$.
There is a gap between cases $1$ and $2$ when $f(n)$ is smaller than $n^{\log_b a}$ but not polynomially smaller.
Similarly, there is a gap between cases $2$ and $3$ when $f(n)$ is larger than $n^{\log_b a}$ but not polynomially larger.
If the function $f(n)$ falls into one of these gaps, or if the regularity condition in case $3$ fails to hold,
the master method cannot be used to solve the recurrence.

Ex: $T(n) = 3T(n/4) + n \lg  n$,\\
we have $a = 3, b = 4, f (n) = n \lg  n$,\\
$f(n)/(n^{\log_b a})=n\lg n/(n^{\log_4 3})=n^{1 - \log_4 3} \times \lg n$\\
so $f(n)$ is polynomial lager, case $3$, $T(n) = \Theta(n\lg  n)$.

\subsubsection{Proof of master method}
树的深度是$\log_b n$
图示证明见\ref{fig.compute.recursion_tree.proof}
\begin{figure}[htbp]
  \centering
  \includegraphics[scale = 0.7]{recursion_tree_proof}\\
  \caption{Recursion tree proof}\label{fig.compute.recursion_tree.proof}
\end{figure}
不太严格的证明,但是有助于理解\par
case 3:前面是呈几何级数递减的,所以前面所有层的和 is dominated by the first member $f(n)$\\
最后一层的和为$a^(\log_b n) \times  \Theta(1)=n^(\log_b a) \Theta(1)= \Theta(n^(\log_b a))$\\
$T(n)=\Theta(f(n))$\\
所以比较$f(n) and (n^(\log_b a))$也就是比较最后一层与前面所有层的

case 1:前面是呈几何级数递增的,因为$f(n)$ is polynomial smaller than $n^(\log_b a)$,所以整个求和中,$n^(\log_b a)$占据主导\\
$T(n)=\Theta(n^(\log_b a))$

case 2: 由于最顶层和最底层的差不多,在中间的,从最顶层呢过到最底层变化又不大,所以基本上都一个量级的,所以总和是:\\
$T(n)=(\log_b n+1)f(n)=(\log_b n+1) \times n^(\log_b a)=\Theta(f(n) \times \lg n)$

\section{分治法 divide and conquer}
The divide-and-conquer design paradigm
\begin{enumerate}
	\item Divide the problem (instance) into subproblems.
	\item Conquer the subproblems by solving them recursively.
	\item Combine subproblem solutions
\end{enumerate}

\subsection{Merge sort}
\begin{enumerate}
	\item Divide:Trivial.
	\item Conquer:Recursively sort 2 subarrays.
	\item Combine:Linear-time merge.
\end{enumerate}
$$
T(n)=2 \times T(n/2) +\Theta(n)=\Theta(n\lg n)
$$

\subsection{Binary serach}
find x in \textbf{sorted array}
\begin{enumerate}
	\item divide, compare x with middle
	\item conquer : recurse in one subarray
	\item combine: trivial
\end{enumerate}
$$
T(n)=\Theta(1)+1 \times T(n/2) = \Theta(\lg n)	\eqnote{master method}
$$

Algorithm $BinarySearch(L; x; first; last)$\\
Input: Array $L[first ; last]$ and value $x$.\\
Output: $1$ if $x \in L$, or $i$, $0<=  i < n$ if $L[i] = x$

pseudo code\\
if $first > last$ then return $-1$\\
else \{\\
$middle \leftarrow (first+last)/2$\\
if $L[middle]=x$ then return $middle$\\
else if $L[middle]<x$ then return $BinarySearch(L; x; middle+1; last)$\\
else return $BinarySearch(L; x; first; middle - 1)$\\
\}

\subsection{Powering a number}
given number $x$, integer $n \geq 0$, compute $x^n$\\
general method:$x \times x \times x \times x \times \cdots \times x =x^n$,  $T(n)=\Theta(n)$

Divide and conquer:
$$
x^n =
\left\{
  \begin{array}{ll}
		  x^{n/2} \times x^{n/2}  \eqnote{if n even}\\
	x^{(n-1)/2} \times x^{(n-1)/2} \times x \eqnote{if n odd}
  \end{array}
\right.
$$
$T(n)=1 \times T(n/2) +\Theta(1)=\Theta(\lg n)$\\
$T(n/2)$表示算出平方根的时间,$\Theta(1)$表示将这个算出来的平方根平方的时间

\subsection{Fibonacci numbers}
$F_0=0,F_1=1,F_2=1,F_3=3.....$\\
通式:$F_n=F_{n-1} + F_{n-2}$\\
General method: $T(n)=\Omega(\phi^n)$ with $\phi=(1+\sqrt{5})/2$
golden ratio\\
exponential time

\textbf{bottom-up}\\
Compute $F_0,F_1,F_2,....$ in order\\
当我们要计算$F_n$的时候,我们已经计算出了$F_{n-1}$ and $F_{n-2}$,直接将这两个相加,就得到$F_n$, 将两个已知数相加所需要的时间为常数\\
所以只需要依次计算各个项就可以了\\
$T(n)=n \times \Theta(1)=\Theta(n)$

\textbf{Naive recursive squaring}\\
根据Fibonacci通项公式,直接计算,and round to the nearest integer\\
但是浮点数的计算误差比较大

\textbf{Recursive squaring}
\begin{theorem}
$$
\left(
  \begin{array}{cc}
		  F_{n+1} & F_n \\
		  F_n & F_{n-1}
  \end{array}
\right)
=
\left(
  \begin{array}{cc}
		  1 & 1 \\
		  1 & 0
  \end{array}
\right)^n
$$
\end{theorem}
prove the matrix equation by induction on n
$T(n)=T(n/2)+\Theta(1)$
$T(n)=\Theta(\lg n)$

\subsection{Matrix multiplication}
input $A=a_{ij}, B=b_{ij}$\\
output $C=A \times B$\\
Standard algorithm
\begin{verbatim}
for i ←1 to n
	do for j←1 to n
			do cij←0
			for k←1 to n
				do cij←cij+ aik \times bkj
\end{verbatim}
直接计算,三层嵌套循环,时间复杂度是$\Theta(n^3)$

divide and conquer algo:\\
直接分治法
$n×n$ matrix = $2×2$ matrix of $(n/2)×(n/2)$ submatrices:

$$
\mathbf{A} =
\begin{bmatrix}
\mathbf{A}_{1,1} & \mathbf{A}_{1,2} \\
\mathbf{A}_{2,1} & \mathbf{A}_{2,2}
\end{bmatrix}
\mbox { , }
\mathbf{B} =
\begin{bmatrix}
\mathbf{B}_{1,1} & \mathbf{B}_{1,2} \\
\mathbf{B}_{2,1} & \mathbf{B}_{2,2}
\end{bmatrix}
\mbox { , }
\mathbf{C} =
\begin{bmatrix}
\mathbf{C}_{1,1} & \mathbf{C}_{1,2} \\
\mathbf{C}_{2,1} & \mathbf{C}_{2,2}
\end{bmatrix}
$$
with
$$
\mathbf{A}_{i,j}, \mathbf{B}_{i,j}, \mathbf{C}_{i,j} \in R^{2^{n-1} \times 2^{n-1}}
$$
then
$$
\begin{aligned}
\mathbf{C}_{1,1} = \mathbf{A}_{1,1} \mathbf{B}_{1,1} + \mathbf{A}_{1,2} \mathbf{B}_{2,1} \\
\mathbf{C}_{1,2} = \mathbf{A}_{1,1} \mathbf{B}_{1,2} + \mathbf{A}_{1,2} \mathbf{B}_{2,2} \\
\mathbf{C}_{2,1} = \mathbf{A}_{2,1} \mathbf{B}_{1,1} + \mathbf{A}_{2,2} \mathbf{B}_{2,1} \\
\mathbf{C}_{2,2} = \mathbf{A}_{2,1} \mathbf{B}_{1,2} + \mathbf{A}_{2,2} \mathbf{B}_{2,2}
\end{aligned}
$$
With this construction we have not reduced the number of multiplications.
We still need $8$ multiplications to calculate the $C_{i,j}$ matrices, the same number of multiplications we need when using standard matrix multiplication.

Now comes the important part. We define new matrices
$$
\begin{aligned}
& \mathbf{M}_{1} := (\mathbf{A}_{1,1} + \mathbf{A}_{2,2}) (\mathbf{B}_{1,1} + \mathbf{B}_{2,2})\\
& \mathbf{M}_{2} := (\mathbf{A}_{2,1} + \mathbf{A}_{2,2}) \mathbf{B}_{1,1}\\
& \mathbf{M}_{3} := \mathbf{A}_{1,1} (\mathbf{B}_{1,2} - \mathbf{B}_{2,2})\\
& \mathbf{M}_{4} := \mathbf{A}_{2,2} (\mathbf{B}_{2,1} - \mathbf{B}_{1,1})\\
& \mathbf{M}_{5} := (\mathbf{A}_{1,1} + \mathbf{A}_{1,2}) \mathbf{B}_{2,2}\\
& \mathbf{M}_{6} := (\mathbf{A}_{2,1} - \mathbf{A}_{1,1}) (\mathbf{B}_{1,1} + \mathbf{B}_{1,2})\\
& \mathbf{M}_{7} := (\mathbf{A}_{1,2} - \mathbf{A}_{2,2}) (\mathbf{B}_{2,1} + \mathbf{B}_{2,2})
\end{aligned}
$$
only using $7$ multiplications (one for each $M_k$) instead of $8$. We may now express the $C_{i,j}$ in terms of $M_k$, like this:
$$
\begin{aligned}
& \mathbf{C}_{1,1} = \mathbf{M}_{1} + \mathbf{M}_{4} - \mathbf{M}_{5} + \mathbf{M}_{7}\\
& \mathbf{C}_{1,2} = \mathbf{M}_{3} + \mathbf{M}_{5}\\
& \mathbf{C}_{2,1} = \mathbf{M}_{2} + \mathbf{M}_{4}\\
& \mathbf{C}_{2,2} = \mathbf{M}_{1} - \mathbf{M}_{2} + \mathbf{M}_{3} + \mathbf{M}_{6}
\end{aligned}
$$
We iterate this division process $n$ times (recursively) until the submatrices degenerate into numbers (elements of the ring $R$).
The resulting product will be padded with zeroes just like $A$ and $B$, and should be stripped of the corresponding rows and columns.

%% $$
%% \left(
%%   \begin{array}{cc}
%% 		  r & s \\
%% 		  t & u
%%   \end{array}
%% \right)
%% =
%% \left(
%%   \begin{array}{cc}
%% 		  a & b \\
%% 		  c & d
%%   \end{array}
%% \right)
%% +
%% \left(
%%   \begin{array}{cc}
%% 		  e & f \\
%% 		  g & h
%%   \end{array}
%% \right)
%% $$
%% $$
%% C = A \dot B
%% $$
%% r =ae+bg
%% s =af +bh
%% t =ce+dh
%% u =cf +dg
%% 8 multiplications of (n/2)×(n/2) submatrices
%% 4^adds of (n/2)×(n/2) submatrices
%% T(n)=8 \times T(n/2)+\Theta(n^2)
%% ($n \times n$的两个矩阵相加的时间复杂度是$\Theta(n^2)$,$n/2 \times n/2$也是$\Theta(n^2)$,与前面只是相隔了常数倍,不影响结果)
%% $n^log_b a = n^log_2 8 = n^3 ? CASE 1 ? T(n) = \Theta(n^3)$.
%% No better than the ordinary algorithm
%%
%% Strassen's algorithm
%% idea: reduce the number of multiplications
%% Multiply $2×2$ matrices with only $7$ recursive mults.
%% \begin{verbatim}
%% P1 = a ? ( f – h)
%% P2 = (a + b) ? h
%% P3 = (c + d) ? e
%% P4 = d ? (g – e)
%% P5 = (a + d) ? (e + h)
%% P6 = (b – d) ? (g + h)
%% P7 = (a – c) ? (e + f )
%%
%% r = P5 + P4 – P2 + P6
%% s = P1 + P2
%% t = P3 + P4
%% u = P5 + P1 – P3 – P7
%% \end{verbatim}
%% 7 mults, 18 adds/subs
%%
%% 我们对r进行验证一下:
%% \begin{verbatim}
%% r = P5 + P4 – P2 + P6
%% = (a + d) (e + h)
%% + d (g - e)
%% - (a + b) h
%% + (b - d) (g + h)
%% = ae + ah + de + dh
%% + dg - de
%% – ah - bh
%% + bg + bh - dg - dh
%% = ae + bg
%% \end{verbatim}

Strassen's algorithm divide and conquer
\begin{enumerate}
	\item Divide: Partition A and B into $(n/2) \times (n/2)$ submatrices. Form terms to be multiplied using $+$ and $-$, time consumed: $\Theta(n^2)$
	\item Conquer: Perform $7$ multiplications$(M1,M2, \ldots, M7)$ of $(n/2)×(n/2)$ submatrices recursively.time consumed: $7 \times T(n/2)$
	\item Combine: Form $C(r, s, t, u)$ using $+$ and $-$ on $(n/2)×(n/2)$ submatrices.time consumed: $\Theta(n^2)$
\end{enumerate}

时间复杂度
$$T(n)=7 \times T(n/2) + \Theta(n^2)=\Theta(n^{\lg7})=\Theta(n^{2.807355})$$
当前最好的为$n^{2.376}$  (理论上)

\section{快排及随机算法}
Tony Hoare在1962年发明\\
-divide and conquer\\
-sorts "in place"(merge sort needs extra space, but qsort does not)\\
-very practical(with tuning)

In the worst case, it makes $O(n^2)$ comparisons, though this behavior is rare. Quicksort is often faster in practice than other $O(n \log n)$ algorithms.\\
Additionally, quicksort's sequential and localized memory references work well with a cache.
Quicksort is a comparison sort and, in efficient implementations, is not a stable sort. Quicksort can be implemented with an in-place partitioning algorithm,
so the entire sort can be done with only $O(\log n)$ additional space used by the stack during the recursion.

Algo\\
1 Divide: partition array into $2$ sub arrays(见图\ref{fig.sort.quick.partition}) around pivot(支点) such that elements in lower $subarray<= x<=elements$ in upper subarray\\
\begin{figure}[htbp]
  \centering
  \includegraphics[scale = 0.7]{sort_quick_partition}\\
  \caption{Quick sort partition}\label{fig.sort.quick.partition}
\end{figure}
2 Conquer: recursively sort 2 subarrays\\
3 Combine: trivial(nothing to do for the combine)

\textbf{key step in quicksort is partition step}\\
快排在递归中也会进行partition\\
key: linear-time $\Theta(n)$ partitioning subroutine
\begin{verbatim}
Partition(A,p,q) //A[p...q]
x← A[p]  //pivot A[p]
i ← p
for j← p+1 to q{
    if A[j]<=x{
		i← i+1 //i加上1之后就到了大于x的那部分, 然后再将A[j] 交换到i 位置,loop invariant 保持
		exchange A[i] with A[j]
    }//end if
}//end for
exchange A[p] with A[i] // 将pivot 交换到中间, 完成partition 的任务
return i  //end function
\end{verbatim}

\begin{figure}[htbp]
  \centering
  \includegraphics[scale = 0.7]{sort_quick_loopInvariant}\\
  \caption{Quick sort loop invariant}\label{fig.sort.quick.loopInvariant}
\end{figure}
At the beginning of each iteration of the loop, loop invariant
\begin{itemize}
\item if $k \in [p,i]$,then $A[k] \leq x$ and $A[p]=x$
\item if $k \in [i+1,j-1]$,then $A[k] >x$
\item if $k \in [j,q]$,then $A[k]$ unknown
\end{itemize}

running time  $T(n)\Theta(n)$  (只需要遍历一遍)
Ex: $2 8 7 1 3 5 6 4$\\
在这个例子中, 最后一个元素被选为pivot, 实际上pivot位于$p$或者$r$处都可以
\begin{figure}[htbp]
  \centering
  \includegraphics[scale = 0.7]{sort_quick_example}\\
  \caption{Quick sort example}\label{fig.sort.quick.example}
\end{figure}

\begin{verbatim}
Quicksort(A,p,q)
if p<q{
	r←Partition(A,p,q)
	Quicksort(A,p,r-1)
	Quicksort(A,r+1,q)
}
Initial call: Quicksort(A,1,n)
\end{verbatim}

\subsection{Analysis}
当元素数目较少时,可以换用其他更快更直接的算法,这样可以避免再简单的情况下也进行递归\\
同时,这是一个tail recursion(尾递归),所以可以使用certain tail recursion optimizations

\textbf{worst-case time}\\
-input sorted or reverse sorted 元素都被分到了一边\\
so one side of partition has no elems, the other side has $n-1$ elems\\
每次递归只减少了$1$,所以递归次数非常多\\
$T(n)
= \Theta(n)+T(0)+T(n-1)
= \Theta(n) + \Theta(1)+T(n-1)
=T(n-1) + \Theta(n)$\\
式中$\Theta(n)$表示partition所需要的时间\\
使用递归树得到时间复杂度为:$T(n)=\Theta(n^2)$

\textbf{best-case analysis(intuition only!)}\\
if we are really lucky, partition splits array $n/2:n/2$\\
$T(n)=\Theta(n)+2T(n/2)=\Theta(n\lg n)$  和merge sort一样

\textbf{Partition $1/10:9/10$}\\
$T(n)=T(n/10)+T(9n/10)+\Theta(n)$\\
用递归树得到\\
$T(n)<=(\log_{10/9}n*cn)+\Theta(n)=O(n\lg n)$\\
$T(n)<=(\log_{10}n*cn)+\Theta(n)=\Omega(n\lg n)$\\
$T(n)=\Theta(n\lg n)$\\
The reason is that any split of constant proportionality yields a recursion tree of depth $\Omega(\lg n)$ whenever the split has constant proportionality.

Suppose we alternate \textit{lucky, unlucky, lucky}...\\
$L(n)=2U(n/2)+\Theta(n)$	lucky的后面是两个unlucky\\
$U(n)=L(n-1)+\Theta(n)$	unlucky\\
Then
$$
\begin{aligned}
L(n)
&=2U(n/2)+\Theta(n) \\
&=2(L(n/2 - 1) +\Theta(n/2))+\Theta(n)\\
&=2L(n/2 - 1) + \Theta(n)\\
&=\Theta(n\lg n)  \eqnote{lucky}
\end{aligned}
$$

Suppose we alternate \textit{unlucky, lucky, unlucky, lucky}...
$$
\begin{aligned}
U(n)
&=L(n-1) + \Theta(n) \\
&=[2U((n-1)/2) + \Theta(n)] +\Theta(n)\\
&=2U((n-1)/2)) + \Theta(n)\\
&=2U(n/2) + \Theta(n)\\
&=\Theta(n\lg n)  \eqnote{lucky}
\end{aligned}
$$

So when we alternate lucky, unlucky, lucky...,we are lucky\\
Or we alternate unlucky, lucky, unlucky, lucky..., we are luck too.\\
So how do we ensure that we are usually lucky? \\
Because if the input is already sorted or reverse sorted, we are going to be unlucky.\\
1.randomly arrange the elements\\
2.randomly choose the pivot

\subsection{Random quicksort}
pick the pivot randomly\\
选择好之后,把这个选中的与array的第一个元素交换位置,这样这个随机选择的pivot就到了array的第一位置,然后再运行Partition函数
\begin{itemize}
\item 运行时间不取决于输入数据的顺序
\item 对输入序列的分布不用做出假设
\item 不存在特定的输入序列会引起worst-case
\item worst-case determined only by random number generator
\end{itemize}

\subsection{Median-of-3 Pivot}
For example, the median-of-3 pivot approach selects three candidate pivots and uses the median one.
If the three pivots are chosen from the first, middle and last positions, then it is easy to see that for the already sorted array,
this will produce an optimum result: each partition will be exactly half ($\pm$ one element) of the problem and we will need exactly ceiling($\log n$) recursive calls.

\subsection{Random method analysis}
Random variable(随机变量) for running time assuming that random numbers are independent.\\
I want to know where I pivoted. 设这个pivot 的位置为随机变量$k$\\
So for $k=0, 1...n-1$ let\\
$x_k=1$ if partition generates a $k: n-k-1$ split, (pivot 算一个数, 所以两者加起来是n-1)\\
$x_k=0$ otherwise\\
这样一个partition,就产生了$n$个random variable,其中只有一个是$1$,其余的都是$0$.\\
例如如果产生的partition为$5:n-6$,那么只有$x_5=1,x_i=0 (i \in  [[0,n-1]]$ and $i!=5)$\\
this type of random variable is called \textbf{indicator random variable}

the expected value of $x_k:\\
E[x_k]=0*P(x_k =0)+1*P(x_k =1) = P(x_k =1) =1/n$\\
因为每一个数$k$ 都有可能取到, 而且概率是一样的, 所以每个数的概率都是 $1/n$
$$
T(n) =
\left\{
  \begin{array}{ll}
		  T(0) + T(n-1) \si 0:n-1 split \\
		  T(1) + T(n-2) \si 1:n-2 split \\
            \vdots \\
		  T(n-1) + T(0) \si n-1:0 split
  \end{array}
\right.
$$
所以这就是T(n)的递归,但是这个递归很麻烦,这里我们就可以看到indicator random variable的优美性,我们将使用indicator random variable将这个递归reduce to(规约到数学上)
$$
T(n) = \sum_{k=0}^{n-1} x_k (T(k) + T(n-k-1) + \Theta(n))
$$

$T(n)$的期望
\begin{equation}
\begin{split}
   E[T(n)] & =E[\sum_{k=0}^{n-1}x_k(T(k)+T(n-k-1)+\Theta(n))] \\
           & =\sum_{k=0}^{n-1}E[x_k(T(k)+T(n-k-1)+\Theta(n))]\\
           & \text{$x_k$随机变量独立于任何其他的partitions, 也就是说$x_k$ 区别于}\\
           & \text{其他递归调用,所以积的期望等于期望的积}\\
           & =\sum_{k=0}^{n-1}E[x_k]*E[(T(k)+T(n-k-1)+\Theta(n))]\\
           & = \frac{1}{n}\sum_{k=0}^{n-1}E[(T(k)] + \frac{1}{n}\sum_{k=0}^{n-1}E[T(n-k-1)] +\frac{1}{n}\sum_{k=0}^{n-1}\Theta(n)\\
           & =\frac{2}{n}\sum_{k=0}^{n-1}E[(T(k)] +\Theta(n)
\end{split}
\end{equation}

$x_k$ 是对$n$进行partition时产生的一个indicator random variable也就是说$x_k$是和$T(n)$ 是相关的, 而后面的$T(k)$ 与$T(n-k-1)$是在有了$x_k$ 之后, 也就是产生了一个$k,n-k-1$的分割后, 对产生的两个新的分割求时间时, 又会产生新的indicator: $x'_k$\\
所以$x_k$ 与$T(k), T(n-k-1)$是相互独立的, 可以运用概率的乘法原则

Absorb $k=0,1$ terms into $\Theta(n)$ for tech convenence,(两个常量加进去不影响$\Theta(n)$)
$$
E[T(n)]=\frac{2}{n}\sum_{k=2}^{n-1}E[T(k)]+\Theta(n)
$$

Use fact\todo{how to prove?}
$$
\sum_{k=2}^{n-1}k\lg k \leq \frac{1}{2}n^2\lg n-\frac{1}{8}n^2
$$
To prove $E[T(n)]<=a*n\lg n$ for constant $a>0$ with the substitution method\\
$E[T(n)] \leq an \lg n –bn (a, b >0)$\\
.....\\
所以我们得到$T(n)$的期望值是$\Theta(n\lg n)$\\
so the running time of randomized quicksort is $\Theta(n\lg n)$

The version of PARTITION given in this chapter is not the original partitioning algorithm. Here is the original partition algorithm, which is due to T.Hoare:
\begin{verbatim}
HOARE-PARTITION(A, p, r)
 1  x ← A[p]
 2  i ← p - 1
 3  j ← r + 1
 4  while TRUE
 5      do repeat j ← j - 1
 6           until A[j] <= x
 7         repeat i ← i + 1
 8           until A[i] >= x
 9         if i < j
10            then exchange A[i] with A[j]
11            else return j
\end{verbatim}
Every element of $A[p ‥ j]$ is less than or equal to every element of $A[j +1 ‥ r]$ when HOARE-PARTITION terminates.\\
The HOARE-PARTITION procedure always places the pivot value (originally in $A[p]$) into one of the two partitions $A[p ‥ j]$ and $A[j + 1 ‥ r]$.

\subsection{Exercise}
[Exercises 7.4-5]
The running time of quicksort can be improved in practice by taking advantage of the fast running time of insertion sort when its input is "nearly" sorted.
When quicksort is called on a subarray with fewer than $k$ elements, let it simply return without sorting the subarray.
After the top-level call to quicksort returns, run insertion sort on the entire array to finish the sorting process.
Argue that this sorting algorithm runs in $O(nk + n \lg(n/k))$ expected time. How should k be picked, both in theory and in practice?

[Problems 7-6: Fuzzy sorting of intervals]
Consider a sorting problem in which the numbers are not known exactly. Instead, for each number, we know an interval on the real line to which it belongs.
That is, we are given n closed intervals of the form $[a_i, b_i]$, where $a_i \leq b_i$.
The goal is to fuzzy-sort these intervals, i.e.,to produce a permutation $〈i_1, i_2,\ldots, i_n〉$ of the intervals such that for $j = 1,2,\ldots,n$
there exist $c_j \in [a_{i_j}, b_{i_j}]$, satisfying $c_1 \leq c_2 \leq \cdots \leq c_n$.\\
1.	Design an algorithm for fuzzy-sorting $n$ intervals. Your algorithm should have the general structure of an algorithm that
quicksorts the left endpoints (the $a_i$ values), but it should take advantage of overlapping intervals to improve the running time.
(As the intervals overlap more and more, the problem of fuzzy-sorting the intervals gets easier and easier.
Your algorithm should take advantage of such overlapping, to the extent that it exists.)\\
2.	Argue that your algorithm runs in expected time $\Theta(n \lg n)$ in general, but runs in expected time $\Theta(n)$
when all of the intervals overlap (i.e., when there exists a value $x$ such that $x \in [a_i, b_i]$ for all $i$).
Your algorithm should not be checking for this case explicitly; rather, its performance should naturally improve as the amount of overlap increases.

\section{Heapsort}
Implementation of heap: tree or array\\
Max-heap: parent $\geq$ children,
	heapsort\\
Min-heap: parent $\leq$ children,
	priority queue

下面我们以Max-heap 进行讲解\\
Array n elements, leaves: $floor(n/2) + 1, floor(n/2) + 2, floor(n/2) + 3, \cdots, n$

The (binary) heap data structure is an array object that we can view as a nearly complete binary tree
\begin{verbatim}
PARENT(i)
   return floor(i/2)
LEFT(i)
   return 2i
RIGHT(i)
   return 2i + 1
\end{verbatim}

\begin{figure}[htbp]
  \centering
  \includegraphics[scale = 0.7]{sort_heap_example}
  \caption{An example of heap}\label{fig.sort.heap.example}
\end{figure}

The height of a node: the length of the longest downward path to a leaf from that node\\
The depth of a node: the length of the path to its root\\
Root node has depth 0, leaf node has height 0

\subsection{Maintaining the heap property}
Its inputs are an array A and an index i into the array\\
A[i] "float down"

\begin{verbatim}
Max-heapify(A, i)
   l = left(i)
   r = right(i)
   if l <= A.heap-size and A[l] > A[i]
      largest = l
   else largest = i
   if r <= A.heap-size and A[r] > A[i]
      largest = r
   if largest != i
      exchange A[i] with A[largest]
      Max-heapify(A, largest)
\end{verbatim}

max heapify: correct a single violation of the heap property in a sub tree's root\\
\textbf{运行Max-heapify(A, i) 的前提条件precondition}: assume that the trees rooted at left(i) and right(i) are max-heaps

\begin{figure}[htbp]
  \centering
  \includegraphics[scale = 0.7]{sort_heap_violation}
  \caption{An example of violation of heap property}\label{fig.sort.heap.violation}
\end{figure}
图\ref{fig.sort.heap.violation}中的第二个节点violates the max heap property, 所以我们可以调用max-heapify(A, 2), 修复好这个之后, 再进行检查直到不出现violation 为止

The children's subtrees each have size at most $2n/3$\todo{not understood}—the worst case occurs when the bottom level of the tree is exactly half full—and therefore we can describe the running time of MAX-HEAPIFY by the recurrence\\
$T(n) \leq T(2n/3) +\Theta(1)$\\
master theorem => $T(n) = O(\lg n)$

add a new item : place the new item at the last position(向左向下走知道不能继续前进), 然后进行修复, compare it with its parent, if it is larger than its parent, exchange them 向上继续进行这个比较直到不违反heap的性质, 也就是向上修复\\
delete the root : 将最后一个节点的值赋给root, 然后向下进行修复

\subsection{Building a heap}
build-max heap: produce a max heap from an unordered array

The elements in the subarray $A[(floor(n/2)+ 1)…n]$ are all leaves of the tree\todo{how to prove}\\
而leaf 已经是max-heap了, 因为leaf 没有children

The procedure BUILD-MAX-HEAP goes through the remaining nodes of the tree and runs MAX-HEAPIFY on each one
\begin{verbatim}
Build-Max-heap(A)
A.heap-size = A.length
    for i = floor(A.length / 2) downto 1
        Max-heapify(A, i) //每次调用max-heapify 之前, 都是满足max-heapify的precondition的
\end{verbatim}

Each call to MAX-HEAPIFY costs $O(\lg n)$ time, and BUILD-MAX-HEAP makes $O(n)$ such calls. Thus, the running time is $O(n\lg n )$. This upper bound, though correct, is not asymptotically tight.

但是我们可以发现:\\
max-heapify takes $O(1)$ for nodes that are one level above the leaves\\
and in general $O(l)$ time for nodes that re l levels above the leaves

an n-element heap has height $floor(\lg n )$ and at most $ceil(n/(2^{h+1}))$ nodes of any height $h$\todo{need proof}

$$
\sum_{h=0}^{\lfloor \lg n \rfloor} \lceil \frac{n}{2^{h+1}} \rceil O(h)
=
O(n \sum_{h=0}^{\lfloor \lg n \rfloor} \frac{h}{2^h})
$$

$$
\sum_{h=0}^{\infty} \frac{ h}{2^h} = \frac{1/2}{(1 - 1/2)^2} = 2
$$
Thus, we can bound the running time of BUILD-MAX-HEAP as
$$
O(n \sum_{h=0}^{\lfloor \lg n \rfloor} \frac{h}{2^h})
= O(n \sum_{h=0}^{\infty} \frac{h}{2^h})
= O(n)
$$

Hence, we can build a max-heap from an unordered array in linear time

\subsection{The heapsort algorithm}
build-max-heap from unordered array\\
find max element $A[1]$\\
swap elements $A[n]$ with $A[1]$, now max element is at the end of the array\\
Discard node $n$ from heap,\\
New root may violate max heap property, fix it

\begin{verbatim}
Heapsort(A)
    Build-Max-heap(A)
    for i = A.length downto 2
        exchange A[1] with A[i]
        A.heap-size = A.heap-size - 1
        Max-heapify(A, 1)
\end{verbatim}

把$A[1]$ 也就是最大值挪到最后一个位置, 同时将heap 的大小减一, 这样最大值就从heap 从取出来了并被放在了正确的位置上, 然后再修复这个heap; 然后再最大值重复这个操作

time $O(n\lg n )$\\
in place sort

\subsection{Priority queues}
max-priority queue, min-priority queue\\
we can use a heap to implement a priority queue

\begin{verbatim}
HEAP-MAXIMUM(A)
    return A[1]
\end{verbatim}
heap-maximum: $\Theta(1)$ time

\begin{verbatim}
HEAP-extract-MAX(A)
    if A.heap-size < 1
        error "heap underflow"
    max = A[1]
    A[1] = A[A.heap-size]
    A.heap-size = A.heap-size - 1
    MAX-HEAPIFY(A,1)
    return max
\end{verbatim}
heap-extract-max: $O(\lg n )$ time

\begin{verbatim}
HEAP-INCREASE-KEY(A,i,key)
    if key < A[i]
        error "new key is smaller than current key"
    A[i] = key
    while i > 1 and A[PARENT(i)] < A[i]
        exchange A[i] with A[PARENT(i)]
        i = PARENT(i)
\end{verbatim}
HEAP-INCREASE-KEY: $O(\lg n )$ time

\begin{verbatim}
MAX-HEAP-INSERT(A,key)
    A.heap-size = A.heap-size + 1
    A[A.heap-size] = -\infty
    HEAP-INCREASE-KEY(A,A.heap-size,key)
\end{verbatim}
MAX-HEAP-INSERT: $O(\lg n )$ time

\section{线性时间排序}
\begin{itemize}
\item quicksort $\Theta(n\lg n )$  randomized
\item heapsort $\Theta(n\lg n )$ 堆排序
\item merge sort $\Theta(n\lg n )$
\item insertion sort $\Theta(n^2)$
\end{itemize}

can we do better than $\Theta(n\lg n )$?\\
comparison sorting model\\
only use comparisons to determine relative order of elements

\subsection{decision tree model 决策树}
Ex: sort $<a_1, a_2, a_3>$

In general $<a_1,a_2...,a_n>$
\begin{itemize}
\item each internal node labeled $i, j$ means compare $a_i$ vs. $a_j$
\item left subtree $a_i <= a_j$
\item right subtree $a_i > a_j$
\item each leaf gives a permutation of the array
\end{itemize}

图\ref{fig.decision_tree.example}是一个决策树的例子
\begin{figure}[htbp]
  \centering
  \includegraphics[scale = 0.7]{decision_tree_example}\\
  \caption{Decision tree example}\label{fig.decision_tree.example}
\end{figure}

\textbf{A lower bound for the worst case}\\
The length of the longest path from the root of a decision tree to any of its reachable leaves represents the worst-case number of comparisons. Consequently,
the worst-case number of comparisons for a given comparison sort algorithm equals the height of its decision tree. A lower bound on the heights of all decision trees in which each permutation appears as a reachable leaf is therefore a lower bound on the running time of any comparison sort algorithm. The following theorem establishes such a lower bound.

[Theorem 8.1] Any comparison sort algorithm requires $\Omega(n \lg n)$ comparisons in the worst case.\\
Proof: From the preceding discussion, it suffices to determine the height of a decision tree in which each permutation appears as a reachable leaf.\\
Consider a decision tree of height h with l reachable leaves corresponding to a comparison sort on n elements.
Because each of the $n!$ permutations of the input appears as some leaf, we have $n! \leq l$.(至少含有$n!$ 个leaves)
Since a binary tree of height h has no more than $2^h$ leaves, we have\\
$n! \leq l \leq 2^h$,
which, by taking logarithms, implies\\
$h \geq \lg (n!) = \Omega(n\lg n)$ (Stirling's approximation $n! \simeq (n/e)^n$

\subsection{Counting sort计数排序}
Element $\in [0, k]$ for some integer $k$. \\
When $k = O(n)$, the sort runs in $\Theta(n)$ time.\\
The \textbf{basic idea} of counting sort is to \textbf{determine, for each input element $x$, the number of elements less than $x$}.
This information can be used to place element $x$ directly into its position in the output array.\\
For example, if there are $17$ elements less than $x$, then $x$ belongs in output position $18$.
This scheme must be modified slightly to handle the situation in which several elements have the same value,
since we don't want to put them all in the same position.

input array $A[1,\ldots, n]$, and thus $length[A] = n$.\\
the array $B[1,\ldots,n]$ holds the sorted output, \\
the array $C[0,\ldots,k]$ provides temporary working storage.

\begin{verbatim}
COUNTING-SORT(A, B, k)
 1  for i ← 0 to k
 2     do C[i] ← 0
 3  for j ← 1 to length[A]
 4     do C[A[j]] ← C[A[j]] + 1
 5  // C[i] now contains the number of elements equal to i.
 6  for i ← 1 to k
 7     do C[i] ← C[i] + C[i - 1]
 8  // C[i] now contains the number of elements leqi
 9  for j ← length[A] downto 1
10     do B[C[A[j]]] ← A[j]
11        C[A[j]] ← C[A[j]] - 1
\end{verbatim}

An important property of counting sort is that it is \textbf{stable}.

Analysis\\
line $1-2 \Theta(k)$\\
line $3-4 \Theta(n)$\\
line $6-7 \Theta(k)$\\
line $9-11 \Theta(n)$\\
Overall time$\Theta(n+k)$\\
在实际上, 当 $k\Theta(n)$ 时, 我们才采用 counting sort

[Exercises 8.2-3] Suppose that the for loop header in line $9$ of the COUNTING-SORT procedure is rewritten as
\begin{verbatim}
9	 for j ← 1 to length[A]
\end{verbatim}
Show that the algorithm still works properly. Is the modified algorithm stable?\\
凭直觉来看, 不是stable

\subsection{Radix sort基数排序}
LSD(Least Significant Digit first)的基数排序适用于位数小的数列,如果位数多的话,使用MSD的效率会比较好.\\
MSD(Most Significant Digit first)的方式与LSD相反,是由高位数为基底开始进行分配,但在分配之后并不马上合并回一个数组中,
而是\textbf{在每个"桶子"中建立"子桶"},将每个桶子中的数值按照下一数位的值分配到"子桶"中.在进行完最低位数的分配后再合并回单一的数组中.

这里讲到的是LSD

\subsubsection{说明}
其原理在于对于待排序的数据,\textbf{整体权重未知}的情况下,
先按权重小的因子排序,然后按权重大的因子排序.\\
例如比较时间,先按日排序,再按月排序,最后按年排序,仅需排序三次.

但是如果先排序高位就没这么简单了.\\
\textbf{基数排序源于老式穿孔机,排序器每次只能看到一个列(这就是一个整体权重未知的例子)},
很多教科书上的基数排序都是对数值排序(这样意义不大),数值的大小是已知的,与老式穿孔机不同.将数值按位拆分再排序,是无聊并自找麻烦的事.算法的目的是找到最佳解决问题的方案,而不是把简单的事搞的更复杂.

\textbf{基数排序更适合用于对时间,字符串等这些整体权值未知的数据进行排序.}
这时候基数排序的思想才能体现出来,例如字符串,如果从高位(第一位)往后排就很麻烦.
而反过来,先对影响力较小,的低位(最后一位)进行排序就非常简单了.
这时候基数排序的思想就能体现出来.

$d$ 位数字, 每一位数字可以取 $k$ 个不同的数, 例如$0$ 到$9$ 十个数, 那么$k=10$
\begin{verbatim}
RADIX-SORT(A, d)
    for i from 1 to d:
        do use a stable sort to sort array A on digit i
\end{verbatim}

\subsubsection{Analysis}
time:$\Theta(d(n+k))$

证明:correctness\\
induct on digit position $t$\\
assume by induction 前$t-1$位已经排好序, 我们需要对第$t$位进行排序\\
1, if two elements have same t\textsuperscript{th} digit, stability $\Rightarrow$ same order $\Rightarrow$ sorted order\\
2, if different t\textsuperscript{th} digit $\Rightarrow$ sorted order

---use counting sort digits\\
---suppose we have $n$ integers each $b$ bits ( $range=[0,2^b-1]$, non-negative)\\
---split into $b/r$ "digits" each $r$ bits (将$r$个bits组合成一个大的"bit")\\
time $O(b/r*(n+k))=O(b/r*(n+2^r))$\\
对r求导, 令其为零求得此时的r值 $r=\lg n$\\
得到$O(bn/\lg n )$

if number in range $0,2^b-1$ then $time = O(dn)$\\
if  $d=O(1)$ then $time=O(n)$

而且只要$d$小于$\lg n$, 就可以击败comparison sort\\
但是在实际上, counting sort is not very good on a cache, in practice, radix sort is not that fast, unless your numbers are really small.\\
但是在理论上, 这个算法很优美

Finally, if you have arbitrary integers, that are one word length long, and you can manipulate a word in constant time.\\
Then the best algorithm we known for sorting runs $n*\sqrt{\lg\lg n }$, and this is a randomized algorithm and very complicated\\
有另外一个算法$n*lg(\lg n )$ worst case, 这篇论文应该可以看懂

\section{Order statistics}
given n elements\\
find $k$th smallest element\\
naive algorithm: sort and return $A[k]$\\
$k=1$ minimum\\
$k=n$ maximum\\
median 中位数 $k=floor((n+1)/2)$ or $ceil((n+1)/2)$

randomized divide and conquer\\
random-select \\
$\Theta(n)$ expected running time\\
$\Theta(n^2)$ worst-case  差不多是$1/n^n$ 的概率\\
intuition for analysis\\
(we assume 所有数都不等)\\
lucky case: $1/10, 9/10$\\
$T(n) \leq T(9/10n)\Theta(n)$  假设第$i$小的位于$9/10$部分\\
unlucky case: $0, n-1$\\
$T(n)=T(n-1)\Theta(n)\Theta(n^2)$

indicator random variable\\
let $T(n)$  be the random variable for running time of random-select\\
define indicator random variable $x_k, k \in [0, n-1]$\\
$x_k = 1$	if partition generates an $k$ to $n-k-1$ splits \\
$x_k = 0$	otherwise

substitution method , $E[T(n)] \leq cn$\\
$3/8n^2$ induction to get $3/8$

worst-case linear time order statistics

Why do we group in $5$ numbers, not $3$ or $7$ or others?\\
$3$是不行的, $7$也行, 但是对于效率没有什么提升

\section{Hashing}
symbol-table problem in compiler\\
table S holding n symbols\\
\textbf{dictionary operations}\\
inset(S, x) x is an element\\
delete(S, x)\\
search(S, k) for a given key\\
search time $\Theta(n)$ in the worst case\\
search time $\Theta(1)$ the expected time\\

\subsection{Direct address tables}
Direct addressing is applicable when we can afford to allocate an array that has one position for every possible key.

\begin{verbatim}
Direct-address-search(T, k)
    return T[k]

Direct-address-insert(T, x)
    T[key[x]] =x

Direct-address-delete(T, x)
    T[key[x]] =nil
\end{verbatim}

\subsection{Hash tables}
With direct addressing, an element with key k is stored in slot $k$.\\
With hashing, this element is stored in slot $h(k)$; that is we use a \textbf{hash function} $h$ to compute the slot from the key $k$.\\
Here $h$ maps the universe $U$ of keys into the slots of a \textbf{hash table} $T[0,\ldots, m-1]$\\
$h: U \rightarrow {0,1, \ldots m-1}$

\paragraph{collision: two keys hash to the same slot.}~\\
Solution: make h appear random thus avoiding collisions or at least minimizing their number.\\
Since $|U|>m$, 肯定会有collisions

\paragraph{Collision resolution by chaining}~\\
We put all elements that hash to the same slot in a linked list.\\
Slot $j$ contains a pointer to the head of list of all stored elements that hash to $j$.\\
通过这种方式解决collision的dictionary operations很容易实现

\begin{verbatim}
chain-hash-insert(T, x)
    insert x at the head of list T[h(key[x])]

chain-hash-search(T, k)
    search for an element with key k in list T[h(k)]

chain-hash-delete(T, x)
    delete x from the list T[h(key[x])]
\end{verbatim}

\paragraph{Analysis}~\\
Insert: worst-case $O(1)$\\
插入很快, 是因为我们假设hash table之前不含有x; 当然我们可以在insert之前先search一遍\\
Search: proportional to the length of linked list\\
Delete: $O(1)$ if the list is doubly linked\\
	same as search if the list is singly linked

\textit{How well does hashing with chaining perform? In particular, how long does it take to search for an element with a given key?}\\
Def.: the \textbf{load factor} of a hash table with n keys and m slots is\\
$\alpha = n/m =$average keys in a slot$=$ average number of elements stored in a chain

\textbf{worst-case}\\
集合S中的所有键经过hash函数生成的hash值是一样的, 因此all elements go to the same slot\\
the access time is $\Theta(n) + $the time to compute the hash function.

\textbf{average-case}\\
assumption of \textbf{simple uniform hashing}\\
---any given element is equally likely to hash into any of the m slots, independently of where any other element has hashed to.

For $i \in [0,m-1]$, let us denote $n_i$ the length of the list $T[i]$\\
So that $n=n_0 + n_1 + \ldots + n_{m-1}$\\
And the average value of $n_i$ is $E[n_i]=\alpha=n/m$

expected unsuccessful search time $\Theta(1+n/m)$  其中1是指进行hash操作的时间\\
expected successful search $time =\Theta(1+n/m)$

Page $227$: \todo{为什么是成比例的, 搜索哪个list不是通过计算key的hash值来确定的吗?}the situation for a successful search is slightly different, since each list is not equally likely to be searched. Instead, \red{the probability that a list is searched is proportional to the number of elements it contains}. Nonetheless, the expected search time is still $\Theta(1+\alpha)$\par
Proof: the number of elements examined during a successful search for an element $x$ is $1$ plus the number of elements that appear before $x$ in x's list. Elements before $x$ in the list were all inserted after $x$ was inserted.\\
Let $x_i$ denote the i\textsuperscript{th} element inserted into the table, and let $k_i=key[x_i]$\\
For keys $k_i$ and $k_j$ ($k_j$注意细微差别), \\
we define indicator random variable $X_{ij}=I{h(k_i)=h(k_j)}$\\
Under the assumption of simple uniform hashing, we have $Pr\{h(k_i)=h(k_j)\}=1/m$\todo{how to compute?}\\
$So E[X_{ij}]=1/m$\\
For $x_i$, the number of elements examined in a successful search is $(1+\sum_{j=i+1}^n X_{ij})$, 其中的$1$是$x_i$ 本身, 他自己也要被检查\\
Thus, the expected number of elements examined in a successful search is:
$$
E[\frac{1}{n} \sum_{i=1}^n (1 + \sum_{j = i + 1}^n X_{ij})]
= 1 + \frac{n-1}{2m}
= 1 + \frac{\alpha}{2} - \frac{\alpha}{2n}
$$

Thus the total time required for a successful search(including the time for computing the hash function) is $\Theta(2 + \alpha/2 - \alpha/{2n}=\Theta(1+\alpha)$

if that is to say When the number of hash-table slots is at least proportional to the number of elements in the table, that is to say $n\Theta(m)$ and $ \alpha \Theta(1)$. Thus searching takes constant time on average. \\
Since insertion takes $O(1)$ worst-case time and deletion takes $O(1)$ worst-case time when the lists are doubly linked, all dictionary operations can be supported in $O(1)$ time on average.

\subsection{Hash functions}
A good hash function satisfies (approximately) the assumption of simple uniform hashing.\\
但是我们很少能够知道keys的distribution, 所以这个条件很难满足.\\
如果我们知道distribution, 例如, if the keys are known to be random real numbers k independently and uniformly distributed in the range $0<=k<1$, the hash function\\
$h(k) = floor(km)$ 满足simple uniform hashing的条件.

In practice, heuristic(启发式的) techniques can often be used to create a good hash function.

\underline{Interpreting keys into natural numbers}:\\
Most hash functions assume that the universe of keys is the set $N={0,1,2, \ldots}$ of natural numbers\\
因此, 当keys不是自然数时, 需要将其转化为自然数.\\
例如, a character string $\rightarrow$ natural number \textbf{in suitable radix notation}\\
For example: the string pt, $(112, 116)$ in ASCII character set, then expressed as a radix$-128$\todo{to be checked} integer, pt becomes $112*128+116=14452$

\subsubsection{Division method}
$h(k)=k \mod m$\\
When using the division method, we usually avoid certain values of $m$.\\
\red{一个数对$2^r$取余, 结果就是这个数的二进制表示的最后$r$位数字\\
同样的道理, 一个数对$10^r$取余, 结果就是这个数 的$10$进制表示的最后$r$位数字}\\
所以选择一个质数来作为$m$比较好, 这个质数一定不要接近$2$或者$10$的某个指数

\subsubsection{Multiplication method}
$h(k)=floor(m(kA\mod 1))$\\
where $x \mod 1$ mans the fractional part of $x$, that is $x - floor(x)$\\
$A$ is constant $\in (0,1)$

An advantage of the multiplication method is that the value of m is not critical. We typically choose it to be a power of $2$($m=2^p$ for some integer $p$). 这样multiplication method 就很容易在计算机上实现

\underline{具体执行方法}\\
word size of the machine: $w$ bits\\
$k$ fits into a single word\\
A is a fraction of the form $s/2^w$, where s is an integer in the range $0 < s < 2^w$. \\
we first multiply $k$ by the $w$-bit integer $s = A \cdot 2^w$. The result is a $2w$-bit value $r1*2^w + r0$, where $r1$ is the high-order word of the product and $r0$ is the low-order word of the product. \\
The desired $p$-bit hash value consists of the $p$ most significant bits of $r0$.\\
%% 这里有一个图片figure :D:\my-container\Documents\算法导论\figure\hash_multiplication_method.png\\
\begin{figure}[htbp]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width= 0.5\textwidth]{hash_multiplication_method}\\
  \caption{Multiplication method for Hash}\label{fig.hash.mult}
\end{figure}

Although this method works with any value of the constant A, it works better with some values than with others.\\
$A=(\sqrt{5} - 1)/2=0.6180339887$

\underline{Example}: suppose we have $k = 123456, p = 14, m = 2^{14} = 16384$, and $w = 32$. Adapting Knuth's suggestion, we choose $A$ to be the fraction of the form $s/232$ that is closest to , so that $A = 2654435769/232$. \\
Then $k \times s = 327706022297664 = (76300 \times 2^{32}) + 17612864$, \\
and so $r1 = 76300$ and $r0 = 17612864$\\
The $14$ most significant bits of $r0$ yield the value $h(k) = 67$\todo{14bits为什么只有67, 这么小??? 不知道这个是怎么算出来的}.\\
$17612864$的二进制表示为: $1000, 0110, 0110, 0000, 0010, 0000, 0$\\
$m=2^p$ and word size of machine is $w$-bit, \\
$k$ fits into a singl word\\
$h(k)=(A*k\mod 2^w) rsh (w-r)$\todo{这个是MIT视频教程中使用的方法}\\
A an odd integer in the range $(2^{w-r}-1, 2^w)$, don't pick $A$ too close to $2^{m-1}$ or $2^w$\\
rsh right shift

\subsubsection{Collision resolution by open addressing}
all elements are stored in the hash table itself.\\
-- no storage for links

进行多次hash操作直到找到一个空的slot\\
probe sequence should be a permutation

table may full up $\mathbf{n \leq m}$\\
deletion is difficult

\todo{这一段没有怎么看懂. 书上的237页}To perform insertion using open addressing, we successively examine, or probe, the hash table until we find an empty slot in which to put the key. Instead of being fixed in the order $0, 1, \ldots, m - 1$ (which requires $\Theta(n)$ search time), the sequence of positions probed depends upon the key being inserted. To determine which slots to probe, we extend the hash function to include the probe number (starting from $0$) as a second input. Thus, the hash function becomes\\
$h : U \times {0, 1, \ldots, m - 1} \rightarrow {0, 1, \ldots, m - 1}$.\\
With open addressing, we require that for every key $k$, the probe sequence\\
$〈h(k,0),h(k,1), \ldots, h(k,m - 1)〉$\\
be a permutation of $〈0, 1, \ldots, m -1〉$, so that every hash-table position is eventually considered as a slot for a new key as the table fills up. In the following pseudocode, we assume that the elements in the hash table $T$ are keys with no satellite information; the key $k$ is identical to the element containing key $k$. Each slot contains either a key or $NIL$ (if the slot is empty).

\begin{verbatim}
HASH-INSERT(T, k)
1  i ← 0
2  repeat j ← h(k, i)
3         if T[j] = NIL
4            then T[j] ← k
5                 return j
6            else i ← i + 1
7   until i = m
8 error "hash table overflow"
\end{verbatim}

搜索用到的sequence和insert用到的sequence一样
\begin{verbatim}
HASH-SEARCH(T, k)
1  i ← 0
2  repeat j ← h(k, i)
3         if T[j] = k
4            then return j
5         i ← i + 1
6    until T[j] = NIL or i = m
7  return NIL
\end{verbatim}

Deletion from an open-address hash table is difficult.\\
对于delete, 我们不能简单的将其标记为nil, 这样在search的时候就会出现问题.\\
例如, 插入$k1$的时候, $i$位置是有值的,所以$k1$会在$i$之后继续probe; 然后$i$位置的值被删除, 被标记成了$nil$, 这样在查找$k1$的时候, 当试探到$i$位置的时候, 发现是$nil$, 达到search中止的条件, 返回$nil$, 但是$k1$确实存在于hash table.\\
所以, 删除不能简单地标记为$nil$, 我们可以标记为DELTETED, 这样insert的代码需要稍作修改, search的代码不用动\\
但是又产生了另外一个问题, search times are no longer dependent on the load factor α, and for this reason chaining is more commonly selected as a collision resolution technique when keys must be deleted.\\
probe strategy\\
1, linear probing: \\
Given an ordinary hash function $h'$, which we refer to as an auxiliary hash function.\\
$h(k,i)=(h'(k)+i) \mod m$\\
Because the initial probe determines the entire probe sequence, there are only m distinct probe sequences.\\
这个方案不好, 如果碰到primary clustering集群现象, 也就是在一个空的slot前面全是full slots, 所以每次探查的效率不高\\

2, quadratic probing $h(k,i)=(h'(k) + c_1 * i + c_2 * i^2) \mod m$\\
better than linear probing\\
but to make full use of hash table\todo{书上没有展开讲解} , c$1$, $c2$ and $m$ are constrained.\\
milder温和的 form of clustering, secondary clustering\\
与linear probing一样, 只有m个不同probe sequence

3, double hashing\\
$h(k,i)=(h_1(k) + i*h_2(k))\mod m$\\
one weay pick $m=2^r$ and $h_2(k)$ always returns an odd奇数 number\\
another way is to let m be prime and $h_2$ always returns a positive integer less than $m$\\
For example:\\
$h_1(k) = k\mod m$\\
$h_2(k) = 1+ (k\mod m')$ where $m'$ is chosen slightly less than $m$ (say $m-1$)\\
$\Theta(m^2)$ probe sequences\todo{未理解}, rather than $\Theta(m)$\\

For uniform hashing, there are $m!$ permutations, so $m!$ probe sequences.

\textbf{Analysis of open addressing hashing}\\
With open addressing, we have at most one element per slot, and thus $n \leq m$, which implies $\alpha \leq 1$

\underline{Theorem}: Given an open-address hash table with load factor $\alpha < 1$, the expected number of probes in an \textit{unsuccessful search} is at most $1/(1-\alpha)$, assuming uniform hashing\\
Proof: In an unsuccessful search, every probe but the last accesses an occupied slot that does not contain the desired key, and the last slot probed is empty.\\
Let us define the random variable $X$ to be the number of probes made in an unsuccessful search, and let us also define the event $A_i$, for $i \in [1,\infty]$ to be the event that there is i\textsuperscript{th} probe and it is an occupied slot. Then \textbf{the event $\{X \geq i\}$ is the intersection of events} \\
$A_1 \cap A_2 \cap \ldots \cap A_{i-1}$\\
$$
\begin{aligned}
Pr\{X \geq i\} & =Pr\{A_1 \cap A_2 \cap \ldots \cap A_{i-1}\} \\
	&= Pr\{A_1\} * Pr\{A_2 | A_1\} * Pr\{A_3 | A_1 \cap A_2\} *\ldots* Pr\{A_{i-1} | A_1 \cap A_2 \cap \ldots \cap A_{i-2}\}
\end{aligned}
$$
Since there are $n$ elements and $m$ slots, $Pr\{A_1\}=n/m$.\\
$Pr\{A_2 | A_1\}$ 由于$A_1$ 是确定发生的, 所以第一个slot已经被占用, 所以还剩下$m-1$个slots, 同时还剩下 $n-1$个elements, 所以$Pr\{A_2 | A_1\}=(n-1)/(m-1)$\\
所以
$$
\begin{aligned}
Pr\{X \geq i\}
&=n/m * (n-1)/(m-1) * (n-2)/(m-2) *\ldots* (n-i+2)/(m-i+2)\\
&\leq (n/m)^{i-1}\\
&=(\alpha)^{i-1}\\
\end{aligned}
$$

$$
E[X]
=\sum_{i=i}^{\infty} i*Pr\{X=i\}\\
=\sum_{i=1}^{\infty} Pr\{X \geq i\}\\
\leq \sum_{i=1}^{\infty} (\alpha)^{i-1}\\
= \sum_{i=0}^{\infty} (\alpha)^i\\
=1/(1 - \alpha)
$$

\noindent table $50$\% full, $2$次\\
table $90$\% full, $10$次

Theorem: Given an open-address hash table with load factor $\alpha < 1$, the expected number of probes in an successful search is at most \\
$1/\alpha * \ln 1/(1-\alpha)$ that is 1ln11-  ,assuming uniform hashing and assuming that each key in the table is equally likely to be searched for.\\
Proof: A search for a key $k$ follows the same probe sequence as was followed when the element with key $k$ was inserted. If $k$ was the $(i+1)$\textsuperscript{st} key inserted into the hash table, the expected number of probes made in a search for $k$ is at most $1/(1 -  i/m)=m/(m - i)$\\
($k$是$i+1$个插入的, 说明hash table中已经含有$i$个element, 所以此时的load factor $i/m$)\\
\textbf{Averaging over all $n$ keys} in the hash table gives us the average number of probes in a successful search:
$$
\begin{aligned}
\frac{1}{n} \sum_{i=0}^{n-1}\frac{m}{m-i}
&= \frac{ m}{n} \sum_{i=0}^{n-1}\frac{1}{m-i}\\
&= \frac{1}{\alpha}(H_m - H_{m-n})
\end{aligned}
$$

where $H_i=\sum_{j=1}^i 1/j$ is the $i$\textsuperscript{th} harmonic number
$$
\begin{aligned}
1/\alpha * (H_m - H_{m-n})
&= 1/\alpha * \sum_{k=m-n+1}^m 1/k\\
&\leq 1/\alpha * \int_{m-n}^m (1/x)dx\\
&=1/\alpha * \ln m/(m-n)\\
&=1/\alpha * \ln 1/(1 - \alpha)
\end{aligned}
$$

\noindent table $50$\% full, $1.387$次\\
table $90$\% full, $2.559$次

\section{Universal hashing and perfect hashing}
全域哈希和完全哈希\\
Perfect hashing: search time $\Theta(1)$ worst case when the set of keys are static(that is, when the set of keys never change once stored).

\subsection{Universal hashing}
weakness of hashing\\
For any choice of hash function, there exists a bad set of keys that all hash to the same slot.\\
The idea is to choose a hash function at \underline{random} independent from keys.\\
This method is called \textbf{universal hashing}.

\underline{Def}: Let $U$ be a universe of keys, and  $H$ a finite collection of hash functions mapping $U$ to ${0, 1, ... m-1}$.

\underline{Universal collection of hash functions}\\
Such a collection is said to be \textbf{universal} if for each pair of distinct keys $k,l \in U$, the number of hash functions $h \in H$ for which $h(k)=h(l)$ is at most $|H|/m$.\\
In other words, with a hash function randomly chosen from $H, Pr\{h(k)=h(l)\} \leq 1/m$\todo{|H|/m and 1/m 都不理解}

\underline{Thm}: if we choose $h$ randomly from the set of hash function $H$, and then we suppose we are hashing $n$ keys into $m$ slots in table $T$, then, for a given key $x$,\\
E\\
\underline{Prof}:\\
Let $C_x$ be random variable denoting total number of collisions of keys in $T$ with $x$, and \\
let indicator random variable $C_{xy}=1$ if $h(x)=h(y)$ and $0$ otherwise \\
Note: $E[C_{xy}]=1/m$\todo{这个概率暂时不懂} and $C_x= \sum_{y \in T-{x}} C_{xy}$\\
After the calculation, we get $E[C_x]=(n-1)/m$\\

\underline{Constructing a universal hash function}\\
Let $m$ be prime, decompose key $k$ into $r+1$ digits:\\
$k=<k_0, k_1, ...., k_r>$ where $0<= k_i <=m-1$\\
Pick $a=<a_0, a-1, ..a_r>$ each $a_i$ is choosen randomly from ${0,1 ... m-1}$\\
Define $h_a(k)=(\sum_{i=0}^r a_i * k_i) \mod m$\\
How big is

\underline{Thm}: H is universal\\
\underline{Proof}: let

\underline{Number theory fact 质数的倒数}\\
Let $m$ be prime. For any $z \in Z_m$ (integers $\mod m$) such that $z \neq 0, \exists$ unique $z' \in Z_m$ such that $z*z' = 1 \mod m$\\
Example: $m=7$\\
\begin{verbatim}
z	1	2	3	4	5	6
z'	1	4	5	2	3	6
\end{verbatim}

\subsection{Perfect hashing 完全哈希}
Hashing can be used to obtain excellent worst-case performance when the set of keys is \textbf{static: once the keys are stored in the table, the set of keys never changes.}\\
Some applications naturally have static sets of keys: consider the set of reserved words保留关键词 in a programming language, or the set of fil names on a CD-ROM.

Perfect hashing: search takes $O(1)$ time in the worst case.\\
Idea: \textbf{$2$ level hashing scheme with universal hashing at each level.} No collisions at level $2$.\\
The first level is essentially the same as for hashing with chainning.\\
Instead of making a list of the keys hashing to slot $j$, however, we use a small secondary hash table $S_j$ with an associated hash function $h_j$. By choosing the hash functions $h_j$ carefully, we can guarantee that there are no collisions at level $2$.\\

\subsubsection{Collision-free}
\underline{Thm}: Hash $n$ keys into $m=n^2$ slots using random $h$ in universal $H$\\
then $Pr\{\text{number of collisions}\}< 1/2$\todo{为什么要用概率, 用期望值我觉得是一个更好的选择}\\
\underline{Proof}: There are $C_n^2$ pairs of keys that may collide; each pair collide with probability $1/m$ if $h$ is chooosen at random from a universal family of $H$ of has$h$ functions.\\
Let $X$ be a random variable that counts the number of collisions.\\
When $m=n^2$, the expected number of collisions is\\
$E[X]=C_n^2 /m=(n^2 - n)/(2n^2) < 1/2$\\
Applying Markov's inequality, $Pr[X \geq 1] \leq E[X]/1 < 1/2$\\

\underline{Markov's inequalty}\todo{Proof}\\
For random variable $x \geq 0, Pr(x \geq t) \leq E[x]/t$\\
Proof : E[x]=

在上面定理所描述的情况下$m=n^2$, 随机选择的一个hash function很有可能没有collisions.\\
所以当the set $K$ of $n$ keys to be hashed(remember that $K$ is static), it is thus easy to find a collision-free hash function $h$ with a few random trials.

当$n$很大时, hash table $m=n^2$ 占用的空间就会很大, 所以我们采用$2$ level hashing approach.\\
First-level, hash function h hashs the keys into $m=n$ slots\\
Second-level, if $n_j$ keys hash to slot $j$, a secondary hash table $S_j$ of size $m_j=n_j^2$ is used to provide collision-free constant-time lookup.

\underline{Corollary}: $Pr\{\text{no collisions}\} \geq 1/2$\\
\underline{Proof}: $Pr{\geq 1 collision} \leq E[number of collisions]/1 < 1/2$

To find a good level $2$ hash function, just test at random. Find one quickly, since $\geq 1/2$ wil work.

\subsubsection{Analysis of storage}
Thm: if we store $n$ keys in hash table of size $m=n$ using a hash function $h$ randomly chosen from a universal class of hash functions, then
$$
E[\sum_{j=0}^{m-1} g_j^2] < 2n
$$

where $n_j$ is the number of keys hashing to slot $j$.\\
\underline{Proof}: $a^2 = a + C_a^2$ 恒等式  $C_a^2$是一个组合数\\
We have:
$$
\begin{aligned}
E[\sum_{j=0}^{m-1} n_j^2]
&=E[\sum_{j=0}^{m-1} (n_j + 2*C_{n_j}^2)]\\
&=E[\sum_{j=0}^{m-1} n_j] + 2*E[\sum_{j=0}^{m-1} C_{n_j}^2]\\
&=E[n] + 2*E[\sum_{j=0}^{m-1} C_{n_j}^2]\\
&=n + 2*E[\sum_{j=0}^{m-1} C_{n_j}^2]
\end{aligned}
$$
to evaluate the summation $\sum_{j=0}^{m-1} C_{n_j}^2$ we observe it is just the total number of collisions. By the proprities of universal hashing, the expected value of this summation is at most\\
$C_n^2 * 1/m =n(n-1)/(2m)=(n-1)/2$\\
since $m=n$. Thus,
$$
E[\sum_{j=0}^{m-1} n_j^2]
\leq n + 2*(n-1)/2
=2n - 1
< 2n
$$
\section{Appendix}
\subsection{Summations}
\subsubsection{Approximation by integrals}
$$
\sum_{k=1}^n \frac{ 1}{k} \geq \int_1^{n+1}\frac{1}{x}dx = \ln(n+1)
$$

$$
\sum_{k=2}^n \frac{ 1}{k} \leq \int_1^{n}\frac{1}{x}dx = \ln n
$$
which yields the bound
$$
\sum_{k=1}^n \frac{ 1}{k} \leq \ln n + 1
$$

\subsection{Relations}
$v_0',v'_0$
A binary relation $R$ on two sets $A$ and $B$ is a subset of the Cartesian product$A \times B$
If $(a, b) \in R$, we sometimes write $a R b$. When we say that R is a binary relation on a set $A$, we mean that R is a subset of $A \times A$. For example, the "less than" relation on the natural numbers is the set $\{(a, b) : a, b$ in $N$ and $a < b\}$. An n-ary relation on sets $A_1, A_2,\ldots, A_n$ is a subset of $A_1 \times A_2 \times \cdots \times  A_n$.

A binary relation $R \subseteq A \times A$ is reflexive if
$a R b$ implies $b R a$ for all $a,b \in A$
\subsection{Probability}
$$
Pr(A\cap B)=\frac{Pr(A)*Pr(B \cap A)}{Pr(B)}
$$

$$
B=(B \cap A)\cup (B \cap \bar{A})
$$

$$
Pr(B)=Pr(B \cap A)+Pr(B \cap \bar{A})
=Pr(A)*Pr(B|A)+Pr(\bar{A})*Pr(B|\bar{A})
$$

$$
Pr(A\cap B)=\frac{Pr(A)*Pr(B \cap A)}{Pr(A)*Pr(B|A)+Pr(\bar{A})*Pr(B|\bar{A})}
$$

\end{document}
