% !Mode:: "TeX:UTF-8"
\documentclass{article}
\input{../public/package}
\input{../public/article}
\begin{document}
\title{Machine Learning}
\author{}
\maketitle
%% \newpage
\tableofcontents
\newpage
\section{Introduction}
algorithms for inferring unkowns from knows.

\textbf{Applications}
\begin{itemize}
\item Spam mail
\item Handwriting
\item Google Streetview
\item Speech Recognition
\item Netflix, 视频推荐
\item Navigation of robot
\item Climate modelling
\end{itemize}

\textbf{Classes of ML problems}:
\begin{itemize}
\item Supervised: Given $((x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n))$
	\begin{itemize}
	\item classification
	\item regression
	\end{itemize}
\item unsupervised: Given $(\vector{x})$, find patterns in the data
	\begin{itemize}
	\item Clustering
	\item density estimation: Given data comes from a unkown probability distribution
	\item dimensional reduction: 降维(but perserve the structure)
	\end{itemize}
\item Semi-supervised: Given $((x_1, y_1), (x_2, y_2), \ldots, (x_k, y_k), x_{k+1}, \ldots, x_n)$, predict $y_{k+1}, \ldots, y_n$
\item Active learning
\item Decision theory
\item Reinforcement learning: rewards/losses, maximize lifetime reward
\end{itemize}
\href{http://i.imgbox.com/sVuBzaqV.png}{unsupervised ML}

\textbf{Generative Discriminative(区别对待) approaches}\\
Given $((x_1, y_1), (x_2, y_2), \ldots, (x_k, y_k), x_{k+1}, \ldots, x_n)$\\
discriminative: $P(y|x)$\\
geneerative: $P(x,y) = f(x|y)P(y) = P(y|x)f(x)$
It is called generative, because it use the density distribution $f(x)$ or $f(y)$

Examples of generative models include:
\begin{itemize}
\item Gaussian mixture model and other types of mixture model
\item Hidden Markov model
\item Probabilistic context-free grammar
\item Naive Bayes
\item Averaged one-dependence estimators
\item Latent Dirichlet allocation
\item Restricted Boltzmann machine
\end{itemize}

Examples of discriminative models used in machine learning include:
\begin{itemize}
\item Logistic regression, a type of generalized linear regression used for predicting binary or categorical outputs (also known as maximum entropy classifiers)
\item Linear discriminant analysis
\item Support vector machines
\item Boosting (meta-algorithm)
\item Conditional random fields
\item Linear regression
\item Neural networks
\end{itemize}

\section{K-Nearest Neighbor(KNN)}
Given $((x_1, y_1), (x_2, y_2), \ldots, (x_k, y_k), x_{k+1}, \ldots, x_n), x$\\
$x_i \in R^d, y_i \in \{0, 1\}$

\textbf{Majority vote of $k$ nearest points}\\
\href{http://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/220px-KnnClassification.svg.png}{KNN demo}

$k=1$, 找到最近的1个点\\
$d(x_i, x_j) = \norm{x_i - x_j}$

$k=3$, 找到最近的3个点, 看label 1多还是label 0多.

Prob interpretation. Fix $k$\\
$P(y) =$ fraction of points $x_i$ in $N_k(x)$ s.t $y_i = y$\\
有时也记为$P(y|x, D)$, this is discriminative

How to select parameter $k$?\\
Cross validation, etc.

\section{Tree: CART approach}
\subsection{Decision Trees(CART Approach)}
Given $((x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)), x$\\

\textbf{Binary tree, minimize error in each leaf}.

see note in paper

\subsection{Regression Tree}
see note in paper

\subsubsection{Growing a regression tree}
Greedy algo

$x_i \in R^d$

1. First split: choose $j$ and $s$ to minimize the quantity:
$$
\min_y (\sum_{i: x_{ij} > s} (y - y_i)^2 + \sum_{i: x_{ij} \leq s} (y - y_i)^2)
$$
As the given data is finite, for a choosen $j$, there is only a finite possibilities for $s$, \\
so we can split the whole region into two regions, we note one of them as $R$.

2. Spliting region $R$: choose $j$ and $s$ to minimize
$$
\min_y (\sum_{i: x_{ij} > s \et x_i \in R} (y - y_i)^2 + \sum_{i: x_{ij} \leq s \et x_i \in R} (y - y_i)^2)
$$

3. Stop when one point

4. only consider splits resulting in regions with $\geq 5$ points in per region

\subsection{classification tree}
\subsubsection{Growing a classification tree}
see note in paper

\subsection{Generalization}
Impurity(不纯, 杂质) measures:
\begin{itemize}
\item misclassfication rate
\item entropy $H_R$
\item $F_R$: Gini index, $G_R = \sum_{y \in Y} P_R(y)(1 - P_R(y))$
\end{itemize}

Other issues about this kind of model
\begin{itemize}
\item Categorical predictiors
\item Loss matrix
\item Missing values, $(x_1, y_1), \ldots, (x_i, y_i)$, and for some $i,j$, $x_{ij}$ is missing, it is noted N/A
\item Linear combinations
\item Instability, means sensitive to the data, that is high variance, but we can use aggregation to deal with it.
\end{itemize}

\section{Bootstrap aggregating(Bagging)}
Ofen prove the performance

\noindent
Given $(x_1, y_1), (x_2, y_2), \ldots (x_n, y_n) \obey P \ iid(Independent Identically Distributed)$\\
Given $x$, predict $y$

我们知道, 如果我们有很多组的given data, 那么, 我们可以在每组数据中, 求得$x$对应的$y$, 然后对这些求得的$y$ 取平均值, 我们有理由确信, 如果given data的组数
足够多, 计算得到的平均值是很接近与真实值的.\\
bootstrap 就是类似于这样操作的一种方法.\\
由于我们知道数据的分布规律, 那么我们可以sampling 多次, 得到多组数据, 如下:\\
$(x_1^{(1)}, y_1^{(1)}), (x_2^{(1)}, y_2^{(1)}), \ldots (x_n^{(1)}, y_n^{(1)})$ \\
$(x_1^{(2)}, y_1^{(2)}), (x_2^{(2)}, y_2^{(2)}), \ldots (x_n^{(2)}, y_n^{(2)})$ \\
$\vdots$\\
$(x_m^{(m)}, y_m^{(m)}), (x_2^{(m)}, y_2^{(m)}), \ldots (x_n^{(m)}, y_n^{(m)})$ \\

\smallskip
True value is $y = f(x)$, $y^{(i)}$ is the estimater, $EY$ is the mean of $y^{(i)}$, and if $EY = y$, then $EY$ is an unbiased estimation.

the squared distance from the true value $y$:
$$E[(Y - y)^2] = E[(Y - EY)^2] = \sigma^2(Y)$$

Aggregation comes in:
define $Z =\dfrac{1}{m} \sum_{i = 1}^m Y^{(i)}$, then $EZ = y$.
And the expected loss:
$$
E[(Z - y)^2] = E[(Z - EZ)^2]
= \sigma^2(Z)
= \sigma^2(\dfrac{1}{m} \sum Y^{(i)})
= \frac{1}{m^2} \sigma^2(\sum Y^{(i)})
= \frac{1}{m^2} \sum \sigma^2(Y^{(i)})
= \frac{1}{m} \sigma^2(Y)
$$
意味着如果取$m \to \infty$, expected loss 将 $\to 0$

用the empirical distribution $\hat{P}$近似$P$, 然后从$\hat{P}$中resampling

\subsection{Bagging for classification}
\begin{itemize}
\item classification: majority vote, $C_1, C_2, \ldots, C_m$
\item regression: estimate probabilities, $P^{(1)}, \ldots, P^{(m)}$, PMF on $y$
\end{itemize}

\subsection{Random forests}
Given data set $D = (x_1, y_1), (x_2, y_2), \ldots (x_n, y_n)$

\begin{verbatim}
for i = 1, ..., B:
    choose bootstrap sample D_i from D
    construct tree T_i using D_i, s.t.
        at each node, choose random subset of features and only consider splitting on those features
\end{verbatim}

Usage:
Give $x$, take majority vote for classification, or average estimate probability for regression.

\section{Maximum Likelihood Estimation(MLE)}
Given data set $D = (\vector{x}$ with $x_i \in R^d$\\
Assume a set of distributions $\{ P_\theta: \theta \in \Theta \}$ on $R^d$.\\
Assume $D$ is a sample from random variables $\vector{X} \obey P_\theta$ iid for some $\theta \in \Theta$

Goal: Estimate the true $\theta$ that $D$ comes from.
\begin{definition}
$\theta_{mle}$ is a MLE for $\theta$ if $\theta_{MLE} = \arg_{\theta \in \Theta} \max P(D|\theta)$ \\
where $P(D|\theta) = P(\vector{x} | \theta) = \prod_{i = 1}^n P(x_i|\theta) = \prod_{i=1}^n P(X_i = x_i|\theta)$,
and $L(\theta) = P(D|\theta)$ is the likelihood function
\end{definition}

Pros:
\begin{itemize}
\item easy
\item interpretable
\item Invariant under reparametrization: $g(\theta_{MLE})$ is a MLE for $g(\theta)$
\item Asymptotic properties
\item Consistent(当$n \to infty$, 有很大的概率使得$\theta$取到真值)
\item Normal
\item Efficient
\end{itemize}

Cons:
\begin{itemize}
\item point estimation: not representative of uncertanity
$$P(x|D) \sim P(x|\theta_{MLE})$$
\item Overfitting: regression example
\item Wrong objective?(may maximize the wrong objective function)
\item Existence or uniqueness not guaranteed
\end{itemize}

\subsection{MLE for univariante Gaussian}
$X \obey N(\theta, \sigma^2)$ with $\theta \in \R$
$$P(x|\theta) = \dfrac{1}{\sqrt{2\pi \sigma^2}} \exp(-\dfrac{1}{2\sigma^2}(x-\theta)^2)$$
$D = (\vector{x}),\ X_i \obey N(\theta, \sigma^2),\ iid$
$$
P(D|\theta)
= P(\vector{x}| \theta)
= \prod_{i=1}^n P(x_i|\theta)
= (\dfrac{1}{\sqrt{2\pi \sigma^2}})^n \exp(-\dfrac{1}{2\sigma^2} \sum_{i=1}^n (x_i -\theta)^2)
$$
为了方便求导, 我们取对数
$$
\log(P(D|\theta)) = -\dfrac{n}{2} \log(2 \pi \sigma^2) - \dfrac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \theta)^2
$$
对$\theta$求导, 然后令导数为$0$
$$
0 = \dfrac{\partial }{\partial \theta} \log(P(D|\theta))
= \dfrac{1}{\sigma^2} \sum_{i=1}^n (x_i - \theta)
= \dfrac{1}{\sigma^2} (\sum_{i=1}^n x_i - n \theta)
\Rightarrow \theta = \dfrac{1}{n} \sum_{i=1}^n x_i
$$
求二阶导我们可以验证这确实是一个maximum.

$$\theta_{MLE} = \mean{x} = \dfrac{1}{n} \sum_{i=1}^n x_i$$
这意味着, sample 的期望的最好估计就是sample的mean.

\subsection{MLE for a PMF on a finite  set}
$X \obey P,\ X \in \{1,2,\ldots, m\}$\\
例如掷骰子, $m=6$\\
$D = (\vector{x}),\ \vector{X} \obey P,\ iid$\\
$\sum_{i=1}^m P_\theta(i) = 1$\\
$P(X=i|\theta) = P(i|\theta) = P_\theta(i) = \theta_i$ with $\theta = (\theta_1, \ldots, \theta_m)$\\
骰子取$i$的概率为$\theta_i$

$$P(x|\theta) = \theta_x= \prod_{j= 1}^m \theta_j^{I(x=j)}$$
where $I(x=j)$ is a indicator.

$$\theta_{MLE} = \arg_{\theta_i: \sum_{i=1}^m \theta_i =1} \max P(D|\theta)$$
$$
\begin{aligned}
P(D|\theta)
& = P(\vector{x}| \theta) \\
& = \prod_{i=1}^n P(x_i|\theta) \\
& = \prod_{i=1}^n \theta_{x_i}\\
& = \prod_{i=1}^n \prod_{j=1}^m \theta_j^{I(x_i = j)}\\
& = \prod_{j=1}^m \prod_{i=1}^n \theta_j^{I(x_i = j)}\\
& = \prod_{j=1}^m \theta_j^{ \sum_{i=1}^n I(x_i = j)}\\
& = \prod_{j=1}^m \theta_j^{n_j}\\
\end{aligned}
$$
where $n_j = \#\{i: x_i = j\}$

取对数
$$ \log(P(D|\theta)) = \sum_{j=1}^m n_j \log(\theta_j) $$
$$ \dfrac{1}{n} \log(P(D|\theta)) = \sum_{j=1}^m \dfrac{n_j}{n} \log(\theta_j) $$
令$q_j = \dfrac{n_j}{n} $\\
由于从data set $D$中, 我们可以count 数字$j$出现的次数, 也就是说$n_j$是已知的,且我们容易发现$\sum_{j=1}^m q_j = 1$

$q$和$\theta$的relative entropy为
$$D(q||\theta) = \sum_{j=1}^m q_j \log(\dfrac{q_j}{\theta_j})$$
And we have $D(q||\theta) \geq 0$ 恒成立.

$$
\dfrac{1}{n} \log(P(D|\theta))
= \sum_{j=1}^m q_j \log(\theta_j)
= \sum_{j=1}^m q_j \log(\dfrac{\theta_j}{q_j}) + \sum_{j=1}^m q_j \log(q_j)
= -D(q||\theta) - H(q)
$$

$$
\theta_{MLE}
= \arg_{\theta_i: \sum_{i=1}^m \theta_i =1} \max P(D|\theta)
= \arg \max \dfrac{1}{n} \log(P(D|\theta))
= \arg \max D(q||\theta)
= q
$$
And it means:
$$\theta_{MLE} = q = (\dfrac{n_i}{n}, \dfrac{n_2}{n}, \ldots, \dfrac{n_m}{n})$$
\textbf{Empirical distribution}\\
同时, 这也是一个很自然的结果, 例如在掷骰子的时候, 为了估计骰子取到每个值的概率, 我们会将骰子掷很多次, 然后统计每个值出现的次数, 除以总的投掷次数, 就是每个值取到的概率.

\section{Exponential Families}
\begin{definition}
An exponential families is a set $\{ P_\theta: \theta \in \Theta \}$ of PMFs or PDFs on $R^d$, s.t.
$$P_\theta(x) = \exp(- \sum_{i=1}^m \eta_i(\theta) s_i(x)) h(x)/z(\theta)$$
where
$\Theta \subset R^k,\ x \in R^d,\ \eta_i: \Theta \rightarrow R,\ s_i:R^d \rightarrow R,\ h: R^d \rightarrow [0, \infty),\ z: \Theta \rightarrow [0, \infty)$
\end{definition}
$s_i$ are sufficient statistics, $h$ is support, scaling, $z$ is partition function.

我们也可以采用矩阵的写法:
$$P_\theta(x) = \exp(- \eta(\theta)^T s(x)) h(x)/z(\theta)$$

\begin{example}
指数分布:$P_\theta(x) = \theta \exp(-\theta x) I(x \geq 0)$\\
$\eta(\theta) = \theta,\ s(x) = x,\ h(x) = I(x \geq 0),\ z(\theta) = 1/\theta,\ \Theta = (0, \infty),\ k=d=1$
\end{example}

\begin{example}
$Bernoulli(\theta)$,
$$
\begin{aligned}
P_\theta 
& = \left\{
  \begin{array}{ll}
	\theta & \si x = 1 \\
	1 - \theta & \si x=0
  \end{array}
\right. \\
& = \theta^{I(x = 1)} (1 - \theta)^{I(x = 0)} h(x) \\
& = h(x) \exp \log (\theta^{I(x = 1)} (1 - \theta)^{I(x = 0)})\\
& = h(x) \exp (I(x=1) \log \theta + I(x=0) \log (1 - \theta))
\end{aligned}
$$

$\eta_1(\theta) = \log \theta,\ n_2(\theta) = \log (1 - \theta),\ s_1(x) = I(x=1),\ s_2(x) = I(x=0)$\\
$h(x) = I(x \in \{0,1\}),\ z(\theta) = 1$

另外, 对于同一个分布, 可以有不同的写法, 例如我们可以把上面的例子写成下面的形式:
$$
\begin{aligned}
P_\theta 
& = \theta^{I(x = 1)} (1 - \theta)^{I(x = 0)} h(x) \\
& = \theta^{I(x = 1)} (1 - \theta)^{1 - I(x = 1)} h(x)\\
& = (1 - \theta)(\dfrac{\theta}{1-\theta}^{I(x=1)}) h(x) \\
& = h(x) (1 - \theta) \exp(I(x=1) \log(\dfrac{\theta}{1-\theta}))
\end{aligned}
$$
\end{example}

\begin{example}
(not an exp family): Uniform $(0, \theta)$
\end{example}

常见的exp fam:\\
Exponential PDF: exponential, normal, Beta, Gamma, Chi-square\\
Exponential PMF: Bernoulli, Bonomial, Poisson, Geometric, Multinomial

exponential families具有的性质:
\begin{itemize}
\item Conjugate priors
\item Maximum entropy
\end{itemize}

\subsection{MLE for an exp fam}
$x \in R^d,\ \theta \in \Theta=R^k,\ \eta(\theta) = \theta$
$$P_\theta(x) = \exp(\theta^T s(x)) h(x)/z(\theta)$$

$D= (\vector{x}),\ x_i \in R^d,\ \vector{X} \obey P_\theta,\ iid$\\
$\theta_{MLE} = \arg_{\theta \in \Theta} P(D|\theta)$

$$
\begin{aligned}
P(D|\theta)
& = \prod_{i=1}^n P(x_i|\theta) \\
& = \prod_{i=1}^n \exp(\theta^T s(x_i)) h(x)/z(\theta) \\
& = z(\theta)^{-n} \exp(\theta^T \sum_{i=1}^n s(x_i)) \prod_{i=1}^n h(x_i) \\
& = z(\theta)^{-n} \exp(\theta^T s(D)) \prod_{i=1}^n h(x_i) \\
\end{aligned}
$$

取对数, 然后对$\theta_j$求导, 令其为零.具体的推导过程参见
\href{https://www.youtube.com/watch?v=LcbwmT1OAKo&list=PLD0F06AA0D2E8FFBA&index=29}{youtube (ML 5.3) MLE for an exponential family}

\section{MAP(Max a posteriori)}
Setup:
\begin{itemize}
\item Given $D = (\vector{x}), x_i \in R^d$
\item Assume a joint dist $P(D, \theta)$, where $\theta$ is a r.v
\item Goal: choose a good value of $\theta$ for $D$
\item Choose $\theta_{MAP} = \arg_\theta \max P(\theta | D)$
\end{itemize}
在给定数据$D$的情况下, $\theta$得到最可能取值, 注意和MLE的区别.
$\theta_{MLE} = \arg_\theta \max P(D|\theta)$

Pros:
\begin{itemize}
\item Easy interpretable
\item Avoid overfitting(MLE的一个缺点) - Regularization/Shrinkage
\item Tends to look like MLE asymptotically($n \to \infty$)
\end{itemize}

Cons:
\begin{itemize}
\item Not invariant under reparametrization, that is to say: $g(\theta_{MLE})$ is a MLE for $g(\theta)$
\item Point estimation - no repr of uncertainity in $\theta$\\
	从图\href{http://i.imgbox.com/k292Tu6D.png}{MAP max}中, 我们可以看到, $P(\theta|D)$在$\theta$接近0的位置取到最大值, 但是$\theta$只有很小的概率处于0的附近, 所以在图中这种情况, 我们的最佳选择是右边的一个极值点.
\item Must assume priori on $\theta$
\end{itemize}

\subsection{MAP for mean of a univariant Gaussian}
$D = (\vector{x},\ x_i \in R^d)$\\
Suppose $\theta$ is a r.v. $\obey N(\mu, 1)$ and $\vector{X}$ is conditionaly indep given $\theta$, 
and $X_i \obey N(\theta, \sigma^2)$ with $\sigma$ know\\
i.e: $P(\vector{x}|\theta) = \prod_{i =1}^n P(x_i|\theta)$


$$
\begin{aligned}
\theta_{MAP} 
& = \arg_\theta \max P(\theta | D)\\ 
& = \arg_\theta \max \dfrac{P(D|\theta) P(\theta)}{P(D)}\\ 
& \text{由于$P(D)$ 不是$\theta$的函数, 所以在$\theta$变化时, $P(D)$是定值}\\ 
& = \arg_\theta \max P(D|\theta) P(\theta)\\ 
& = \arg_\theta \max (\log(P(D|\theta)) + \log P(\theta))\\ 
\end{aligned}
$$
对$\theta$求导数, 令之为零, 求得$\theta$, 然后它是最大值点.
$$
\begin{aligned}
0 
& = \dfrac{\partial}{\partial \theta} (\log(P(D|\theta)) + \log P(\theta))\\
& = \dfrac{1}{\sigma^2} (\sum x_i - n \theta) + (\mu - \theta)\\
& = (\dfrac{\sum x_i}{\sigma^2} + \mu) - (\dfrac{n}{\sigma^2} + 1)\theta\\
\end{aligned}
$$
推出
$$
\theta 
= \dfrac{\dfrac{\sum x_i}{\sigma^2} + \mu}{\dfrac{n}{\sigma^2} + 1}
= \dfrac{\sum x_i + \sigma^2 \mu}{n + \sigma^2}
= \dfrac{n \dfrac{1}{n} \sum x_i + \sigma^2 \mu}{n + \sigma^2}
$$

$$ \theta_{MAP} = \dfrac{n}{n + \sigma^2} \mean{x} + \dfrac{\sigma^2}{n + \sigma^2} \mu $$
convex combination of sample mean and priori mean, and note that: 
$$\si n \to \infty,\ \theta_{MAP} = \mean{x}$$

$$ \theta_{MLE} = \mean{x} $$

\section{K-means Cluestering}
无监督, greedy algo, a special case of EM algo

Given $D = (\vector{x}), x_i \in R^d$

Intuition: points are near to the center of their cluster

Assume $K$ clusters, with unknown centers $\mu_1, \mu_2, \ldots, \mu_k$
$$
L
= \sum_{j = 1}^k \sum_{i | x_i \in\ cluster\ j}\norm{x_i - \mu_j}^2
= \sum_{j = 1}^k \sum_i^n a_{ij}\norm{x_i - \mu_j}^2
$$
where
$$
a_{ij} =
\left\{
  \begin{array}{ll}
    1, & \si x_i \in\ cluster\ j\\
    0, & \sinon
  \end{array}
\right.
$$
We want to choose $a_{ij}$ and $\mu_i$ to minimize $L$.

K-means: try to minimize $L$ with repect to(wrt) $a$ and $\mu$
\begin{enumerate}
\item Init $\mu_1, \mu_2, \ldots, \mu_k$
\item Choose optimal $a$ for fixed $\mu$
\item Choose optimal $\mu$ for fixed $a$
\item Repeat 2 and 3 until convergence
\end{enumerate}

For step 2:\\
对于固定的$\mu$, 也就是说我们知道了所有clusters的centers, 需要将$x_i$分配到这些clusters中, 直觉告诉我们应该\\
assign $x_i$ to the nearset $\mu_j$. i.e.
$$
a_{ij} =
\left\{
  \begin{array}{ll}
  1, & \si j = \arg \min_l \norm{x_i - \mu_l}^2\\
    0, & \sinon
  \end{array}
\right.
$$

For step 3:\\
对于固定$a$, 也就是说我们知道了$x_i$都是属于哪个cluster的, 然后需要找出最佳的cluster's center.\\
首先我们的直觉告诉我们, cluster center应该为属于这个cluster的点的中心, 也就是平均值.

$$
\begin{aligned}
\end{aligned}
L
= \sum_{j = 1}^k \sum_i^n a_{ij}\norm{x_i - \mu_j}^2
= \sum_{j = 1}^k \sum_i^n a_{ij}(x_i - \mu_j)^T (x_i - \mu_j)
$$

$$
0 = \grad_{\mu_j}{L} = \sum_i \sum_l a_{ij} \grad{(x_i - \mu_l)^T (x_i - \mu_l)}
$$

例如: $n=3, k=2, d=2$, 也就是说把3个二维的数据分到两个clusters中.\\
$x_i = (x_{i1}, x_{i2})^T$
$\mu_i = (\mu_{i1}, \mu_{i2})^T$
$a_{ij}$ 表示第$i$个点属于第$j$个cluster.

$$
\begin{aligned}
L
= & \sum_{j = 1}^2 \sum_{i=1}^3 a_{ij}(x_i - \mu_j)^T (x_i - \mu_j) \\
= &  a_{11}(x_1 - \mu_1)^T (x_1 - \mu_1) + a_{12}(x_1 - \mu_2)^T (x_1 - \mu_2) \\
& + a_{21}(x_2 - \mu_1)^T (x_2 - \mu_1) + a_{22}(x_2 - \mu_2)^T (x_2 - \mu_2) \\
& + a_{31}(x_3 - \mu_1)^T (x_3 - \mu_1) + a_{32}(x_3 - \mu_2)^T (x_3 - \mu_2) \\
\end{aligned}
$$
如果我对$\mu_1$求gradient, 那么不含有$\mu_1$(这里就是含有$\mu_2$)的项就为0,
$$
\begin{aligned}
\grad_{\mu_1}{L}
& = \sum_{i = 1}^3 a_{i1}\grad{(x_i - \mu_1)^T (x_i - \mu_1)} \\
& = \sum_{i = 1}^3 a_{i1} \times (-2(x_i - \mu_1)) \\
& = \sum_{i = 1}^3  -2a_{i1}(x_i - \mu_1) \\
\end{aligned}
$$
上式用到了simulation et optimisation中学到的一个公式:
$$
F(x)= \frac{1}{2} \ps{Ax}{x} - \ps{b}{x}
$$
\`ou, A une matrice de dimension $n$, sym\'trique d\'efinie positive. 那么
$$DF(x).h = \ps{Ax - b}{h}$$
$$\grad{F(x)} = Ax - b$$
且$F(x)$ est une fontion convexe, 所有有一个minimum

所以
$$
\begin{aligned}
0 = \grad_{\mu_j}{L}
&= \sum_i \sum_l a_{il} \grad{(x_i - \mu_l)^T (x_i - \mu_l)} \\
&= \sum_i a_{ij} \grad{(x_i - \mu_j)^T (x_i - \mu_j)} \\
&= \sum_i -2a_{ij} (x_i - \mu_j) \\
&= -2(\sum_i a_{ij}x_i - \mu_j \sum_i a_{ij})
\end{aligned}
$$

$$
\Rightarrow
\mu_j = \dfrac{\sum_i a_{ij}x_i}{\sum_i a_{ij}}
$$
$\sum_i a_{ij}$  表示属于cluster $j$个点的个数\\
所以这确实是cluster的重心

为了确认L在这个$\mu_j$取到的是一个minimum, 而不是maximum, 我们求一下二阶导.

$$
\frac{\partial }{\partial \mu_{jk}} \grad_{\mu_{j} L}
= 2(\sum_i a_{ij}) e_k
\Rightarrow
\grad^2_{\mu_j} L = 2(\sum_i a_{ij})I > 0
$$
所以这是一个minimum, 而不是一个maximum

\href{https://www.youtube.com/watch?v=zHbxbb2ye3E}{K-means demo}

\section{EM(expectation-maximization) algo}
A class of algos,
it's more of an approach to derive an aglo for approximately obtaining (MLE)maximum-likelihood estimation or (MAP)maximum a posteriori estimates of parameters
when some of the data is missing.

Given: $x = (\vector(x))$\\
Model: $(X, Z) \obey p_\theta$ for some (unkown) $\theta \in \Theta$

Motivation of EM:

\end{document}
