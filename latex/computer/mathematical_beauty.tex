% !Mode:: "TeX:UTF-8"
\documentclass{article}
\input{../../public/package}
\input{../../public/article}
\begin{document}
\title{数学之美}
\maketitle
\tableofcontents
\newpage
\section{统计语言模型}
 2014-04-03 15:27:32
给大家举个例子:在很多涉及到自然语言处理的领域,如机器翻译,语音识别,印刷体或手写体识别,拼写纠错,汉字输入和文献查询中,我们都需要知道一个文字序列是否能构成一个大家能理解的句子,显示给使用者.对这个问题,我们可以用一个简单的统计模型来解决这个问题. 
如果 $S$ 表示一连串特定顺序排列的词 $w_1, w_2,\cdots, w_n$,换句话说,$S$ 可以表示某一个由一连串特定顺序排练的词而组成的一个有意义的句子.
现在,机器对语言的识别从某种角度来说,就是想知道S在文本中出现的可能性,也就是数 学上所说的$S$ 的概率用 $P(S)$ 来表示.

利用条件概率的公式,S 这个序列出现的概率等于每一个词出现的概率相乘,于是$P(S)$ 可展开为: $$P(S) = P(w_1)P(w_2|w_1)P(w_3| w_1 w_2)…P(w_n|w_1 w_2…w_{n-1})$$ 
其 中 $P(w_1)$ 表示第一个词$w_1$ 出现的概率,$P(w_2|w_1)$ 是在已知第一个词的前提下,第二个词出现的概率,以次类推.不难看出,到了词$w_n$,它的出现概率取决于它前面所有词.

从计算上来看,各种可能性太多,无法 实现.因此我们假定任意一个词$w_i$的出现概率只同它前面的词 $w_{i-1}$ 有关(即马尔可夫假设),于是问题就变得很简单了.
现在,S 出现的概率就变为: $$P(S) = P(w_1)P(w_2|w_1)P(w_3|w_2)…P(w_i|w_{i-1})… $$
(当然,也可以假设一个词又前面N-1个词决定,模型稍微复杂些.) 接 下来的问题就是如何估计 $P (w_i|w_{i-1})$.
现在有了大量机读文本后,这个问题变得很简单,只要数一数这对词$(w_{i-1},w_i)$ 在统计的文本中出现了多少次,以及 $w_{i-1}$ 本身在同样的文本中前后相邻出现了多少次,然后用两个数一除就可以了,$P(w_i|w_{i-1}) = P(w_{i-1},w_i)/ P (w_{i-1})$. 也许很多人不相信用这么简单的数学模型能解决复杂的语音识别,机器翻译等问题.其实不光是常人,就连很多语言学家都曾质疑过这种方法的有效性,但事实证明,统计语言模型比任何已知的借助某种规则的解决方法都有效.比如在 Google 的中英文自动翻译中,用的最重要的就是这个统计语言模型.去年美国标准局(NIST) 对所有的机器翻译系统进行了评测,Google 的系统是不仅是全世界最好的,而且高出所有基于规则的系统很多.

十几年后,李开复用统计语言模型把 997 词语音识别的问题简化成了一个 20 词的识别问题,实现了有史以来第一次大词汇量非特定人连续语音的识别.

\subsection{汉语分词}
利用统计语言模型分词的方法,可以用几个数学公式简单概括如下: 我们假定一个句子S可以有几种分词方法,为了简单起见我们假定有以下三种: $A_1, A_2, A_3, ..., A_k, B_1, B_2, B_3, ..., B_m C_1, C_2, C_3, ..., C_n$ 其中,$A_1, A_2, B_1, B_2, C_1, C_2$ 等等都是汉语的词.那么最好的一种分词方法应该保证分完词后这个句子出现的概率最大.也就是说如果 $A_1,A_2,..., A_k$ 是最好的分法,那么 ($P$ 表示概率): $P (A_1, A_2, A_3, ..., A_k) 〉 P (B_1, B_2, B_3, ..., B_m)$, 并且 $P (A_1, A_2, A_3, ..., A_k) 〉 P(C_1, C_2, C_3, ..., C_n)$ 
因此,只要我们利用上回提到的统计语言模型计算出每种分词后句子出现的概率,并找出其中概率最大的,我们就能够找到最好的分词方法. 当然,这里面有一个实现的技巧.如果我们穷举所有可能的分词方法并计算出每种可能性下句子的概率,那么计算量是相当大的.
因此,我们可以把它看成是一个动态规划(Dynamic Programming) 的问题,并利用 "维特比"(Viterbi) 算法快速地找到最佳分词.

一般来讲,根 据不同应用,汉语分词的颗粒度大小应该不同.比如,在机器翻译中,颗粒度应该大一些,"北京大学"就不能被分成两个词.而在语音识别中,"北京大学"一般 是被分成两个词.

也许你想不到,中文分词的方法也被应用到英语处理,主要是手写体识别中.因为在识别手写体时,单词之间的空格就不很清楚了.中文分词方法可以帮助判别英语单 词的边界.其实,语言处理的许多数学方法通用的和具体的语言无关.在 Google 内,我们在设计语言处理的算法时,都会考虑它是否能很容易地适用于各种自然语言.这样,我们才能有效地支持上百种语言的搜索. 

对中文分词有兴趣的读者,可以阅读以下文献:\\
1. 梁南元 \href{http://www.touchwrite.com/demo/LiangNanyuan-JCIP-1987.pdf}{书面汉语自动分词系统}\\
2. 郭进 \href{http://www.touchwrite.com/demo/GuoJin-JCIP-1993.pdf 3. 郭进 Critical Tokenization and its Properties http://acl.ldc.upenn.edu/J/J97/J97-4004.pdf 4. 孙茂松 Chinese word segmentation without using lexicon and hand-crafted training data http://portal.acm.org/citation.cfm?coll=GUIDE&dl=GUIDE&id=980775}{统计语言模型和汉语音字转换的一些新结果}

\section{隐马尔科夫模型}
隐含马尔可夫模型是一个数学模型,到目前为之,它一直被认为是实现快速精确的语音识别系统的最成功的方法.

很多自然语言处理问题都可以等同于通信系统中的解码问题 -- 一个人根据接收到的信息,去猜测发话人要表达的意思.这其实就象通信中,我们根据接收端收到的信号去分析,理解,还原发送端传送过来的信息.

根据声学信号来推测说话者的意思,就是语音识别.
在计算机中,如果我们要根据接收到的英语信息,推测说话者的汉语意思,就是机器翻译,如果我们要根据带有拼写错误的语句推测说话者想表达的正确意思,那就是自动纠错.

以语音识别为例,当我们观测到语音信号 $o_1,o_2,o_3$ 时,我们要根据这组信号推测出发送的句子 $s_1,s_2,s_3$.\\
显然,我们应该在所有可能的句子中找最有可能性的一个.用数学语言来描述,就是在已知 $o_1,o_2,o_3$,...的情况下,
求使得条件概率 $P (s_1,s_2,s_3,...|o_1,o_2,o_3....) $达到最大值的那个句子 $s_1,s_2,s_3,...$ 当然,上面的概率不容易直接求出,于是我们可以间接地计算它.\\
利用贝叶斯公式并且省掉一个常数项,可以把上述公式等价变换成 $P(o_1,o_2,o_3,...|s_1,s_2,s_3....) * P(s_1,s_2,s_3,...)$ 
其中 $P(o_1,o_2,o_3,...|s_1,s_2,s_3....)$ 表示某句话 $s_1,s_2,s_3...$被读成 $o_1,o_2,o_3,...$的可能性, 
而 $P(s_1,s_2,s_3,...)$ 表示字串 $s_1,s_2,s_3,...$本身能够成为一个合乎情理的句子的可能性,
所以这个公式的意义是用发送信号为 $s_1,s_2,s_3...$这个数列的可能性乘以 $s_1,s_2,s_3...$本身可以一个句子的可能性,得出概率.

我们在这里做两个假设: 第一,$s_1,s_2,s_3,...$ 是一个马尔可夫链,也就是说,$s_i$ 只由 $s_{i-1}$ 决定 (详见系列一), 第二, 第 $i$ 时刻的接收信号 $o_i$ 只由发送信号 $s_i$ 决定(又称为独立输出假设, 即 $P(o_1,o_2,o_3,...|s_1,s_2,s_3....) = P(o_1|s_1) * P(o_2|s_2)*P(o_3|s_3)....$ 那么我们就可以很容易利用算法 Viterbi 找出上面式子的最大值,进而找出要识别的句子 $s_1,s_2,s_3,....$ 满足上述两个假设的模型就叫隐含马尔可夫模型.我们之所以用"隐含"这个词,是因为状态 $s_1,s_2,s_3,...$是无法直接观测到的.

隐含马尔可夫模型的应用远不只在语音识别中.在上面的公式中,如果我们把 $s_1,s_2,s_3,...$当成中文,把 $o_1,o_2,o_3,...$当成对应的英文,那么我们就能利用这个模型解决机器翻译问题, 如果我们把 $o_1,o_2,o_3,...$当成扫描文字得到的图像特征,就能利用这个模型解决印刷体和手写体的识别.

 \section{信息熵Entropy}
直到 1948 年,香农提出了"信息熵"的概念,才解决了对信息的量化度量问题. 一条信息的信息量大小和它的不确定性有直接的关系.比如说,我们要搞清楚一件非常非常不确定的事,或是我们一无所知的事情,就需要了解大量的信息.相反,如果我们对某件事已经有了较多的了解,我们不需要太多的信息就能把它搞清楚.所以,从这个角度,我们可以认为,信息量的度量就等于不确定性的多少.

那么我们如何量化的度量信息量呢?

我们来看一个例子,马上要举行世界杯赛了.大家都很关心谁会是冠军.\\
假如我错过了看世界杯,赛后我问一个知道比赛结果的观众"哪支球队是冠军"? 
他不愿意直接告诉我,而要让我猜,并且我每猜一次,他要收一元钱才肯告诉我是否猜对了,那么我需要付给他多少钱才能知道谁是冠军呢? \\
我可以把球队编上号,从 $1$ 到 $32$, 然后提问: "冠军的球队在 $1-16$ 号中吗?" 假如他告诉我猜对了, 我会接着问: "冠军在 1-8 号中吗?" 假如他告诉我猜错了, 我自然知道冠军队在 9-16 中. 这样只需要五次,我就能知道哪支球队是冠军.所以,谁是世界杯冠军这条消息的信息量只值五块钱. \\
当然,香农不是用钱,而是用 "比特"(bit)这个概念来度量信息量.一个比特是一位二进制数,计算机中的一个字节是八个比特.\\

在上面的例子中,这条消息的信息量是五比特.(如果有朝一日有六十四个队进入决赛阶段的比赛,那么"谁世界杯冠军"的信息量就是六比特,因为我们要多猜一次.) \\
读者可能已经发现, 信息量的比特数和所有可能情况的对数函数 $\log$ 有关. ($\log_{2}32=5, \log_{2}64=6$.) \\
有些读者此时可能会发现我们实际上可能不需要猜五次就能猜出谁是冠军,因为象巴西,德国,意大利这样的球队得冠军的可能性比日本,美国,韩国等队大的多.
因此,我们第一次猜测时不需要把 32 个球队等分成两个组,而可以把少数几个最可能的球队分成一组,把其它队分成另一组.然后我们猜冠军球队是否在那几只热门队中.
我们重复这样的过程,根据夺冠概率对剩下的候选球队分组,直到找到冠军队.这样,我们也许三次或四次就猜出结果.\\
因此,当每个球队夺冠的可能性(概率)不等时,"谁世界杯冠军"的信息量的信息量比五比特少.

香农指出,它的准确信息量应该是 $= -(p_1*\log_{2}p_1 + p_2 * \log_{2}p_2 +　．．．　＋p_{32} *\log_{2}p_{32})$, 
其中,$p_1,p_2 ,　．．．,p_{32}$ 分别是这 32 个球队夺冠的概率.\\
香农把它称为"信息熵" (Entropy),一般用符号 $H$ 表示,单位是比特.\\
有兴趣的读者可以推算一下当 32 个球队夺冠概率相同时,对应的信息熵等于五比特.
有数学基础的读者还可以证明上面公式的值不可能大于五.对于任意一个随机变量 X(比如得冠军的球队),
它的熵定义如下: 变量的不确定性越大,熵也就越大,把它搞清楚所需要的信息量也就越大.

有了"熵"这个概念,我们就可以回答本文开始提出的问题,即一本五十万字的中文书平均有多少信息量.\\
我们知道常用的汉字(一级二级国标)大约有 $7000 $字.假如每个字等概率,那么我们大约需要 $13$($\log_2{7000} = 12.77$) 个比特(即 $13$ 位二进制数)表示一个汉字.\\
但汉字的使用是不平衡的.实际上,前 1$0\%$ 的汉字占文本的 $95\%$ 以上.因此,即使不考虑上下文的相关性,而只考虑每个汉字的独立的概率,那么,每个汉字的信息熵大约也只有 $8-9$
(注: $ -(700 \times \dfrac{0.95}{700} \log_2{\dfrac{0.95}{700}} + (7000-700) \times \dfrac{0.05}{7000-700} \log_2{\dfrac{0.05}{7000-700}}) = 9.896$)
个比特.如果我们再考虑上下文相关性,每个汉字的信息熵只有5比特左右.\\
所以,一本五十万字的中文书,信息量大约是 $250$ 万比特.如果用一个好的算法压缩一下,整本书可以存成一个 $320$KB 的文件.
如果我们直接用两字节的国标编码存储这本书,大约需要 $1$MB(注: $50 \times 10^4 \times 2 Byte \simeq 1MB$)大小,是压缩文件的三倍.\\
这两个数量的差距,在信息论中称作"冗余度"(redundancy). \\
需要指出的是我们这里讲的 250 万比特是个平均数,同样长度的书,所含的信息量可以差很多.如果一本书重复的内容很多,它的信息量就小,冗余度就大.
不同语言的冗余度差别很大,而汉语在所有语言中冗余度是相对小的.这和人们普遍的认识"汉语是最简洁的语言"是一致的.
\end{document}
