% !Mode:: "TeX:UTF-8"
\documentclass{article}
\input{../public/package}
\input{../public/article}
\begin{document}
\title{数学之美}
\maketitle
\tableofcontents
\newpage
\section{统计语言模型}
 2014-04-03 15:27:32
给大家举个例子:在很多涉及到自然语言处理的领域,如机器翻译,语音识别,印刷体或手写体识别,拼写纠错,汉字输入和文献查询中,我们都需要知道一个文字序列是否能构成一个大家能理解的句子,显示给使用者.对这个问题,我们可以用一个简单的统计模型来解决这个问题. 
如果 $S$ 表示一连串特定顺序排列的词 $w_1, w_2,\cdots, w_n$,换句话说,$S$ 可以表示某一个由一连串特定顺序排练的词而组成的一个有意义的句子.
现在,机器对语言的识别从某种角度来说,就是想知道S在文本中出现的可能性,也就是数 学上所说的$S$ 的概率用 $P(S)$ 来表示.

利用条件概率的公式,S 这个序列出现的概率等于每一个词出现的概率相乘,于是$P(S)$ 可展开为: $$P(S) = P(w_1)P(w_2|w_1)P(w_3| w_1 w_2)…P(w_n|w_1 w_2…w_{n-1})$$ 
其 中 $P(w_1)$ 表示第一个词$w_1$ 出现的概率,$P(w_2|w_1)$ 是在已知第一个词的前提下,第二个词出现的概率,以次类推.不难看出,到了词$w_n$,它的出现概率取决于它前面所有词.

从计算上来看,各种可能性太多,无法 实现.因此我们假定任意一个词$w_i$的出现概率只同它前面的词 $w_{i-1}$ 有关(即马尔可夫假设),于是问题就变得很简单了.
现在,S 出现的概率就变为: $$P(S) = P(w_1)P(w_2|w_1)P(w_3|w_2)…P(w_i|w_{i-1})… $$
(当然,也可以假设一个词又前面N-1个词决定,模型稍微复杂些.) 接 下来的问题就是如何估计 $P (w_i|w_{i-1})$.
现在有了大量机读文本后,这个问题变得很简单,只要数一数这对词$(w_{i-1},w_i)$ 在统计的文本中出现了多少次,以及 $w_{i-1}$ 本身在同样的文本中前后相邻出现了多少次,然后用两个数一除就可以了,$P(w_i|w_{i-1}) = P(w_{i-1},w_i)/ P (w_{i-1})$. 也许很多人不相信用这么简单的数学模型能解决复杂的语音识别,机器翻译等问题.其实不光是常人,就连很多语言学家都曾质疑过这种方法的有效性,但事实证明,统计语言模型比任何已知的借助某种规则的解决方法都有效.比如在 Google 的中英文自动翻译中,用的最重要的就是这个统计语言模型.去年美国标准局(NIST) 对所有的机器翻译系统进行了评测,Google 的系统是不仅是全世界最好的,而且高出所有基于规则的系统很多.

十几年后,李开复用统计语言模型把 997 词语音识别的问题简化成了一个 20 词的识别问题,实现了有史以来第一次大词汇量非特定人连续语音的识别.

\subsection{汉语分词}
利用统计语言模型分词的方法,可以用几个数学公式简单概括如下: 我们假定一个句子S可以有几种分词方法,为了简单起见我们假定有以下三种: $A_1, A_2, A_3, ..., A_k, B_1, B_2, B_3, ..., B_m C_1, C_2, C_3, ..., C_n$ 其中,$A_1, A_2, B_1, B_2, C_1, C_2$ 等等都是汉语的词.那么最好的一种分词方法应该保证分完词后这个句子出现的概率最大.也就是说如果 $A_1,A_2,..., A_k$ 是最好的分法,那么 ($P$ 表示概率): $P (A_1, A_2, A_3, ..., A_k) 〉 P (B_1, B_2, B_3, ..., B_m)$, 并且 $P (A_1, A_2, A_3, ..., A_k) 〉 P(C_1, C_2, C_3, ..., C_n)$ 
因此,只要我们利用上回提到的统计语言模型计算出每种分词后句子出现的概率,并找出其中概率最大的,我们就能够找到最好的分词方法. 当然,这里面有一个实现的技巧.如果我们穷举所有可能的分词方法并计算出每种可能性下句子的概率,那么计算量是相当大的.
因此,我们可以把它看成是一个动态规划(Dynamic Programming) 的问题,并利用 "维特比"(Viterbi) 算法快速地找到最佳分词.

一般来讲,根 据不同应用,汉语分词的颗粒度大小应该不同.比如,在机器翻译中,颗粒度应该大一些,"北京大学"就不能被分成两个词.而在语音识别中,"北京大学"一般 是被分成两个词.

也许你想不到,中文分词的方法也被应用到英语处理,主要是手写体识别中.因为在识别手写体时,单词之间的空格就不很清楚了.中文分词方法可以帮助判别英语单 词的边界.其实,语言处理的许多数学方法通用的和具体的语言无关.在 Google 内,我们在设计语言处理的算法时,都会考虑它是否能很容易地适用于各种自然语言.这样,我们才能有效地支持上百种语言的搜索. 

对中文分词有兴趣的读者,可以阅读以下文献:\\
1. 梁南元 \href{http://www.touchwrite.com/demo/LiangNanyuan-JCIP-1987.pdf}{书面汉语自动分词系统}\\
2. 郭进 \href{http://www.touchwrite.com/demo/GuoJin-JCIP-1993.pdf 3. 郭进 Critical Tokenization and its Properties http://acl.ldc.upenn.edu/J/J97/J97-4004.pdf 4. 孙茂松 Chinese word segmentation without using lexicon and hand-crafted training data http://portal.acm.org/citation.cfm?coll=GUIDE&dl=GUIDE&id=980775}{统计语言模型和汉语音字转换的一些新结果}

\section{隐马尔科夫模型}
隐含马尔可夫模型是一个数学模型,到目前为之,它一直被认为是实现快速精确的语音识别系统的最成功的方法.

很多自然语言处理问题都可以等同于通信系统中的解码问题 -- 一个人根据接收到的信息,去猜测发话人要表达的意思.这其实就象通信中,我们根据接收端收到的信号去分析,理解,还原发送端传送过来的信息.

根据声学信号来推测说话者的意思,就是语音识别.
在计算机中,如果我们要根据接收到的英语信息,推测说话者的汉语意思,就是机器翻译,如果我们要根据带有拼写错误的语句推测说话者想表达的正确意思,那就是自动纠错.

以语音识别为例,当我们观测到语音信号 $o_1,o_2,o_3$ 时,我们要根据这组信号推测出发送的句子 $s_1,s_2,s_3$.\\
显然,我们应该在所有可能的句子中找最有可能性的一个.用数学语言来描述,就是在已知 $o_1,o_2,o_3$,...的情况下,
求使得条件概率 $P (s_1,s_2,s_3,...|o_1,o_2,o_3....) $达到最大值的那个句子 $s_1,s_2,s_3,...$ 当然,上面的概率不容易直接求出,于是我们可以间接地计算它.\\
利用贝叶斯公式并且省掉一个常数项,可以把上述公式等价变换成 $P(o_1,o_2,o_3,...|s_1,s_2,s_3....) * P(s_1,s_2,s_3,...)$ 
其中 $P(o_1,o_2,o_3,...|s_1,s_2,s_3....)$ 表示某句话 $s_1,s_2,s_3...$被读成 $o_1,o_2,o_3,...$的可能性, 
而 $P(s_1,s_2,s_3,...)$ 表示字串 $s_1,s_2,s_3,...$本身能够成为一个合乎情理的句子的可能性,
所以这个公式的意义是用发送信号为 $s_1,s_2,s_3...$这个数列的可能性乘以 $s_1,s_2,s_3...$本身可以一个句子的可能性,得出概率.

我们在这里做两个假设: 第一,$s_1,s_2,s_3,...$ 是一个马尔可夫链,也就是说,$s_i$ 只由 $s_{i-1}$ 决定 (详见系列一), 第二, 第 $i$ 时刻的接收信号 $o_i$ 只由发送信号 $s_i$ 决定(又称为独立输出假设, 即 $P(o_1,o_2,o_3,...|s_1,s_2,s_3....) = P(o_1|s_1) * P(o_2|s_2)*P(o_3|s_3)....$ 那么我们就可以很容易利用算法 Viterbi 找出上面式子的最大值,进而找出要识别的句子 $s_1,s_2,s_3,....$ 满足上述两个假设的模型就叫隐含马尔可夫模型.我们之所以用"隐含"这个词,是因为状态 $s_1,s_2,s_3,...$是无法直接观测到的.

隐含马尔可夫模型的应用远不只在语音识别中.在上面的公式中,如果我们把 $s_1,s_2,s_3,...$当成中文,把 $o_1,o_2,o_3,...$当成对应的英文,那么我们就能利用这个模型解决机器翻译问题, 如果我们把 $o_1,o_2,o_3,...$当成扫描文字得到的图像特征,就能利用这个模型解决印刷体和手写体的识别.

 \section{信息熵Entropy}
直到 1948 年,香农提出了"信息熵"的概念,才解决了对信息的量化度量问题. 一条信息的信息量大小和它的不确定性有直接的关系.比如说,我们要搞清楚一件非常非常不确定的事,或是我们一无所知的事情,就需要了解大量的信息.相反,如果我们对某件事已经有了较多的了解,我们不需要太多的信息就能把它搞清楚.所以,从这个角度,我们可以认为,信息量的度量就等于不确定性的多少.

那么我们如何量化的度量信息量呢?

我们来看一个例子,马上要举行世界杯赛了.大家都很关心谁会是冠军.\\
假如我错过了看世界杯,赛后我问一个知道比赛结果的观众"哪支球队是冠军"? 
他不愿意直接告诉我,而要让我猜,并且我每猜一次,他要收一元钱才肯告诉我是否猜对了,那么我需要付给他多少钱才能知道谁是冠军呢? \\
我可以把球队编上号,从 $1$ 到 $32$, 然后提问: "冠军的球队在 $1-16$ 号中吗?" 假如他告诉我猜对了, 我会接着问: "冠军在 1-8 号中吗?" 假如他告诉我猜错了, 我自然知道冠军队在 9-16 中. 这样只需要五次,我就能知道哪支球队是冠军.所以,谁是世界杯冠军这条消息的信息量只值五块钱. \\
当然,香农不是用钱,而是用 "比特"(bit)这个概念来度量信息量.一个比特是一位二进制数,计算机中的一个字节是八个比特.\\

在上面的例子中,这条消息的信息量是五比特.(如果有朝一日有六十四个队进入决赛阶段的比赛,那么"谁世界杯冠军"的信息量就是六比特,因为我们要多猜一次.) \\
读者可能已经发现, 信息量的比特数和所有可能情况的对数函数 $\log$ 有关. ($\log_{2}32=5, \log_{2}64=6$.) \\
有些读者此时可能会发现我们实际上可能不需要猜五次就能猜出谁是冠军,因为象巴西,德国,意大利这样的球队得冠军的可能性比日本,美国,韩国等队大的多.
因此,我们第一次猜测时不需要把 32 个球队等分成两个组,而可以把少数几个最可能的球队分成一组,把其它队分成另一组.然后我们猜冠军球队是否在那几只热门队中.
我们重复这样的过程,根据夺冠概率对剩下的候选球队分组,直到找到冠军队.这样,我们也许三次或四次就猜出结果.\\
因此,当每个球队夺冠的可能性(概率)不等时,"谁世界杯冠军"的信息量的信息量比五比特少.

香农指出,它的准确信息量应该是 $= -(p_1*\log_{2}p_1 + p_2 * \log_{2}p_2 +　．．．　＋p_{32} *\log_{2}p_{32})$, 
其中,$p_1,p_2 ,　．．．,p_{32}$ 分别是这 32 个球队夺冠的概率.\\
香农把它称为"信息熵" (Entropy),一般用符号 $H$ 表示,单位是比特.\\
有兴趣的读者可以推算一下当 32 个球队夺冠概率相同时,对应的信息熵等于五比特.
有数学基础的读者还可以证明上面公式的值不可能大于五.对于任意一个随机变量 X(比如得冠军的球队),
它的熵定义如下: 变量的不确定性越大,熵也就越大,把它搞清楚所需要的信息量也就越大.

有了"熵"这个概念,我们就可以回答本文开始提出的问题,即一本五十万字的中文书平均有多少信息量.\\
我们知道常用的汉字(一级二级国标)大约有 $7000 $字.假如每个字等概率,那么我们大约需要 $13$($\log_2{7000} = 12.77$) 个比特(即 $13$ 位二进制数)表示一个汉字.\\
但汉字的使用是不平衡的.实际上,前 1$0\%$ 的汉字占文本的 $95\%$ 以上.因此,即使不考虑上下文的相关性,而只考虑每个汉字的独立的概率,那么,每个汉字的信息熵大约也只有 $8-9$
(注: $ -(700 \times \dfrac{0.95}{700} \log_2{\dfrac{0.95}{700}} + (7000-700) \times \dfrac{0.05}{7000-700} \log_2{\dfrac{0.05}{7000-700}}) = 9.896$)
个比特.如果我们再考虑上下文相关性,每个汉字的信息熵只有5比特左右.\\
所以,一本五十万字的中文书,信息量大约是 $250$ 万比特.如果用一个好的算法压缩一下,整本书可以存成一个 $320$KB 的文件.
如果我们直接用两字节的国标编码存储这本书,大约需要 $1$MB(注: $50 \times 10^4 \times 2 Byte \simeq 1MB$)大小,是压缩文件的三倍.\\
这两个数量的差距,在信息论中称作"冗余度"(redundancy). \\
需要指出的是我们这里讲的 250 万比特是个平均数,同样长度的书,所含的信息量可以差很多.如果一本书重复的内容很多,它的信息量就小,冗余度就大.
不同语言的冗余度差别很大,而汉语在所有语言中冗余度是相对小的.这和人们普遍的认识"汉语是最简洁的语言"是一致的.

\section{信息熵}
我们知道,语言模型是为了用上下文预测当前的文字,模型越好,预测得越准,那么当前文字的不确定性就越小. 

信息熵正是对不确定性的衡量,因此信息熵可以直接用于衡量统计语言模型的好坏.
贾里尼克从信息熵出发,定义了一个称为语言模型复杂度 (Perplexity)的概念,直接衡量语言模型的好坏.一个模型的复杂度越小,模型越好.

信息论中仅次于熵的另外两个重要的概念是"互信息"(Mutual Information) 和"相对熵"(Kullback-Leibler Divergence). 

"互信息"是信息熵的引申概念,它是对两个随机事件相关性的度量.比如说今天随机事件北京下雨和随机变量空气湿度的相关性就很大,但是和姚明所在的休斯敦火

相对熵用来衡量两个正函数是否相似,对于两个完全相同的函数,它们的相对熵等于零.在自然语言处理中可 以用相对熵来衡量两个常用词(在语法上和语义上)是否同义,或者两篇文章的内容是否相近等等.利用相对熵,我们可以到处信息检索中最重要的一个概念:词频 率-逆向文档频率(TF/IDF).我们下回会介绍如何根据相关性对搜索出的网页进行排序,就要用到TF/IDF

信息论中另外一个重要的概念是"相对熵",在有些文献中它被称为成"交叉熵".

\section{贾里尼克}
贾里尼克从麻省理工获得博士学位后,在哈佛大学教了一年书,然后到康乃尔大学任教.他之所以选择康乃尔大学,是因为找工作时和那里的一位语言学家谈得颇为投

"\textbf{我每开除一名语言学家,我的语音识别系统错误率就降低一个百分点.}" 这句话后来在业界广为流传,为每一个搞语音识别和语言处理的人所熟知.

1972年,贾里尼克到IBM 华生实验室(IBM T.G.watson Labs)做学术休假,无意中领导了语音识别实验室,两年后他在康乃尔和IBM 之间选择了留在IBM.在那里,贾里尼克组建了阵容空前绝后强大的研究队伍,其中包括他的著名搭档波尔(Bahl),著名的语音识别 Dragon 公司的创始人贝克夫妇,解决最大熵迭代算法的达拉皮垂(Della Pietra)孪生兄弟,BCJR 算法的另外两个共同提出者库克(Cocke)和拉维夫(Raviv),以及第一个提出机器翻译统计模型的布朗.

\section{关键词与文章关联性分析}
根据网页的长度,对关键词的次数进行归一化,也就是用关键词的次数除以网页的总字数.我们把这个商称为"关键词的频率",或者"单文本词汇频率"(Term Frequency).

比如,在某个一共有一千词的网页中"原子能", "的"和"应用"分别出现了 2 次,35 次 和 5 次,那么它们的词频就分别是 0.002, 0.035 和 0.005. 
我们将这三个数相加,其和 0.042 就是相应网页和查询"原子能的应用" 相关性的一个简单的度量.

概括地讲,如果一个查询包含关键词 w1,w2,...,wN, 它们在一篇特定网页中的词频分别是: TF1, TF2, ..., TFN. (TF: term frequency).\\
那么,这个查询和该网页的相关性就是: TF1 + TF2 + ... + TFN.

"应删除词"(Stopwords),也就是说在度量相关性是不应考虑它们的频率.

在汉语中,"应用"是个很通用的词,而"原子能"是个很专业的词,后者在相关性排名中比前者重要.因此我们需要给汉语中的每一个词给一个权重,这个权重的设定必须满足下面两个条件: 
\begin{enumerate}
\item 一个词预测主题能力越强,权重就越大,反之,权重就越小.我们在网页中看到"原子能"这个词,或多或少地能了解网页的主题.我们看到"应用"一次,对主题基本上还是一无所知.因此,"原子能"的权重就应该比应用大.

\item 应删除词的权重应该是零.
\end{enumerate}

我们很容易发现,如果一个关键词只在很少的网页中出现,我们通过它就容易锁定搜索目标,它的权重也就应该大.
反之如果一个词在大量网页中出现,我们看到它仍然不很清楚要找什么内容,因此它应该小.
概括地讲,假定一个关键词 w 在 Dw 个网页中出现过,那么 Dw 越大,w 的权重越小,反之亦然.

在信息检索中,使用最多的权重是"逆文本频率指数" (Inverse document frequency 缩写为IDF),它的公式为$\log(D／Dw)$其中D是全部网页数.
比如,我们假定中文网页数是D＝１０亿,应删除词"的"在所有的网页中都出现,即 Dw＝１０亿,那么它的$IDF＝\log(10亿/10亿)= \log (1) = 0$.

假如专用词"原子能"在两百万个网页中出现,即Dw＝200万,则它的权重IDF＝log(500) =6.2.
又假定通用词"应用",出现在五亿个网页中,它的权重IDF = log(2) 则只有 0.7.也就只说,在网页中找到一个"原子能"的比配相当于找到九个"应用"的匹配.

利用 IDF,上述相关性计算个公式就由词频的简单求和变成了加权求和,即 TF1*IDF1 +　TF2*IDF2 ＋... + TFN*IDFN.在上面的例子中,该网页和"原子能的应用"的相关性为

IDF 的概念就是一个特定条件下,关键词的概率分布的交叉熵(Kullback-Leibler Divergence)(详见上一系列).这样,信息检索相关性的度量,又回到了信息论.

\section{地址解析与有限状态机}
北京市双清路83号"对于上面的有限状态来讲有效,而 "上海市辽宁省马家庄"则无效(因为无法从市走回到省).

使用有限状态机识别地址,关键要解决两个问题,即通过一些有效的地址建立状态机,以及给定一个有限状态机后,地址字串的匹配算法.好在这两个问题都有现成的算法.有了关于地址的有限状态机后,我们就可又用它分析网页,找出网页中的地址部分,建立本地搜索的数据库.同样,我们也可以对用户输入的查询进行分析,挑出其中描述地址的部分,当然,剩下的关键词就是用户要找的内容.比如,对于用户输入的"北京市双清路附近的酒家",Google 本地会自动识别出地址"北京市双清路"和要找的对象"酒家".

它只能进行严格匹配.(其实,有限状态机在计算机科学中早期的成功应用是在程序语言编译器的设计中.一个能运行的程序在语法上必须是没有错的,所以不需要模糊匹配.而自然语言则很随意,无法用简单的语法描述.)

当用户输入的地址不太标准或者有错别字时,有限状态机会束手无策,因为它只能进行严格匹配.(其实,有限状态机在计算机科学中早期的成功应用是在程序语言编译器的设计中.一个能运行的程序在语法上必须是没有错的,所以不需要模糊匹配.而自然语言则很随意,无法用简单的语法描述.)

\textbf{基于概率的有限状态机}.这种基于概率的有限状态机和离散的马尔可夫链(详见前面关于马尔可夫模型的系列)基本上等效.

计算机独立磁盘冗余阵列(RAID)的发明人凯茨(Randy Katz) 教授.

辛格因为舍不得放下两个孩子,很少参加各种会议,但是他仍然被学术界公认为是当今最权威的网络搜索专家.2005年,

被学术界公认为是当今最权威的网络搜索专家.2005年,

\section{信息指纹}
任何一段信息文字,都可以对应一个不太长的随机数,作为区别它和其它信息的指纹(Fingerprint).只要算法设计的好,任何两段信息的指纹都很难重复,就如同人类的指纹一样.信息指纹在加密,信息压缩和处理中有着广泛的应用.

我们在图论和网络爬虫一 文中提到,为了防止重复下载同一个网页,我们需要在哈希表中纪录已经访问过的网址(URL).但是在哈希表中以字符串的形式直接存储网址,既费内存空间, 又浪费查找时间.现在的网址一般都较长,比如,如果在 Google 或者百度在查找数学之美,对应的网址长度在一百个字符以上.下面是百度的链接

产生信息指纹的关键算法是伪随机数产生器算法(prng).最早的

信息指纹的用途远不止网址的消重,信息指纹的的孪生兄弟是密码.信息指纹的一个特征是其不可逆性, 也就是说, 无 法根据信息指纹推出原有信息,这种性质, 正是网络加密传输所需要的.比如说,一个网站可以根据用户的Cookie 识别不同用户,这个 cookie 就是信息指纹.但是网站无法根据信息指纹了解用户的身份,这样就可以保护用户的隐私.在互联网上,加密的可靠性,取决于是否很难人为地找到拥有同一指纹的

\section{日心说与地心说的启示}
\begin{enumerate}
\item 一个正确的数学模型应当在形式上是简单的.(托勒密的模型显然太复杂.) 
\item 一个正确的模型在它开始的时候可能还不如一个精雕细琢过的错误的模型来的准确,但是,如果我们认定大方向是对的,就应该坚持下去.(日心说开始并没有地心说准确.)
\item 大量准确的数据对研发很重要
\item 正确的模型也可能受噪音干扰,而显得不准确,这时我们不应该用一种凑合的修正方法来弥补它,而是要找到噪音的根源,这也许能通往重大发现. 在网络搜索的研发中,我们在前面提到的单文本词频/逆文本频率指数(TF/IDF) 和网页排名(page rank)都相当于是网络搜索中的"椭圆模型",它们都很简单易懂.
\end{enumerate}

\section{柯林斯与布莱尔}
柯林斯:追求完美

柯林斯的特点就是把事情做到极致

柯林斯的博士论文堪称是自然语言处理领域的范文.它像一本优秀的小说,把所有事情的来龙去脉介绍的清清楚楚,对于任何有一点计算机和自然语言处理知识的人,都可以轻而易举地读懂他复杂的方法.

虽然柯林斯的师兄布莱尔 (Eric Brill) 和 Ratnaparkhi 以及师弟 Eisnar 都完成了相当不错的语言文法分析器,但是柯林斯却将它做到了极致,使它在相当长一段时间内成为世界上最好的文法分析器.柯林斯成功的关键在于将文法分析的 每一个细节都研究得很仔细.柯林斯用的数学模型也很漂亮,整个工作可以用完美来形容.

\bigskip
布莱尔:简单才美

布莱尔的成名作是基于变换规则的机器学习方法 (transformation rule based machine learning).

这个方法名称虽然很复杂,其实非常简单. 我们以拼音转换字为例来说明它:
\begin{enumerate}
\item 第一步,我们把每个拼音对应的汉字中最常见的找出来作为第一遍变换的结果,当然结果有不少错误.比如,"常识"可能被转换成"长识", 
\item 第二步,可以说是"去伪存真",我们用计算机根据上下文,列举所有的同音字替换的规则,比如,如果 chang 被标识成"长",但是后面的汉字是"识",则将"长"改成"常", 
\item 第三步,应该就是"去粗取精",将所有的规则用到事先标识好的语料中,挑出有用的,删掉无用的.
\item 然后重复二三步,直到找不到有用的为止. 
\end{enumerate}

布莱尔就靠这么简单的方法,在很多自然语言研究领域,得到了几乎最好的结果.由于他的方法再简单不过了,许许多多的人都跟着学.

\section{最大熵原理}
最大熵原理指出,当我们需要对一个随机事件的概率分布进行预测时,我们的预测应当满足全部已知的条件,而对未知的情况不要做任何主观假设.(不做主观假设这点很重要.)\\
在这种情况下,概率分布最均匀,预测的风险最小.因为这时概率分布的信息熵最大,所以人们称这种模型叫"最大熵模型".\\
我们常说,不要把所有的鸡蛋放在一个篮子里,其实就是最大熵原理的一个朴素的说法,因为当我们遇到不确定性时,就要保留各种可能性.

第一,根据语言模型,wang-xiao-bo 可以被转换成王晓波和王小波,第二,根据主题,王小波是作家,<黄金时代>的作者等等,而王晓波是台湾研究两岸关系的学者.

因此,我们就可以建立一个最大熵模型,同时满足这两种信息.现在的问题是,这样一个模型是否存在.匈牙利著名数学家,信息论最高奖香农奖得主希萨(Csiszar)证明,对任何一组不自相矛盾的信息,这个最大熵模型不仅存在,而且是唯一的.而且它们都有同一个非常简单的形式

对任何一组不自相矛盾的信息,这个最大熵模型不仅存在,而且是唯一的.而且它们都有同一个非常简单的形式 -- 指数函数.下面公式是根据上下文(前两个词)和主题预测下一个词的最大熵模型,其中 w3 是要预测的词(王晓波或者王小波)w1 和 w2 是它的前两个字(比如说它们分别是"出版",和""),也就是其上下文的一个大致估计,subject 表示主题. 我们看到,在上面的公式中,有几个参数 lambda 和 Z ,他们需要通过观测数据训练出来.

最大熵模型在自然语言处理和金融方面很多有趣的应用.

最原始的最大熵模型的训练方法是一种称为通用迭代算法 GIS(generalized iterative scaling) 的迭代算法.GIS 的原理并不复杂,大致可以概括为以下几个步骤: 
\begin{enumerate}
\item 假定第零次迭代的初始模型为等概率的均匀分布. 
\item 用第 N 次迭代的模型来估算每种信息特征在训练数据中的分布,如果超过了实际的,就把相应的模型参数变小,否则,将它们便大. 
\item 重复步骤2 直到收敛.
\end{enumerate}

GIS 算法每次迭代的时间都很长,需要迭代很多次才能收敛,而且不太稳定,即使在 64 位计算机上都会出现溢出.\\
因此,在实际应用中很少有人真正使用 GIS.大家只是通过它来了解最大熵模型的算法.

改进迭代算法 IIS(improved iterative scaling).这使得最大熵模型的训练时间缩短了一到两个数量级.这样最大熵模型才有可能变得实用.

词性标识系统和句法分析器.
拉纳帕提的论文发表后让人们耳目一新.拉纳帕提的词性标注系统,至今仍然是使用单一方法最好的系统.科学家们从拉纳帕提的成就中,又看到了用最大熵模型解决复杂的文字信息处理的希望.

最大熵模型,可以说是集简与繁于一体,形式简单,实现复杂.值得一提的是,在Google的很多产品中,比如机器翻译,都直接或间接地用到了最大熵模型.

达拉皮垂兄弟等科学家在那里,用于最大熵模型和其他一些先进的数学工具对股票预测,获得了巨大的成功.从该基金

信息处理的很多数学手段,包括隐含马尔可夫模型,子波变换,贝叶斯网络等等,在华尔街多有直接的应用.由此可见,数学模型的作用.

\section{搜索引擎作弊}
针对搜索引擎网页排名的作弊(SPAM).以至于用户发现在搜索引擎中排名靠前的网页不一定就是高质量的,用句俗话说,闪光的不一定是金子.

抓作弊的方法很像信号处理中的去噪音的办法.学过信息论和有信号处理经验的读者可能知道这么一个事实,
我们如果在发动机很吵的汽车里用手机打电话,对方可能听不清,但是如果我们知道了汽车发动机的频率,我们可以加上一个和发动机噪音相反的信号,很容易地消除发动机的噪音,
这样,收话人可以完全听不到汽车的噪音.\\
事实上,现在一些高端的手机已经有了这种检测和消除噪音的功能.

消除噪音的流程可以概括如下: 在图中,原始的信号混入了噪音,在数学上相当于两个信号做卷积.噪音消除的过程是一个解卷积的过程.这在信号处理中并不是什么难题.因为第一,汽车发动机的频率是固定的,第二,这个频率的噪音重复出现,只要采集几秒钟的信号进行处理就能做到.\\
从广义上讲,只要噪音不是完全随机的,并且前后有相关性,就可以检测到并且消除.(事实上,完全随机不相关的高斯白噪音是很难消除的.) 

搜索引擎的作弊者所作的事,就如同在手机信号中加入了噪音,使得搜索结果的排名完全乱了.\\
但是,这种人为加入的噪音并不难消除,因为作弊者的方法不可能是随机的(否则就无法提高排名了).而且,作弊者也不可能是一天换一种方法,即作弊方法是时间相关的.\\
因此,搞搜索引擎排名算法的人,可以在搜集一段时间的作弊信息后,将作弊者抓出来,还原原有的排名.当然这个过程需要时间,就如同采集汽车发动机噪音需要时间一样,在这段时间内,作弊者可能会尝到些甜头.

\section{分类问题: 矩阵奇异值分解}
当两条新闻向量夹角的余弦等于一时,这两条新闻完全重复(用这个办法可以删除重复的网页),当夹角的余弦接近于一时,两条新闻相似,从而可以归成一类,夹角的余弦越小,两条新闻越不相关.

在自然语言处理中,最常见的两类的分类问题分别是,将文本按主题归类(比如将所有介绍亚运会的新闻归到体育类)和将词汇表中的字词按意思归类(比如将各种体育运动的名称个归成一类).这两种分类问题都可用通过矩阵运算来圆满地,同时解决.为了说明如何用矩阵这个工具类解决这两个问题的,让我们先来来回顾一下我们在余弦定理和新闻分类中介绍的方法.

从理论上讲,这种算法非常好.但是计算时间特别长.通常,我们要处理的文章的数量都很大,至少在百万篇以上,二次回标有非常长,比如说有五十万个词(包括人名地名产品名称等等).如果想通过对一百万篇文章两篇两篇地成对比较,来找出所有共同主题的文章,就要比较五千亿对文章.现在的计算机一秒钟最多可以比较一千对文章,完成这一百万篇文章相关性比较就需要十五年时间.注意,要真正完成文章的分类还要反复重复上述计算.

用一个大矩阵A来描述这一百万篇文章和五十万词的关联性.这个矩阵中,每一行对应一篇文章,每一列对应一个词. 在上面的图中,M=1,000,000,N=500,000.第 i 行,第 j 列的元素,是字典中第 j 个词在第 i 篇文章中出现的加权词频(比如,TF/IDF).读者可能已经注意到了,这个矩阵非常大,有一百万乘以五十万,即五千亿个元素.

奇异值分解就是把上面这样一个大矩阵,分解成三个小矩阵相乘,如下图所示.比如把上面的例子中的矩阵分解成一个一百万乘以一百的矩阵X,一个一百乘以一百的矩阵B,和一个一百乘以五十万的矩阵Y.这三个矩阵的元素总数加起来也不过1.5亿,仅仅是原来的三千分之一.相应的存储量和计算量都会小三个数量级以上.

三个矩阵有非常清楚的物理含义.第一个矩阵X中的每一行表示意思相关的一类词,其中的每个非零元素表示这类词中每个词的重要性(或者说相关性),数值越大越相关.最后一个矩阵Y中的每一列表示同一主题一类文章,其中每个元素表示这类文章中每篇文章的相关性.中间的矩阵则表示类词和文章雷之间的相关性.因此,我们只要对关联矩阵A进行一次奇异值分解,w 我们就可以同时完成了近义词分类和文章的分类.(同时得到每类文章和每类词的相关性).

Google 中国的张智威博士和几个中国的工程师及实习生已经实现了奇异值分解的并行算法,我认为这是 Google 中国对世界的一个贡献.
\end{document}
